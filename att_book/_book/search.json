[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning and AI for Health Sciences",
    "section": "",
    "text": "Preface\nTo be written.\nThis is a beta version of a book. Use it carefully!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction_AI.html",
    "href": "introduction_AI.html",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "",
    "text": "1.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#data-era",
    "href": "introduction_AI.html#data-era",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.2 Data era",
    "text": "1.2 Data era\nThis is an introductory chapter about data and data modelling for evidence based actions, decisions and policy making with a particular interest in advanced therapeutic technologies. Data are representations of facts, observations, or measurements structured or unstructured that describe the attributes or states of objects, events, or phenomena. They are symbols or signals without inherent meaning until interpreted within a context or combined with other information. In scientific and computational contexts, data are typically considered the raw inputs from which information and knowledge are derived through processes of organization, analysis, and interpretation. Data can assume several different forms including, numeric readings from a sensor, characters in a database, or pixels in an image, text in a message, electronic information, sequence of bases in DNA and RNA, etc. Data are evidence of our immediate reality and contexts therefore being crucial for fact based argumentation.\nWe live in an era of explosion in the availability of data, driven by the rapid expansion of the internet, advances in computational power, and the massive growth of digital storage and connectivity devices . Over recent decades, the cost of computing and storage has plummeted while the ability to collect, transmit, and analyze information has risen exponentially; in many domains, the data generated in the last few years surpass those produced in all prior human history (Hilbert and López 2011; Stephens et al. 2015). Historical analyses of data production and storage capacity show orders-of-magnitude increases year over year, reshaping how science, industry, and society operate, imposing almost naturally a new data driven paradigm (Hilbert and López 2011).\nA particular example of transformative shift is the omics era in the life sciences, a period in which substantial amount of data has been produced in genomics, transcriptomics, proteomics, metabolomics, implying in massive, complex datasets capturing biological systems in high resolution (Stephens et al. 2015; Hasin, Seldin, and Lusis 2017). Similar surges occur across disciplines from astronomy to agriculture and environmental monitoring where instruments and sensors continuously collect structured and unstructured data at scales that exceed traditional analytical capacities (Stephens et al. 2015).\nIn health care, data streams such as electronic health records (EHR), laboratory tests, medical imaging, wearable sensors, and omics profiles create a multidimensional view of patients (Jensen, Jensen, and Brunak 2012; Hripcsak and Albers 2013). The later kind of view can be integrated and enhanced with the possibilities of contrasting omics and biological data with other sources including social economic information bringing an opportunity for new solutions in health care.\nA particular challenge and frontier of data era is the need for integration and to fuse heterogeneous sources into coherent, trustworthy insights for predictive modeling, personalized treatment, and clinical decision support (Raghupathi and Raghupathi 2014; Karczewski and Snyder 2018; Jensen, Jensen, and Brunak 2012). Such an integration can enhance outcomes while maintaining safety, fairness, and transparency (Topol 2019) when done responsibly.\nSeveral concepts, and ideas can help us to understand and navigate this complex landscape of data sourcing, storing, processing and modelling for evidence actions. The rest of this chapter will cover these topics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#sources-of-data",
    "href": "introduction_AI.html#sources-of-data",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.3 Sources of data",
    "text": "1.3 Sources of data\nAI and machine-learning methods depend fundamentally on the quality and nature of the data used to train them. In health sciences and particularly in the field of Advanced Therapeutic Technologies (ATT), data can emerge from a wide range of processes, yet most of them can be meaningfully grouped into two broad classes: observational and experimental. Understanding the differences between these types is essential because each one carries specific strengths, limitations, and implications for modelling and decision-making.\nObservational data arise from situations in which no intervention is imposed by the researcher; instead, information is simply recorded as it naturally occurs in clinical or biological settings. Much of modern health data falls into this category. Electronic health records, for instance, routinely capture demographic information, laboratory results, diagnoses, procedures, and medication histories as part of everyday clinical care. Wearable sensors provide continuous streams of physiological measurements such as heart rate, oxygen saturation, sleep quality, or activity levels, reflecting the patient’s lived environment rather than a controlled experiment. Medical imaging repositories CT, MRI, ultrasound, histopathology are also observational in nature, as they document the state of the patient without manipulating it. Registries, whether for cancer, transplantation, rare diseases or adverse events, add another important layer, offering longitudinal and population-level perspectives. In ATT contexts, observational data play a crucial role in understanding real-world performance of new therapeutic technologies, monitoring long-term safety after regulatory approval, and identifying predictors of treatment success or failure within routine clinical practice.\nIn contrast, experimental data are generated in controlled settings where a researcher actively intervenes by assigning treatments, conditions, or exposures according to a predefined design. This category encompasses a wide spectrum of studies, from early in vitro experiments and animal models to human clinical trials. For example, before a new gene or cell therapy reaches patients, it undergoes a series of structured tests designed to quantify biological response, evaluate safety profiles, and characterize potential off-target effects. Controlled laboratory experiments may examine cellular behavior when exposed to a regenerative biomaterial or probe the efficiency of a CRISPR-based gene-editing protocol. Preclinical in vivo studies evaluate therapeutic effects and toxicity under standardized conditions. At the clinical end of the spectrum, randomized controlled trials compare one treatment against another (or against standard care) under carefully regulated environments, providing the most rigorous evidence for causality. These experimental datasets form the backbone for regulatory assessments in ATT, supporting decisions related to trial approval, dosing strategies, safety monitoring, and eventual market authorization.\nAlthough observational and experimental data differ in structure and purpose, they are deeply complementary. Observational data offer a broad, naturalistic view of patient populations and allow researchers to capture the complexity of real-world clinical behavior. Experimental data provide the depth and rigor needed to establish causal relationships and quantify the true therapeutic effect of an intervention. In practice, robust AI and ML applications in health care frequently rely on both. Predictive models developed from observational data may later be validated or even refined using experimental evidence. Similarly, insights from experimental studies often guide the design of models applied to real-world datasets. Together, these data sources provide a foundation on which trustworthy, transparent, and clinically meaningful AI systems can be built.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#models",
    "href": "introduction_AI.html#models",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.4 Models",
    "text": "1.4 Models\nThe definition of what is a model is the first useful conceptual tool we will discuss intuitively with the example depicted in Figure 1.1. In the center right of the Figure we have a cylinder that was cut in half and which could be represented in different manners, including, the obvious half cylinder, but also as triangle, an ellipsis and a semi-ellipsis, depending on the point of view. If we consider the cylinder as the reality we want to represent, we see that we have at least three different possible representations. The word representation has an interesting synonym: model. Thus, models can be understood and seen merely as representations of a given situation we want to represent. Figure 1.1 is a didactic example that also brings to our perception the fact that for each situation we may have different possible models with different structures and characteristics, with the implication that such models are comparable regarding their capacity to represent reality. Another important point about models is that they are built based on assumptions, e.g, we assume that the structure of the cylinders is not modified when we are creating the models, and that we can represent a 3-D structure with a 2-D figure. When creating models we should be always aware of our assumptions and of the need to check if they are valid in the particular situation we are addressing.\n\n\n\n\n\n\nFigure 1.1: A conceptual example of creation of models. A half cylinder illustrating an instance in reality is approximated by 3 different 2-D representations.\n\n\n\nOnce a model is basically a representation it can be built in several different manners including figures, flowcharts, and with the usage of equations and other statistical mathematical tools. Consider the dataset att_demo2 in Table 1.1 containing observations of 5 patients regarding a heart condition (health or Heart Condition), the level of activity (rest or moderate), the age in years, the heart rate in bpm and saturation per oxygen (spo2). As discussed in the first section of the chapter this dataset is factual evidence about the patients, representing a reality about they health status (their reality statuts), the equivalent to the half cyllinder in our Figure 1.1. For this dataset, we could creat different possible representations depending on our goals, particularly, statistical and mathematical representations. Suppose we want to explain the heart rate and spo2 using the rest of information in the dataset, we could describe this notation with the following notation:\n\\[\n(hr,\\, spO_2) = f(\\text{condition},\\, \\text{activity},\\, \\text{age})\n\\tag{1.1}\\]\nThe model/representation in Equation 1.1 is a specific example of a generic model in which we say that the heart rate and saturation of patients are a function of their heart condition, level of activity and age. Equation Equation 1.1 for now is an example of a mathematical function (we assume no error). We could create another representation like in Equation 1.2, and assume that we also have an error implied in using condition, activity and age to explain, heart rate and spo2, in that case we have a statistical model.\n\\[ (hr,spo2)=f(condition,activity,age) + error  \\tag{1.2}\\]\nMajor part of the rest of this book will be dedicate on different ways to define the function that will be connecting the things we want to explain with we use to explain them.\nIn this section we learnt that models are representations we use to assess a reality, situation or context, passible to be comparable. We also learnt that we can use equations to create models based on datasets.\n\n\n\nTable 1.1: Simulated ATT micro-dataset (n = 5) with heart condition (binary) and age (continuous).\n\n\natt_demo2\n\n  patient_id heart_condition activity age_years heart_rate_bpm spo2_percent\n1        P01         Healthy     Rest      52.4             73         98.9\n2        P02  HeartCondition Moderate      73.1             88         93.4\n3        P03  HeartCondition     Rest      61.2             73         95.8\n4        P04  HeartCondition Moderate      36.0             90         94.0\n5        P05         Healthy     Rest      53.7             72         98.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#decisions",
    "href": "introduction_AI.html#decisions",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.5 Decisions",
    "text": "1.5 Decisions\nIn an analogous manner we will introduce the idea of decision using first an intuitive example. Consider Figure 1.2 that depicts a crossroad in which some different routes can be chosen when someone is travelling and stopping at the stop signal. After the STOP one can turn to the left, turn to the right, go ahead, reverse and come back, etc.\nThe case in the Figure Figure 1.2 brings to our mind some core notions. First we have a state of ambiguity between different routes, each of these routes will bring us to different places, when reaching the crossroad we may know exactly or not the outcomes of each of the routes, etc. Generally speaking we say that we have some ambiguity in the courses of action we can take, because at a single moment we can take only one of them and not all of them at the same time, we have different alternatives of choice, each of them leading to a consequence regarding our journey.\n\n\n\n\n\n\nFigure 1.2: The crossroad for decision learning.\n\n\n\nFormalizing the ideas discussed in the previous paragraph we can say, following (White 1970), that a decision is a cognitive process \\(\\theta\\), clearly identifiable, that resolves a state of ambiguity \\(Q\\) within a state of knowledge \\(K\\), such that \\((Q, K) \\rightarrow q \\in Q\\), where \\(q\\) is a possible alternative within the state of ambiguity. Formalizing the ideas illustrated in the crossroads example, we can express the act of choosing a route as a decision process in White’s sense. In the case of the crossroads, \\(Q\\) corresponds to the set of possible paths available to the traveler, and \\(K\\) represents the traveler’s knowledge or expectations about the consequences of each path perhaps partial, uncertain, or even incorrect. The cognitive process \\(\\theta\\) is the mechanism by which the traveler evaluates this information and selects one route, \\(q\\), from among the available alternatives. Thus, the simple act of deciding which way to go operationalize the theoretical construct of decision-making: resolving ambiguity under a given state of knowledge to produce a concrete action.\nExamples in health care and advanced therapeutic technology include:\n\nSelect a treatment strategy for a patient using clinical history, imaging, genetics, and comorbidities.\nDesign and optimize clinical trials (inclusion criteria, dose/regimen, endpoints, adaptive rules).\nChoose a therapeutic modality (gene therapy, cell therapy, device, pharmacological intervention) for a given indication.\nInitiate/continue/discontinue therapy based on longitudinal biomarkers, response, and adverse events.\nIntegrate multi-omics evidence (genomics, proteomics, metabolomics) to guide precision medicine.\nAllocate scarce resources (ICU beds, organs, specialized equipment) under ethical and operational constraints.\nAssess risk–benefit and cost-effectiveness for adopting a new drug, device, or regenerative technology.\nDetermine optimal dosing/delivery via PK/PD modeling, model-informed precision dosing, or digital-twin simulation.\nPrioritize R&D pipelines based on clinical potential, unmet need, feasibility, and budget impact.\nEvaluate AI decision-support systems for safety, interpretability, bias/robustness, and regulatory compliance.\n\nDifferent theories approach decisions via different methods but can be broadly aggregated in the three main classes depending on how the \\(\\theta\\) process is addressed Bell, Raiffa, and Tversky (1988).\n\n\n\nTable 1.2: Types of approaches to the decision-making process (adapted from Bell, Raiffa, and Tversky (1988).\n\n\n\n\n\n\n\n\n\n\n\nCharacteristics\nNormative Models\nDescriptive Models\nPrescriptive Models\n\n\n\n\nDecision approach (\\(\\theta\\))\nAxiomatic, based on rational principles and expected utility theory.\nDescribe how the decision process actually occurs.\nFocus on how to decide well given cognitive and practical constraints.\n\n\nGeneral dynamics\nHow people should ideally make decisions.\nHow people actually make decisions, including cognitive limitations.\nHow people can act to make better decisions.\n\n\nAreas of use and examples\nMicroeconomics (expected utility paradigm), contingent state insurance.\nCognitive psychology (heuristics, biases, bounded rationality).\nEngineering, risk and uncertainty management, operations research.\n\n\n\n\n\n\nFrom the different approaches proposed in Table 1.2 we will give a particular focus on the prescriptive approaches specifically in one named Decision Analysis.\n\n1.5.1 Decision Analysis\nDecision analysis is a science that develop methods allowing the application of results from decision theory and elements from normative, prescriptive, and descriptive models to the solution of practical problems (Howard 1988). The main objective of this tool is to support the construction of sound decisions, given the cognitive and contextual realities of decision makers. In this sense, the field can also be referred to as decision engineering Sarti (2013).\n\nWhat exactly is the decision problem, and how should it be modeled?\n\nWhat are the possible alternatives, and how can they be identified?\n\nWhich variables should be included in the analysis?\n\nWhat level of detail should the analysis consider?\n\nHow much should be invested, and what is the nature of the information to be used?\n\nHow can the uncertainties involved be modeled?\n\nHow can the decision problem be clearly communicated?\n\nHow can the principles of decision theory be applied to the resolution of practical problems?\n\nHoward (1988) and Sarti (2013) describe the concept of a decision basis as a tool for addressing the questions outlined above and illustrated in Figure . The concept consists of a set of information, denoted as \\(BD\\), in which the alternatives, consequences, and uncertainties or risks associated with a decision are made explicit. The decision basis can be implemented through tables, graphical parameters, or, more formally, by means of influence diagrams and decision trees. The construction of the decision basis should be an iterative process, carried out until sufficient information has been gathered to support a decision. The dynamics of formulating the decision basis and the agents involved in the process are summarized in Figure 1.3. In this figure, we can observe that the process of analysis and construction of the decision basis begins with a state of obscurity regarding the problem.\nFrom that point, the technical experts, the analyst, and the decision makers act in an integrated manner, complementing their expertise with the available literature related to the issue.\nThrough this exchange of knowledge emerges the foundation for building the decision basis, which makes explicit the alternatives, consequences, preferences of those involved for each alternative, and the associated uncertainties. The process should be repeated iteratively until a decision is reached.\n\n\n\n\n\n\nFigure 1.3: Decision Basis\n\n\n\nSpetzler, Winter, and Meyer (2016) further develops the notion of decision basis proposing a whole cycle of decision quality as implemented via six elements: 1) Appropriate framing, 2) Creative Alternatives, 3) Relevant and Reliable Information, 4) Clear Values and Trade Offs, 5) Sound Reasoning and 6) Commitment to Action. Going through these six steps when approaching a decision helps us to avoid common pitfalls and biases of judgement.\nStep 3 in the last sequence involves the usage of relevant and reliable information, implying in the need for data and good representations of data, therefore, good modelling strategies, for achieving good quality decisions. This step is the conceptual motivation of models as decision support tools and main practical motivation for our course.\n\n\n\n\n\n\n\n\nFigure 1.4: Decision Quality elements adapted from Spetzler, Winter & Meyer (2016).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#defining-ml-and-ai",
    "href": "introduction_AI.html#defining-ml-and-ai",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.6 Defining ML and AI",
    "text": "1.6 Defining ML and AI\nIn the last sections of this chapter we had motivated and illustrated the reasons why we need to dedicate our time to learn how to create models or representations of data. We learnt that models are important information providers being key for the quality of decision making. In this section, we will define what is Intelligence, Artificial Intelligence and Machine Learning and show how these techniques can classified regarding the tasks they realize and much more interesting things.\n\n1.6.1 Intelligence\n\n1.6.1.1 Etymological and philosophical perspectives\nEtymologically, the word intelligence derives from the Latin intelligere, meaning “the capacity to comprehend or understand,” formed from inter (“between”) and legere (“to read” or “to choose”). Philosophically, intelligence has often been linked to the capacity for apprehending truth, understood as the correspondence or coincidence between facts and ideas.\nAn interesting nuance in this philosophical view is that intelligence and thought are not identical. We often think without necessarily judging whether something is true or false. Thought can operate freely in imagination or association, whereas intelligence involves an orientation toward truth and understanding. This distinction also separates intelligence from mere skills or procedural abilities, philosophically speaking intelligence implies discernment, not only execution.\n\n\n1.6.1.2 Computer science perspective\nFrom a computational standpoint, machines do not possess intelligence in the philosophical sense. Computers are fundamentally symbol-manipulating systems: they operate on data according to predefined rules, algorithms, and formal structures, without awareness or access to meaning. What we call machine intelligence (as in Artificial Intelligence and Machine Learning) refers to the ability of systems to perform tasks that, for humans, require cognitive effort such as recognizing patterns, making predictions, or generating language. However, these systems do not understand or apprehend truth; they only evaluate consistency or likelihood within the frameworks they are programmed with or have learned statistically.\nIn formal logic or mathematics, a computer can determine whether a statement is derivable or satisfiable according to given axioms but not whether it is “true” in an absolute or semantic sense. In empirical contexts, it can approximate “truth” through probabilistic inference based on data, but that remains a syntactic or statistical notion of truth, not a semantic or philosophical one. Thus, from a computer science perspective, truth is always relative to a system of rules, models, or data, while human intelligence seeks truth as correspondence between thought and reality.\n\n\n1.6.1.3 Artificial Intelligence\n\n1.6.1.3.1 Definition and Scope\nArtificial Intelligence (AI) can be broadly defined as the field of study and practice dedicated to creating systems capable of performing tasks that, if carried out by humans, would require cognitive processes. This includes perception, reasoning, learning, language understanding, decision-making, and problem-solving. AI is not a single technology but a multidisciplinary domain that integrates computer science, mathematics, cognitive psychology, statistics, linguistics, and philosophy. In practical terms, AI systems operate by processing inputs (data) through computational models designed to produce outputs that emulate aspects of human cognition for instance, recognizing an object in an image, predicting the outcome of a treatment, or generating coherent text.\n\n\n\nRelationship between AI and Machine Learning\n\n\nClassic examples of AI applications include:\n\nExpert systems that replicate human decision-making in domains such as medicine or engineering;\nSpeech and image recognition systems such as digital assistants or facial recognition software;\nAutonomous systems such as self-driving vehicles or robotic surgery;\nNatural language processing applications such as machine translation, chatbots, and summarization tools.\n\n\n\n\n1.6.1.4 Machine Learning\nMachine Learning (ML) is a core subfield of AI focused on building models that learn from data. Instead of being explicitly programmed with step-by-step rules, ML methods discover patterns, relationships, or functions from examples, allowing them to make predictions, classifications, or decisions when confronted with new data. The building block of machine learning methods are algorithms that consist in sets of rules and steps that method needs to do to performe a task. In short we can also say that machine learning is a context in which a machine learns to perform a task in the presence of data and improves its performance the more data we offer.\n\n\n\nTypes of Machine Learning\n\n\nIn a typical ML workflow, a model is trained on a dataset where both inputs and desired outputs are known (the training phase), and its performance is later evaluated on unseen data (the testing phase). Through iterative optimization, the model adjusts internal parameters to minimize prediction errors.\nExamples of ML applications include:\n\nPredicting disease risk from medical records;\nClassifying tumor images as benign or malignant;\nRecommending personalized treatments or products;\nForecasting energy demand or environmental variables;\nDetecting fraud in financial transactions.\n\nML methods can be supervised (trained with labeled outcomes), unsupervised (discovering structure without labels, e.g., clustering), or reinforcement-based (learning by trial and reward) ?fig-ml_classes. We will discuss what these terms mean when detailing the anatomy of a ML method in a future section.\n\n\n1.6.1.5 Predictive and Generative AI\nThe recent expansion of AI can be viewed through two complementary paradigms: predictive AI and generative AI.\nPredictive AI focuses on forecasting outcomes or estimating probabilities based on existing data. It aims to answer questions such as “What will happen next?” or “What is the most likely class or value?”. In our course predictive AI will be sometimes referred merely as Machine Learning and vice versa.\nExamples:\n\nPredicting patient readmission risk in healthcare;\nForecasting market behavior or demand in economics;\nAnticipating equipment failure in industrial systems.\n\nPredictive models rely on statistical and ML techniques (e.g., regression, decision trees, ensemble methods, neural networks) to generalize patterns from past data to future cases.\nGenerative AI, in contrast, focuses on creating new data that resemble observed patterns. These systems learn the underlying structure of their training data and then generate plausible new instances, such as text, images, code, or molecular structures.\nExamples:\n\nLarge Language Models (LLMs) that produce human-like text;\nGenerative Adversarial Networks (GANs) that create realistic images;\nVariational Autoencoders (VAEs) that synthesize complex data distributions;\nText-to-image models used for design, art, and simulation.\n\nGenerative AI represents a major shift from systems that merely predict to systems that can synthesize and imagine, extending AI from decision support to creative collaboration.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#bridging-predictive-and-generative-paradigms",
    "href": "introduction_AI.html#bridging-predictive-and-generative-paradigms",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.7 Bridging Predictive and Generative Paradigms",
    "text": "1.7 Bridging Predictive and Generative Paradigms\nWhile predictive AI is predominantly analytical, generative AI is synthetic. In practice, modern intelligent systems often integrate both: generative models can simulate scenarios for decision-making, while predictive models can assess the likelihood or impact of those simulated outcomes.\nIn the context of Advanced Therapeutic Technologies (ATT) and health sciences, this convergence allows AI not only to anticipate risks and outcomes but also to propose new molecules, optimize treatment protocols, and design personalized therapeutic strategies.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#more-on-models-for-data-and-anatomy-of-an-ai-method",
    "href": "introduction_AI.html#more-on-models-for-data-and-anatomy-of-an-ai-method",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.8 More on Models for data and anatomy of an AI method",
    "text": "1.8 More on Models for data and anatomy of an AI method\nWe will use again the dataset in Table 1.1 to illustrate a general scheme we will use for the rest of the book and to introduce some other key terms and jargon.\n\n\n  patient_id heart_condition activity age_years heart_rate_bpm spo2_percent\n1        P01         Healthy     Rest      52.4             73         98.9\n2        P02  HeartCondition Moderate      73.1             88         93.4\n3        P03  HeartCondition     Rest      61.2             73         95.8\n4        P04  HeartCondition Moderate      36.0             90         94.0\n5        P05         Healthy     Rest      53.7             72         98.0\n\n\nEach row in a dataset is called an instance, and each column represents a feature (or variable). Depending on the goal of the analysis, some features can be used as predictors (inputs), or explanatory variables, and others as responses (outputs) or responses. Sometimes we may refer to a variable as a label, the known value of the output (response) that we want a model to predict.\nOur dataset example can be viewed from a modelers point of view line in Figure 1.5.\n\n\n\n\n\n\nFigure 1.5: The way modellers see a dataset!\n\n\n\nIn the case each Y in Figure 1.5 is modelled alone we have an uni-response model and in the case they are modeled simultaneously we say it is multi-response.\nMachine learning tasks are classified in supervised, unsupervised, reinforcement and generative tasks. All methods share a general anatomy consisted of the data and motivational context, the task the algorithms perform, and measures of performance. Whenever studying a method we will use always describe it in this context of data, algorithm and performance.\n\n\n\n\n\n\nFigure 1.6: Anatomy of Machine Learning. All methods require data to be implemented, different algorithms as function of the kind of task. All methods need to have their performance evaluated via metrics.\n\n\n\nThe features are also classified into\n\n\n\n\n\n\n\n\n\nFeature Type\nDefinition\nExample from Model\nExplanation\n\n\n\n\nCategorical\nVariables that represent distinct groups or classes with no inherent order.\ncondition = {Healthy, HeartCondition}; activity = {Rest, Moderate}\nThese describe qualitative differences between individuals. One level is not “more” or “less” than another only different. Encoded typically as dummy variables (0/1).\n\n\nOrdinal\nVariables that have a clear, ranked order, but where differences between levels are not necessarily uniform.\ne.g., if activity were coded as {Rest &lt; Moderate &lt; Vigorous}\nHere, the order matters “Moderate” implies higher exertion than “Rest” but the distance between levels is undefined. These often appear in surveys or clinical scoring scales (e.g., pain level: mild &lt; moderate &lt; severe).\n\n\nContinuous\nVariables measured on a numeric scale with meaningful and consistent intervals.\nage (in years)\nThe differences are measurable: a 10-year increase has the same quantitative meaning regardless of where it occurs (e.g., 40→50 vs 60→70). Continuous variables allow arithmetic operations and smooth modeling (e.g., linear effects).\n\n\n\nIn the simulated dataset:\n\nheart_condition and activity are categorical.\nage_years is continuous.\nIf an ordered variable (like “disease severity” or “activity level”) were added, it would be ordinal.\n\n\n1.8.0.0.1 Supervised Learning\nUses labeled data: inputs (features) + known output (label).\n\nGoal: learn a mapping from features → label.\nExample: Predict heart_condition (Healthy vs HeartCondition) from activity, age_years, heart_rate_bpm, spo2_percent\nTypical algorithms: Logistic regression, random forest, SVM, neural networks\nOutput: class label or numeric value (classification/regression).\n\n\n\n1.8.0.0.2 Unsupervised Learning\nUses unlabeled data: only features, no target.\n\nGoal: find structure/patterns (clusters, components).\n\nExample: Cluster patients by heart_rate_bpm, spo2_percent, age_years to reveal low/high-risk profiles without using heart_condition.\n\nTypical algorithms: k-means, hierarchical clustering, Gaussian mixtures, PCA.\n\nOutput: groupings, embeddings, or density estimates.\n\n\n\n1.8.0.0.3 Reinforcement Learning\nAn agent interacts with an environment and learns via rewards/penalties over time.\n\nGoal: learn a policy that maximizes long-term reward.\n\nExample: A wearable adjusts activity level to keep spo2_percent and heart_rate_bpm in range; rewards for healthy ranges, penalties otherwise.\n\nTypical algorithms: Q-learning, DQN, policy gradients.\n\nOutput: policy for sequential decisions.\n\n\n\n1.8.0.0.4 Generative Learning\nLearns the data distribution to create new, realistic samples.\n\nGoal: model (p()) and generate synthetic data.\n\nExample: Create synthetic patient records with plausible heart_rate_bpm/spo2_percent/activity combinations for testing or augmentation.\n\nTypical algorithms: VAEs, GANs, transformer-based models.\n\nOutput: new samples; also used for simulation, augmentation, imputation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#introduction-to-model-comparison",
    "href": "introduction_AI.html#introduction-to-model-comparison",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.9 Introduction to model comparison",
    "text": "1.9 Introduction to model comparison\nSo far we have learnt that models are representations of a given reality, which in AI contexts, relate to supervised, unsupervised, reinforcement and generative tasks. Intuitively, we also learnt from Figure 1.1 that for a same given reality we can have different models. In this section we will address the matter of how to compare the different possible machine learning methods for the same situation.\nIn our efforts, we will consider that the best model is the one that most accurately learns and represents the reality it is intended to approximate. To assess this, we rely on performance metrics that quantify how closely a model’s outputs align with the true or expected outcomes.\nIn supervised learning, these metrics measure the discrepancy between predicted and observed values  for instance we use concepets like accuracy, precision–recall, ROC-AUC for classification, or RMSE and MAE for regression. In unsupervised learning, where there are no predefined labels, we instead evaluate how well the model captures underlying structure, using metrics such as silhouette score, cluster purity, or reconstruction error.\nIn reinforcement learning, performance is gauged by the cumulative reward obtained through interaction with the environment, reflecting how well the agent learns optimal decisions over time.\nFinally, in generative modeling, evaluation focuses on fidelity, diversity, and realism of the generated data, often through metrics like FID (Fréchet Inception Distance), likelihood, or domain-specific validation.\nIn the next chapters when discussing the different methods we will always refer to metrics of model performance and how they could be used to select best models for a given situation.\nWe also can compare models regarding they usage for inference and for answering research questions.\nIn general we should advocate for a balance between predictive performance and interpretability x inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#understanding-the-aiml-predictive-pipeline",
    "href": "introduction_AI.html#understanding-the-aiml-predictive-pipeline",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.10 Understanding the AI/ML Predictive Pipeline",
    "text": "1.10 Understanding the AI/ML Predictive Pipeline\nFigure 1.7 illustrates a simplified version of the predictive pipeline commonly used in Artificial Intelligence and Machine Learning. Although implementations may vary across domains and algorithms, most supervised learning workflows follow the same fundamental structure: a training stage, in which the model learns patterns from data, and a deployment stage, in which the trained model is applied to new cases.\n\n1.10.1 Training Phase\nThe process begins with a dataset containing observations of several variables. In the example, each row corresponds to a patient and includes both:\nPredictors (or features): such as heart_condition, activity, and age_years\nResponses (or labels): such as heart_rate_bpm and spo2_percent\nTo evaluate how well a model can generalize, the dataset is divided into two distinct subsets:\nTraining set\n\nThis subset is used to fit the model. It provides the examples from which the algorithm learns the relationship between predictors and responses. During this stage, the model adjusts its internal parameters to capture regularities in the training data.\nTest set\n\nThis subset is intentionally withheld during model fitting. It serves as an independent benchmark to assess the model’s performance on new, unseen data. By comparing predicted values with the true responses in the test set, we obtain an estimate of the model’s ability to generalize beyond the data from which it learned.\nThis separation between training and testing is essential; without it, the evaluation would be overly optimistic and would fail to indicate whether the model can perform reliably outside the initial dataset.\nOnce a model has been trained and its performance has been verified, it can be deployed. Deployment refers to the use of the model in real or simulated operational settings, where it is asked to make predictions for entirely new instances.\nIn the example, new patients (P06 to P100) arrive with predictor information (e.g., heart condition, activity, age), but their outcomes are unknown. The trained model processes these inputs and generates predictions for variables such as expected heart rate. These predictions illustrate the core function of machine learning models: using previously learned structure to inform decision-making in new contexts.\n\n\n\n\n\n\nFigure 1.7: The pipeline of developing AI/ML solutions\n\n\n\nIf well trained a models works well in new unseen data in other terms this models generalizes well.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#a-very-short-introduction-to-model-bias-overfitting-and-crossvalidation",
    "href": "introduction_AI.html#a-very-short-introduction-to-model-bias-overfitting-and-crossvalidation",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.11 A very short introduction to model bias, overfitting and crossvalidation",
    "text": "1.11 A very short introduction to model bias, overfitting and crossvalidation\nWhen we build a model, the goal is to approximate reality well enough to make useful predictions and inferences. Models must generalize from the data they see to new data they have not seen. Three core ideas help us judge this: bias, variance/overfitting, and cross-validation.\n\n1.11.1 Model bias\nBias is the systematic difference between a model’s predictions and the true values. High-bias models make strong simplifying assumptions and typically under fit the data, missing important structure, see Figure 1.9.\nExample: predicting heart rate using only age while ignoring activity level and heart condition is too rigid, so real patterns are missed.\n\n\n1.11.2 Variance and overfitting\nVariance is a model’s sensitivity to the specific training set. High-variance models are very flexible; they fit random noise as if it were signal and thus overfit great training performance, poor test performance.\nExample: a highly tuned model that perfectly matches small fluctuations in five patients’ heart-rate/SpO₂ readings but fails on a sixth.\n\n\n\n\n\n\nFigure 1.8\n\n\n\n\n\n1.11.3 Cross-validation, training and test datasets\nCross-validation estimates how well a model will generalize. Instead of training and testing on the same data, we split the dataset into several folds. We train on some folds and test on the remaining fold, then rotate until every fold has served as a test set.\nCommon choices:\n\nk-fold cross-validation: split into k parts (for example, k = 5). Average the metric across folds for a stable estimate.\nLeave-one-out (LOOCV): useful for very small datasets; train on all but one observation and test on the left-out one, repeating for all observations.\n\n\n\n\n1.11.4 Importance of cross-validating\nBias, overfitting, and cross-validation address the same question: is the model learning meaningful signal rather than noise? A good model balances flexibility (to capture structure) with restraint (to avoid noise). Cross-validation provides an honest estimate of out-of-sample performance before using models to inform real decisions in clinical, laboratory, or policy settings.\n\n\n\n\n\n\n\n\nFigure 1.9: Three situations side-by-side: Good fit (well-tuned), High bias (underfit), and Overfit (too complex). The dashed line is the true signal.\n\n\n\n\n\n\n\n1.11.5 Causal and non causal machine learning\nMachine learning models can be grouped according to how they treat the data-generating process (DGP). The main difference between causal and non-causal approaches lies in whether the model tries to represent how the data were produced or only what patterns appear in them.\n\n1.11.5.1 Non-causal (Associational) Machine Learning\nNon-causal machine learning assumes that the available data contain stable relationships between variables but does not try to represent the mechanisms that produced those data. Algorithms such as linear regression, random forest, and neural networks are trained to find statistical associations that best predict an outcome (Y) from a set of inputs (X):\n[ = f(X) ]\nThe goal is to minimize prediction error. However, the model ignores the process that generated (X) and (Y). If the environment changes, for example when an intervention or policy modifies the DGP, these models may perform poorly. This situation is known as a distributional shift or a non-stationarity problem.\n\n\n1.11.5.2 Causal Machine Learning\nCausal machine learning explicitly represents the mechanisms that generate the data. It seeks to estimate how outcomes would change if a variable were intervened upon, moving from correlation toward causal inference. This requires assumptions about the DGP, which are often expressed using a causal graph or a structural causal model (SCM):\n[ Y = f(X, U) ]\nwhere (U) represents unobserved causes. Causal machine learning answers questions such as: “What would happen to (Y) if (X) were changed while keeping everything else constant?”\n\n\n\n\n\n\n\n\nAspect\nNon-causal ML\nCausal ML\n\n\n\n\nMain goal\nAccurate prediction\nEstimation of causal effects\n\n\nData used\nObserved variables\nObserved and counterfactual variables\n\n\nDGP considered\nImplicit or ignored\nExplicitly modeled\n\n\nOutput\n( = f(X) )\nEffect of (X) on (Y)\n\n\nLimitations\nSensitive to confounding and bias\nRequires strong causal assumptions such as no hidden confounders\n\n\n\n\n\n1.11.5.3 Associational pattern without explicit DGP\n\n\n\n\n\nflowchart LR\n  X[X features] --&gt; Y[Outcome Y]\n  style X fill:#e8f1ff,stroke:#2f63c0\n  style Y fill:#e8f1ff,stroke:#2f63c0\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\nZ[Confounder Z] --&gt; X[Exposure X]\nZ --&gt; Y[Outcome Y]\nX --&gt; Y\nstyle X fill:#e8f1ff,stroke:#2f63c0\nstyle Y fill:#e8f1ff,stroke:#2f63c0\nstyle Z fill:#fff4d6,stroke:#c0902f\n\n\n\n\n\n\nThe first diagram at the top represents a purely associational pattern.\nIt shows that we can find a statistical relationship between a set of predictors ( X ) (or features) and an outcome ( Y ), even without knowing how these data were generated.\nThis is the standard setup of non-causal machine learning, where the goal is to learn a predictive mapping:\n[ = f(X) ]\nSuch models can predict ( Y ) given ( X ), but they do not explain why the relationship exists.\nIf the data-generating process changes (for example, after a new treatment policy or environmental shift), the learned pattern might no longer hold.\nThe second diagram introduces a causal structure with a confounder ( Z ) that affects both the exposure ( X ) and the outcome ( Y ).\nThis setup represents a situation where the association between ( X ) and ( Y ) is partly due to a third variable ( Z ) that influences both.\nTo correctly identify the effect of ( X ) on ( Y ), we must account for ( Z ).\nIf ( Z ) is not measured or controlled for, the observed relationship between ( X ) and ( Y ) can be biased this is known as confounding.\nInterpretation.\nTo estimate the true causal effect of ( X ) on ( Y ), the confounder ( Z ) must be included in the model.\nIgnoring ( Z ) can lead to misleading conclusions about the direction or strength of the effect.\nTakeaway.\n- Non-causal machine learning captures what tends to occur together.\n- Causal machine learning aims to estimate what would happen if we changed something, considering the data-generating process and possible confounders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#coding-and-version-control",
    "href": "introduction_AI.html#coding-and-version-control",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.12 Coding and Version Control",
    "text": "1.12 Coding and Version Control\nIn other to perform tasks machine learning algorithms and methods need to be implemented in computers. To communicate to the machine what it is necessary to do we use programming languages in the form of code. Below in Table 1.3 we have an example of code that creates a vector of numbers, calculates the averages of such numbers, prints a message and creates a simulated data frame.\n\n\n\nTable 1.3: A tiny example of coding\n\n\n# 1) Create a small numeric vector\nx &lt;- c(72, 75, 70, 78, 74)\n\n# 2) Compute a simple summary\navg &lt;- mean(x)\n\n# 3) Print a message that mixes text and a computed value\ncat(\"The average value is:\", avg, \"\\n\")\n\nThe average value is: 73.8 \n\n# 4) Make a tiny data frame and show the first rows\ndf &lt;- data.frame(id = 1:5, value = x)\nhead(df)\n\n  id value\n1  1    72\n2  2    75\n3  3    70\n4  4    78\n5  5    74\n\n\n\n\nWhen creating code we usually start with an initial version which often is modified or adapted, created by a single individual or by several people’s contributions which are using the same machines or not, in the same place or not among other details. These dynamics bring the need for mechanism that facilitate the control of code version, allowing for remote contributions and collaborative creation of code which are very well implemented in version control technologies like Git.\nGit is a version control system (VCS) designed to track changes in code and other text files over time. It allows developers to record, compare, and revert modifications, ensuring that every update is safely stored and that collaboration does not overwrite previous work. Each version of your project is stored as a commit, which captures the exact state of all tracked files at a given point in time, along with a descriptive message about what changed.\nWhile Git works locally on your computer, platforms such as GitHub, GitLab, and Bitbucket provide remote repositories, online spaces that store your project’s history, enable backup, and make team collaboration possible from anywhere.\nWe recommend the readers to set up a github account, install git in their system and integrate git with R and Rstudio.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#set-up-git-github-for-rrstudio-workflows",
    "href": "introduction_AI.html#set-up-git-github-for-rrstudio-workflows",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.13 Set up Git & GitHub (for R/RStudio workflows)",
    "text": "1.13 Set up Git & GitHub (for R/RStudio workflows)\nWe recommend that readers set up a GitHub account, install Git, and integrate it with R and RStudio to make their workflows versioned, collaborative, and reproducible.\n\n1.13.1 1) Create a GitHub account\n\nGo to https://github.com and create a free account.\nPick a concise username (becomes part of your repo URLs).\n(Optional) Enable 2FA: Settings → Password and authentication.\nLearn key concepts: repository, commit, branch, pull request.\n\n\n\n1.13.2 2) Install Git\nCheck if Git is already available:\ngit --version\nIf not, install it for your OS:\nWindows\n\nDownload and install: https://git-scm.com/download/win\n(or via Winget):\nwinget install --id Git.Git -e\n\nmacOS\n\nEasiest (Apple CLT):\nxcode-select --install\n(Optional) With Homebrew:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew install git\n\nLinux (Debian/Ubuntu)\nsudo apt update\nsudo apt install -y git\n\n\n1.13.3 3) Configure Git (one-time)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\ngit config --global init.defaultBranch main\n# Optional:\ngit config --global pull.rebase false   # use merge on pull\ngit config --global core.autocrlf input # good default on macOS/Linux (use \"true\" on Windows)\n\n\n1.13.4 4) Authenticate with GitHub (Personal Access Token)\nFrom R (recommended):\ninstall.packages(c(\"usethis\",\"gitcreds\"))\nusethis::create_github_token()  # opens browser to create a PAT (scopes: repo, read:org, workflow)\ngitcreds::gitcreds_set()        # paste the token once\ngitcreds::gitcreds_get()        # verify stored token\nTip: If you’re prompted for a “password” during git push, paste the PAT (not your GitHub password).\n\n\n1.13.5 5) Enable Git in RStudio\n\nRStudio → Tools → Global Options → Git/SVN\n\nEnsure Git executable is detected (browse to it if needed).\nTick Enable version control interface.\n\nClick Apply (restart RStudio if prompted).\n\n\n\n1.13.6 6) Create a repo and push to GitHub\nOption A via RStudio UI\n\nFile → New Project → choose a directory → tick Create a git repository.\nMake a change → Git tab → Commit → Push.\n\nOption B via terminal (works on both macOS and Windows)\n# Run inside your project folder\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin https://github.com/&lt;youruser&gt;/&lt;repo&gt;.git\ngit push -u origin main\n\n\n1.13.7 7) Daily workflow\ngit pull                      # get others' changes\ngit add .                     # stage your changes\ngit commit -m \"Meaningful message\"\ngit push                      # publish your work\n\n\n1.13.8 8) (Optional) Use SSH instead of HTTPS\nGenerate a key and add it to GitHub:\nmacOS/Linux\nssh-keygen -t ed25519 -C \"you@example.com\"\ncat ~/.ssh/id_ed25519.pub\nWindows (PowerShell)\nssh-keygen -t ed25519 -C \"you@example.com\"\ntype $env:USERPROFILE\\.ssh\\id_ed25519.pub\n\nCopy the .pub key → GitHub → Settings → SSH and GPG keys → New SSH key.\nPoint your repo to SSH:\n\ngit remote set-url origin git@github.com:&lt;youruser&gt;/&lt;repo&gt;.git\n\n\n1.13.9 9) Quick fixes\n\n“git: command not found” (macOS) → run xcode-select --install or brew install git.\nRStudio doesn’t see Git → set Git path in Tools → Global Options → Git/SVN.\nDivergent branches on pull → choose one:\ngit pull --no-rebase   # merge (default if configured)\n# or\ngit pull --rebase      # rebase to keep linear history\nToken expired → in R:\ngitcreds::gitcreds_set()\nWrong email in commits → update config and amend latest commit:\ngit config --global user.email \"you@newmail.com\"\ngit commit --amend --reset-author\ngit push --force-with-lease\n\n\n\n1.13.10 A minimal portfolio structure for data professionals\nEvery craft needs the right setup: carpenters need a bench and tools; data professionals need a basic, reliable workspace. If you’re starting in AI/ML and data science, invest early in a lean, well-organized portfolio. The simplest and most effective format is a set of GitHub repositories that showcase your work each with clear documentation, runnable code, and reproducible results. Over time, this portfolio becomes your toolkit and your calling card: easy to share, easy to maintain, and easy for others to trust.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#reproducibility",
    "href": "introduction_AI.html#reproducibility",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.14 Reproducibility",
    "text": "1.14 Reproducibility\n\nAs discussed in the previous sections of this chaoter, Data are our evidence about the world; models turn that evidence into information for decision-making. Most AI and ML projects follow the same steps: collect and organize data, build a model, evaluate performance and communicate results. Across all these stages, one principle matters above all: reproducibility, defined as the ability for others (or your future self) to repeat the process and obtain the same results for learning, auditing, or verification.\nReproducibility is best achieved with a minimal, transparent workflow: make data (when permissible) and code available so that someone else can rerun the analysis and arrive at the same outputs. Version control (e.g., Git) is essential here. Whenever data governance, intellectual property, and legal constraints allow, we recommend sharing both data and code to support reproducible science.\nIn health applications, reproducible analyses also underpin evidence-based regulation. National authorities issue guidelines for approving drugs, procedures, and other therapeutic technologies, typically grounded in literature and empirical evidence. Researchers and developers are therefore frequently called upon to produce transparent, well-documented analyses that regulatory boards can evaluate and trust.\n\n\n\n\nBell, David E., Howard Raiffa, and Amos Tversky, eds. 1988. Decision Making: Descriptive, Normative, and Prescriptive Interactions. Cambridge: Cambridge University Press. https://www.cambridge.org/core/books/decision-making/D29E12748C31B85FE83C9CEDEE9D4D88.\n\n\nHasin, Yoram, Michael Seldin, and Aldons Lusis. 2017. “Multi-Omics Approaches to Disease.” Genome Biology 18: 83. https://doi.org/10.1186/s13059-017-1215-1.\n\n\nHilbert, Martin, and Priscila López. 2011. “The World’s Technological Capacity to Store, Communicate, and Compute Information.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nHoward, Ronald A. 1988. “Decision Analysis: Practice and Promise.” Management Science 34 (6): 679–95.\n\n\nHripcsak, George, and David J. Albers. 2013. “Next-Generation Phenotyping of Electronic Health Records.” Journal of the American Medical Informatics Association 20 (1): 117–21. https://doi.org/10.1136/amiajnl-2012-001145.\n\n\nJensen, Peter B., Lars J. Jensen, and Søren Brunak. 2012. “Mining Electronic Health Records: Towards Better Research Applications and Clinical Care.” Nature Reviews Genetics 13 (6): 395–405. https://doi.org/10.1038/nrg3208.\n\n\nKarczewski, Konrad J., and Michael P. Snyder. 2018. “Integrative Omics for Health and Disease.” Nature Reviews Genetics 19: 299–310. https://doi.org/10.1038/nrg.2018.4.\n\n\nRaghupathi, Wullianallur, and Viju Raghupathi. 2014. “Big Data Analytics in Healthcare: Promise and Potential.” Health Information Science and Systems 2 (3): 1–10. https://doi.org/10.1186/2047-2501-2-3.\n\n\nSarti, Danilo Augusto. 2013. “Uncertainty Management Through Decision Analysis: Applications to Production Optimization and Uncertain Demands.” PhD thesis, Universidade de São Paulo.\n\n\nSpetzler, Carl, Hannah Winter, and Jennifer Meyer. 2016. Decision Quality: Value Creation from Better Business Decisions. Hoboken, NJ: John Wiley & Sons.\n\n\nStephens, Zachary D., Skylar Y. Lee, Faraz Faghri, R. Ilkayeva Campbell, et al. 2015. “Big Data: Astronomical or Genomical?” PLOS Biology 13 (7): e1002195. https://doi.org/10.1371/journal.pbio.1002195.\n\n\nTopol, Eric J. 2019. Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again. New York: Basic Books.\n\n\nWhite, Douglas John. 1970. Decision Theory. New Brunswick, NJ: Routledge / Transaction Publishers. https://www.routledge.com/Decision-Theory/White/p/book/9780202308982.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bell, David E., Howard Raiffa, and Amos Tversky, eds. 1988. Decision\nMaking: Descriptive, Normative, and Prescriptive Interactions.\nCambridge: Cambridge University Press. https://www.cambridge.org/core/books/decision-making/D29E12748C31B85FE83C9CEDEE9D4D88.\n\n\nHasin, Yoram, Michael Seldin, and Aldons Lusis. 2017. “Multi-Omics\nApproaches to Disease.” Genome Biology 18: 83. https://doi.org/10.1186/s13059-017-1215-1.\n\n\nHilbert, Martin, and Priscila López. 2011. “The World’s\nTechnological Capacity to Store, Communicate, and Compute\nInformation.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nHoward, Ronald A. 1988. “Decision Analysis: Practice and\nPromise.” Management Science 34 (6): 679–95.\n\n\nHripcsak, George, and David J. Albers. 2013. “Next-Generation\nPhenotyping of Electronic Health Records.” Journal of the\nAmerican Medical Informatics Association 20 (1): 117–21. https://doi.org/10.1136/amiajnl-2012-001145.\n\n\nJensen, Peter B., Lars J. Jensen, and Søren Brunak. 2012. “Mining\nElectronic Health Records: Towards Better Research Applications and\nClinical Care.” Nature Reviews Genetics 13 (6): 395–405.\nhttps://doi.org/10.1038/nrg3208.\n\n\nKarczewski, Konrad J., and Michael P. Snyder. 2018. “Integrative\nOmics for Health and Disease.” Nature Reviews Genetics\n19: 299–310. https://doi.org/10.1038/nrg.2018.4.\n\n\nRaghupathi, Wullianallur, and Viju Raghupathi. 2014. “Big Data\nAnalytics in Healthcare: Promise and Potential.” Health\nInformation Science and Systems 2 (3): 1–10. https://doi.org/10.1186/2047-2501-2-3.\n\n\nSarti, Danilo Augusto. 2013. “Uncertainty Management Through\nDecision Analysis: Applications to Production Optimization and Uncertain\nDemands.” PhD thesis, Universidade de São Paulo.\n\n\nSpetzler, Carl, Hannah Winter, and Jennifer Meyer. 2016. Decision\nQuality: Value Creation from Better Business Decisions. Hoboken,\nNJ: John Wiley & Sons.\n\n\nStephens, Zachary D., Skylar Y. Lee, Faraz Faghri, R. Ilkayeva Campbell,\net al. 2015. “Big Data: Astronomical or Genomical?”\nPLOS Biology 13 (7): e1002195. https://doi.org/10.1371/journal.pbio.1002195.\n\n\nTopol, Eric J. 2019. Deep Medicine: How Artificial Intelligence Can\nMake Healthcare Human Again. New York: Basic Books.\n\n\nWhite, Douglas John. 1970. Decision Theory. New Brunswick, NJ:\nRoutledge / Transaction Publishers. https://www.routledge.com/Decision-Theory/White/p/book/9780202308982.",
    "crumbs": [
      "References"
    ]
  }
]