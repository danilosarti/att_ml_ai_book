[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning and AI for Health and Sciences",
    "section": "",
    "text": "Preface\nMachine Learning and Artificial Intelligence have rapidly moved from specialized technical domains into central tools for discovery, decision support, and innovation across the health and life sciences. From clinical research and therapeutic development to genomics, imaging, and population health, these methods are now deeply embedded in how evidence is generated, interpreted, and acted upon. Yet, for many students and professionals working in these fields, the conceptual foundations, assumptions, and limitations of modern AI/ML methods often remain opaque.\nThis beta version of this book is being written to address that gap.\nIntroduction to Machine Learning and Artificial Intelligence for Health and Life Sciences is conceived as a living, evolving resource that provides a structured, concept-driven introduction to AI and ML, grounded in the realities of biomedical and translational research. The version you are reading represents a beta release of the book. Its content will continue to expand, be refined, and adapt over time as the field itself evolves, new methods mature, and pedagogical insights emerge from teaching and practice.\nRather than focusing on algorithms in isolation, the book emphasizes how models are built, evaluated, interpreted, and ultimately used as tools for scientific reasoning and decision-making in health-related contexts. The early chapters introduce core ideas—what distinguishes machine learning from traditional statistical modeling, how supervised learning methods such as regression, tree-based models, and neural networks operate, and what their strengths and limitations are in applied settings. Subsequent chapters expand into Bayesian perspectives, missing-data mechanisms, and high-dimensional data challenges, reflecting common realities of biomedical datasets. Unsupervised learning methods, including clustering and dimension reduction, are presented not as exploratory afterthoughts, but as essential tools for structure discovery and hypothesis generation.\nA central theme throughout the book is interpretability, trust, and responsible use. Models applied in health and life sciences do not exist in a vacuum: they interact with human judgment, regulatory frameworks, ethical constraints, and real clinical consequences. For this reason, interpretability, bias, and transparency are treated as first-class methodological concerns rather than optional additions. The final chapters introduce generative AI and large language models, situating them within a broader framework of realistic capabilities, limitations, and appropriate use in scientific and clinical workflows.\nAs a beta version, this book should be read in the spirit of an open and iterative work. Examples, explanations, and emphases may be refined over time, and new chapters or sections may be added as methods, tools, and best practices evolve. Readers are encouraged to approach the material critically, not only learning how models work, but reflecting on when they should be used, how their outputs should be interpreted, and where their limitations lie.\nThis book is intended for advanced undergraduate and postgraduate students in health, biomedical, pharmaceutical, and life-science disciplines, as well as researchers and practitioners seeking a principled introduction to AI and ML. It assumes basic quantitative literacy but does not require prior expertise in machine learning. Mathematical details are introduced only when they enhance understanding, with emphasis placed on intuition, assumptions, and consequences rather than formalism for its own sake.\nUltimately, the goal of this book is not simply to teach techniques, but to cultivate informed, reflective, and responsible users of AI and machine learning in the health and life sciences—recognizing that both the field and this book will continue to evolve together.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction_AI.html",
    "href": "introduction_AI.html",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "",
    "text": "1.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#data-era",
    "href": "introduction_AI.html#data-era",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.2 Data era",
    "text": "1.2 Data era\nThis is an introductory chapter about data and data modelling for evidence based actions, decisions and policy making with a particular interest in advanced therapeutic technologies. Data are representations of facts, observations, or measurements structured or unstructured that describe the attributes or states of objects, events, or phenomena. They are symbols or signals without inherent meaning until interpreted within a context or combined with other information. In scientific and computational contexts, data are typically considered the raw inputs from which information and knowledge are derived through processes of organization, analysis, and interpretation. Data can assume several different forms including, numeric readings from a sensor, characters in a database, or pixels in an image, text in a message, electronic information, sequence of bases in DNA and RNA, etc. Data are evidence of our immediate reality and contexts therefore being crucial for fact based argumentation.\nWe live in an era of explosion in the availability of data, driven by the rapid expansion of the internet, advances in computational power, and the massive growth of digital storage and connectivity devices . Over recent decades, the cost of computing and storage has plummeted while the ability to collect, transmit, and analyze information has risen exponentially; in many domains, the data generated in the last few years surpass those produced in all prior human history (Hilbert and López 2011; Stephens et al. 2015). Historical analyses of data production and storage capacity show orders-of-magnitude increases year over year, reshaping how science, industry, and society operate, imposing almost naturally a new data driven paradigm (Hilbert and López 2011).\nA particular example of transformative shift is the omics era in the life sciences, a period in which substantial amount of data has been produced in genomics, transcriptomics, proteomics, metabolomics, implying in massive, complex datasets capturing biological systems in high resolution (Stephens et al. 2015; Hasin, Seldin, and Lusis 2017). Similar surges occur across disciplines from astronomy to agriculture and environmental monitoring where instruments and sensors continuously collect structured and unstructured data at scales that exceed traditional analytical capacities (Stephens et al. 2015).\nIn health care, data streams such as electronic health records (EHR), laboratory tests, medical imaging, wearable sensors, and omics profiles create a multidimensional view of patients (Jensen, Jensen, and Brunak 2012; Hripcsak and Albers 2013). The later kind of view can be integrated and enhanced with the possibilities of contrasting omics and biological data with other sources including social economic information bringing an opportunity for new solutions in health care.\nA particular challenge and frontier of data era is the need for integration and to fuse heterogeneous sources into coherent, trustworthy insights for predictive modeling, personalized treatment, and clinical decision support (Raghupathi and Raghupathi 2014; Karczewski and Snyder 2018; Jensen, Jensen, and Brunak 2012). Such an integration can enhance outcomes while maintaining safety, fairness, and transparency (Topol 2019) when done responsibly.\nSeveral concepts, and ideas can help us to understand and navigate this complex landscape of data sourcing, storing, processing and modelling for evidence actions. The rest of this chapter will cover these topics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#sources-of-data",
    "href": "introduction_AI.html#sources-of-data",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.3 Sources of data",
    "text": "1.3 Sources of data\nAI and machine-learning methods depend fundamentally on the quality and nature of the data used to train them. In health sciences and particularly in the field of Advanced Therapeutic Technologies (ATT), data can emerge from a wide range of processes, yet most of them can be meaningfully grouped into two broad classes: observational and experimental. Understanding the differences between these types is essential because each one carries specific strengths, limitations, and implications for modelling and decision-making.\nObservational data arise from situations in which no intervention is imposed by the researcher; instead, information is simply recorded as it naturally occurs in clinical or biological settings. Much of modern health data falls into this category. Electronic health records, for instance, routinely capture demographic information, laboratory results, diagnoses, procedures, and medication histories as part of everyday clinical care. Wearable sensors provide continuous streams of physiological measurements such as heart rate, oxygen saturation, sleep quality, or activity levels, reflecting the patient’s lived environment rather than a controlled experiment. Medical imaging repositories CT, MRI, ultrasound, histopathology are also observational in nature, as they document the state of the patient without manipulating it. Registries, whether for cancer, transplantation, rare diseases or adverse events, add another important layer, offering longitudinal and population-level perspectives. In ATT contexts, observational data play a crucial role in understanding real-world performance of new therapeutic technologies, monitoring long-term safety after regulatory approval, and identifying predictors of treatment success or failure within routine clinical practice.\nIn contrast, experimental data are generated in controlled settings where a researcher actively intervenes by assigning treatments, conditions, or exposures according to a predefined design. This category encompasses a wide spectrum of studies, from early in vitro experiments and animal models to human clinical trials. For example, before a new gene or cell therapy reaches patients, it undergoes a series of structured tests designed to quantify biological response, evaluate safety profiles, and characterize potential off-target effects. Controlled laboratory experiments may examine cellular behavior when exposed to a regenerative biomaterial or probe the efficiency of a CRISPR-based gene-editing protocol. Preclinical in vivo studies evaluate therapeutic effects and toxicity under standardized conditions. At the clinical end of the spectrum, randomized controlled trials compare one treatment against another (or against standard care) under carefully regulated environments, providing the most rigorous evidence for causality. These experimental datasets form the backbone for regulatory assessments in ATT, supporting decisions related to trial approval, dosing strategies, safety monitoring, and eventual market authorization.\nAlthough observational and experimental data differ in structure and purpose, they are deeply complementary. Observational data offer a broad, naturalistic view of patient populations and allow researchers to capture the complexity of real-world clinical behavior. Experimental data provide the depth and rigor needed to establish causal relationships and quantify the true therapeutic effect of an intervention. In practice, robust AI and ML applications in health care frequently rely on both. Predictive models developed from observational data may later be validated or even refined using experimental evidence. Similarly, insights from experimental studies often guide the design of models applied to real-world datasets. Together, these data sources provide a foundation on which trustworthy, transparent, and clinically meaningful AI systems can be built.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#models",
    "href": "introduction_AI.html#models",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.4 Models",
    "text": "1.4 Models\nThe definition of what is a model is the first useful conceptual tool we will discuss intuitively with the example depicted in Figure 1.1. In the center right of the Figure we have a cylinder that was cut in half and which could be represented in different manners, including, the obvious half cylinder, but also as triangle, an ellipsis and a semi-ellipsis, depending on the point of view. If we consider the cylinder as the reality we want to represent, we see that we have at least three different possible representations. The word representation has an interesting synonym: model. Thus, models can be understood and seen merely as representations of a given situation we want to represent. Figure 1.1 is a didactic example that also brings to our perception the fact that for each situation we may have different possible models with different structures and characteristics, with the implication that such models are comparable regarding their capacity to represent reality. Another important point about models is that they are built based on assumptions, e.g, we assume that the structure of the cylinders is not modified when we are creating the models, and that we can represent a 3-D structure with a 2-D figure. When creating models we should be always aware of our assumptions and of the need to check if they are valid in the particular situation we are addressing.\n\n\n\n\n\n\nFigure 1.1: A conceptual example of creation of models. A half cylinder illustrating an instance in reality is approximated by 3 different 2-D representations.\n\n\n\nOnce a model is basically a representation it can be built in several different manners including figures, flowcharts, and with the usage of equations and other statistical mathematical tools. Consider the dataset att_demo2 in Table 1.1 containing observations of 5 patients regarding a heart condition (health or Heart Condition), the level of activity (rest or moderate), the age in years, the heart rate in bpm and saturation per oxygen (spo2). As discussed in the first section of the chapter this dataset is factual evidence about the patients, representing a reality about they health status (their reality statuts), the equivalent to the half cyllinder in our Figure 1.1. For this dataset, we could creat different possible representations depending on our goals, particularly, statistical and mathematical representations. Suppose we want to explain the heart rate and spo2 using the rest of information in the dataset, we could describe this notation with the following notation:\n\\[\n(hr,\\, spO_2) = f(\\text{condition},\\, \\text{activity},\\, \\text{age})\n\\tag{1.1}\\]\nThe model/representation in Equation 1.1 is a specific example of a generic model in which we say that the heart rate and saturation of patients are a function of their heart condition, level of activity and age. Equation Equation 1.1 for now is an example of a mathematical function (we assume no error). We could create another representation like in Equation 1.2, and assume that we also have an error implied in using condition, activity and age to explain, heart rate and spo2, in that case we have a statistical model.\n\\[ (hr,spo2)=f(condition,activity,age) + error  \\tag{1.2}\\]\nMajor part of the rest of this book will be dedicate on different ways to define the function that will be connecting the things we want to explain with we use to explain them.\nIn this section we learnt that models are representations we use to assess a reality, situation or context, passible to be comparable. We also learnt that we can use equations to create models based on datasets.\n\n\n\nTable 1.1: Simulated ATT micro-dataset (n = 5) with heart condition (binary) and age (continuous).\n\n\natt_demo2\n\n  patient_id heart_condition activity age_years heart_rate_bpm spo2_percent\n1        P01         Healthy     Rest      52.4             73         98.9\n2        P02  HeartCondition Moderate      73.1             88         93.4\n3        P03  HeartCondition     Rest      61.2             73         95.8\n4        P04  HeartCondition Moderate      36.0             90         94.0\n5        P05         Healthy     Rest      53.7             72         98.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#decisions",
    "href": "introduction_AI.html#decisions",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.5 Decisions",
    "text": "1.5 Decisions\nIn an analogous manner we will introduce the idea of decision using first an intuitive example. Consider Figure 1.2 that depicts a crossroad in which some different routes can be chosen when someone is travelling and stopping at the stop signal. After the STOP one can turn to the left, turn to the right, go ahead, reverse and come back, etc.\nThe case in the Figure Figure 1.2 brings to our mind some core notions. First we have a state of ambiguity between different routes, each of these routes will bring us to different places, when reaching the crossroad we may know exactly or not the outcomes of each of the routes, etc. Generally speaking we say that we have some ambiguity in the courses of action we can take, because at a single moment we can take only one of them and not all of them at the same time, we have different alternatives of choice, each of them leading to a consequence regarding our journey.\n\n\n\n\n\n\nFigure 1.2: The crossroad for decision learning.\n\n\n\nFormalizing the ideas discussed in the previous paragraph we can say, following (White 1970), that a decision is a cognitive process \\(\\theta\\), clearly identifiable, that resolves a state of ambiguity \\(Q\\) within a state of knowledge \\(K\\), such that \\((Q, K) \\rightarrow q \\in Q\\), where \\(q\\) is a possible alternative within the state of ambiguity. Formalizing the ideas illustrated in the crossroads example, we can express the act of choosing a route as a decision process in White’s sense. In the case of the crossroads, \\(Q\\) corresponds to the set of possible paths available to the traveler, and \\(K\\) represents the traveler’s knowledge or expectations about the consequences of each path perhaps partial, uncertain, or even incorrect. The cognitive process \\(\\theta\\) is the mechanism by which the traveler evaluates this information and selects one route, \\(q\\), from among the available alternatives. Thus, the simple act of deciding which way to go operationalize the theoretical construct of decision-making: resolving ambiguity under a given state of knowledge to produce a concrete action.\nExamples in health care and advanced therapeutic technology include:\n\nSelect a treatment strategy for a patient using clinical history, imaging, genetics, and comorbidities.\nDesign and optimize clinical trials (inclusion criteria, dose/regimen, endpoints, adaptive rules).\nChoose a therapeutic modality (gene therapy, cell therapy, device, pharmacological intervention) for a given indication.\nInitiate/continue/discontinue therapy based on longitudinal biomarkers, response, and adverse events.\nIntegrate multi-omics evidence (genomics, proteomics, metabolomics) to guide precision medicine.\nAllocate scarce resources (ICU beds, organs, specialized equipment) under ethical and operational constraints.\nAssess risk–benefit and cost-effectiveness for adopting a new drug, device, or regenerative technology.\nDetermine optimal dosing/delivery via PK/PD modeling, model-informed precision dosing, or digital-twin simulation.\nPrioritize R&D pipelines based on clinical potential, unmet need, feasibility, and budget impact.\nEvaluate AI decision-support systems for safety, interpretability, bias/robustness, and regulatory compliance.\n\nDifferent theories approach decisions via different methods but can be broadly aggregated in the three main classes depending on how the \\(\\theta\\) process is addressed Bell, Raiffa, and Tversky (1988).\n\n\n\nTable 1.2: Types of approaches to the decision-making process (adapted from Bell, Raiffa, and Tversky (1988).\n\n\n\n\n\n\n\n\n\n\n\nCharacteristics\nNormative Models\nDescriptive Models\nPrescriptive Models\n\n\n\n\nDecision approach (\\(\\theta\\))\nAxiomatic, based on rational principles and expected utility theory.\nDescribe how the decision process actually occurs.\nFocus on how to decide well given cognitive and practical constraints.\n\n\nGeneral dynamics\nHow people should ideally make decisions.\nHow people actually make decisions, including cognitive limitations.\nHow people can act to make better decisions.\n\n\nAreas of use and examples\nMicroeconomics (expected utility paradigm), contingent state insurance.\nCognitive psychology (heuristics, biases, bounded rationality).\nEngineering, risk and uncertainty management, operations research.\n\n\n\n\n\n\nFrom the different approaches proposed in Table 1.2 we will give a particular focus on the prescriptive approaches specifically in one named Decision Analysis.\n\n1.5.1 Decision Analysis\nDecision analysis is a science that develop methods allowing the application of results from decision theory and elements from normative, prescriptive, and descriptive models to the solution of practical problems (Howard 1988). The main objective of this tool is to support the construction of sound decisions, given the cognitive and contextual realities of decision makers. In this sense, the field can also be referred to as decision engineering Sarti (2013).\n\nWhat exactly is the decision problem, and how should it be modeled?\n\nWhat are the possible alternatives, and how can they be identified?\n\nWhich variables should be included in the analysis?\n\nWhat level of detail should the analysis consider?\n\nHow much should be invested, and what is the nature of the information to be used?\n\nHow can the uncertainties involved be modeled?\n\nHow can the decision problem be clearly communicated?\n\nHow can the principles of decision theory be applied to the resolution of practical problems?\n\nHoward (1988) and Sarti (2013) describe the concept of a decision basis as a tool for addressing the questions outlined above and illustrated in Figure . The concept consists of a set of information, denoted as \\(BD\\), in which the alternatives, consequences, and uncertainties or risks associated with a decision are made explicit. The decision basis can be implemented through tables, graphical parameters, or, more formally, by means of influence diagrams and decision trees. The construction of the decision basis should be an iterative process, carried out until sufficient information has been gathered to support a decision. The dynamics of formulating the decision basis and the agents involved in the process are summarized in Figure 1.3. In this figure, we can observe that the process of analysis and construction of the decision basis begins with a state of obscurity regarding the problem.\nFrom that point, the technical experts, the analyst, and the decision makers act in an integrated manner, complementing their expertise with the available literature related to the issue.\nThrough this exchange of knowledge emerges the foundation for building the decision basis, which makes explicit the alternatives, consequences, preferences of those involved for each alternative, and the associated uncertainties. The process should be repeated iteratively until a decision is reached.\n\n\n\n\n\n\nFigure 1.3: Decision Basis\n\n\n\nSpetzler, Winter, and Meyer (2016) further develops the notion of decision basis proposing a whole cycle of decision quality as implemented via six elements: 1) Appropriate framing, 2) Creative Alternatives, 3) Relevant and Reliable Information, 4) Clear Values and Trade Offs, 5) Sound Reasoning and 6) Commitment to Action. Going through these six steps when approaching a decision helps us to avoid common pitfalls and biases of judgement.\nStep 3 in the last sequence involves the usage of relevant and reliable information, implying in the need for data and good representations of data, therefore, good modelling strategies, for achieving good quality decisions. This step is the conceptual motivation of models as decision support tools and main practical motivation for our course.\n\n\n\n\n\n\n\n\nFigure 1.4: Decision Quality elements adapted from Spetzler, Winter & Meyer (2016).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#defining-ml-and-ai",
    "href": "introduction_AI.html#defining-ml-and-ai",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.6 Defining ML and AI",
    "text": "1.6 Defining ML and AI\nIn the last sections of this chapter we had motivated and illustrated the reasons why we need to dedicate our time to learn how to create models or representations of data. We learnt that models are important information providers being key for the quality of decision making. In this section, we will define what is Intelligence, Artificial Intelligence and Machine Learning and show how these techniques can classified regarding the tasks they realize and much more interesting things.\n\n1.6.1 Intelligence\n\n1.6.1.1 Etymological and philosophical perspectives\nEtymologically, the word intelligence derives from the Latin intelligere, meaning “the capacity to comprehend or understand,” formed from inter (“between”) and legere (“to read” or “to choose”). Philosophically, intelligence has often been linked to the capacity for apprehending truth, understood as the correspondence or coincidence between facts and ideas.\nAn interesting nuance in this philosophical view is that intelligence and thought are not identical. We often think without necessarily judging whether something is true or false. Thought can operate freely in imagination or association, whereas intelligence involves an orientation toward truth and understanding. This distinction also separates intelligence from mere skills or procedural abilities, philosophically speaking intelligence implies discernment, not only execution.\n\n\n1.6.1.2 Computer science perspective\nFrom a computational standpoint, machines do not possess intelligence in the philosophical sense. Computers are fundamentally symbol-manipulating systems: they operate on data according to predefined rules, algorithms, and formal structures, without awareness or access to meaning. What we call machine intelligence (as in Artificial Intelligence and Machine Learning) refers to the ability of systems to perform tasks that, for humans, require cognitive effort such as recognizing patterns, making predictions, or generating language. However, these systems do not understand or apprehend truth; they only evaluate consistency or likelihood within the frameworks they are programmed with or have learned statistically.\nIn formal logic or mathematics, a computer can determine whether a statement is derivable or satisfiable according to given axioms but not whether it is “true” in an absolute or semantic sense. In empirical contexts, it can approximate “truth” through probabilistic inference based on data, but that remains a syntactic or statistical notion of truth, not a semantic or philosophical one. Thus, from a computer science perspective, truth is always relative to a system of rules, models, or data, while human intelligence seeks truth as correspondence between thought and reality.\n\n\n1.6.1.3 Artificial Intelligence\n\n1.6.1.3.1 Definition and Scope\nArtificial Intelligence (AI) can be broadly defined as the field of study and practice dedicated to creating systems capable of performing tasks that, if carried out by humans, would require cognitive processes. This includes perception, reasoning, learning, language understanding, decision-making, and problem-solving. AI is not a single technology but a multidisciplinary domain that integrates computer science, mathematics, cognitive psychology, statistics, linguistics, and philosophy. In practical terms, AI systems operate by processing inputs (data) through computational models designed to produce outputs that emulate aspects of human cognition for instance, recognizing an object in an image, predicting the outcome of a treatment, or generating coherent text.\n\n\n\nRelationship between AI and Machine Learning\n\n\nClassic examples of AI applications include:\n\nExpert systems that replicate human decision-making in domains such as medicine or engineering;\nSpeech and image recognition systems such as digital assistants or facial recognition software;\nAutonomous systems such as self-driving vehicles or robotic surgery;\nNatural language processing applications such as machine translation, chatbots, and summarization tools.\n\n\n\n\n1.6.1.4 Machine Learning\nMachine Learning (ML) is a core subfield of AI focused on building models that learn from data. Instead of being explicitly programmed with step-by-step rules, ML methods discover patterns, relationships, or functions from examples, allowing them to make predictions, classifications, or decisions when confronted with new data. The building block of machine learning methods are algorithms that consist in sets of rules and steps that method needs to do to performe a task. In short we can also say that machine learning is a context in which a machine learns to perform a task in the presence of data and improves its performance the more data we offer.\n\n\n\nTypes of Machine Learning\n\n\nIn a typical ML workflow, a model is trained on a dataset where both inputs and desired outputs are known (the training phase), and its performance is later evaluated on unseen data (the testing phase). Through iterative optimization, the model adjusts internal parameters to minimize prediction errors.\nExamples of ML applications include:\n\nPredicting disease risk from medical records;\nClassifying tumor images as benign or malignant;\nRecommending personalized treatments or products;\nForecasting energy demand or environmental variables;\nDetecting fraud in financial transactions.\n\nML methods can be supervised (trained with labeled outcomes), unsupervised (discovering structure without labels, e.g., clustering), or reinforcement-based (learning by trial and reward) ?fig-ml_classes. We will discuss what these terms mean when detailing the anatomy of a ML method in a future section.\n\n\n1.6.1.5 Predictive and Generative AI\nThe recent expansion of AI can be viewed through two complementary paradigms: predictive AI and generative AI.\nPredictive AI focuses on forecasting outcomes or estimating probabilities based on existing data. It aims to answer questions such as “What will happen next?” or “What is the most likely class or value?”. In our course predictive AI will be sometimes referred merely as Machine Learning and vice versa.\nExamples:\n\nPredicting patient readmission risk in healthcare;\nForecasting market behavior or demand in economics;\nAnticipating equipment failure in industrial systems.\n\nPredictive models rely on statistical and ML techniques (e.g., regression, decision trees, ensemble methods, neural networks) to generalize patterns from past data to future cases.\nGenerative AI, in contrast, focuses on creating new data that resemble observed patterns. These systems learn the underlying structure of their training data and then generate plausible new instances, such as text, images, code, or molecular structures.\nExamples:\n\nLarge Language Models (LLMs) that produce human-like text;\nGenerative Adversarial Networks (GANs) that create realistic images;\nVariational Autoencoders (VAEs) that synthesize complex data distributions;\nText-to-image models used for design, art, and simulation.\n\nGenerative AI represents a major shift from systems that merely predict to systems that can synthesize and imagine, extending AI from decision support to creative collaboration.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#bridging-predictive-and-generative-paradigms",
    "href": "introduction_AI.html#bridging-predictive-and-generative-paradigms",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.7 Bridging Predictive and Generative Paradigms",
    "text": "1.7 Bridging Predictive and Generative Paradigms\nWhile predictive AI is predominantly analytical, generative AI is synthetic. In practice, modern intelligent systems often integrate both: generative models can simulate scenarios for decision-making, while predictive models can assess the likelihood or impact of those simulated outcomes.\nIn the context of Advanced Therapeutic Technologies (ATT) and health sciences, this convergence allows AI not only to anticipate risks and outcomes but also to propose new molecules, optimize treatment protocols, and design personalized therapeutic strategies.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#more-on-models-for-data-and-anatomy-of-an-ai-method",
    "href": "introduction_AI.html#more-on-models-for-data-and-anatomy-of-an-ai-method",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.8 More on Models for data and anatomy of an AI method",
    "text": "1.8 More on Models for data and anatomy of an AI method\nWe will use again the dataset in Table 1.1 to illustrate a general scheme we will use for the rest of the book and to introduce some other key terms and jargon.\n\n\n  patient_id heart_condition activity age_years heart_rate_bpm spo2_percent\n1        P01         Healthy     Rest      52.4             73         98.9\n2        P02  HeartCondition Moderate      73.1             88         93.4\n3        P03  HeartCondition     Rest      61.2             73         95.8\n4        P04  HeartCondition Moderate      36.0             90         94.0\n5        P05         Healthy     Rest      53.7             72         98.0\n\n\nEach row in a dataset is called an instance, and each column represents a feature (or variable). Depending on the goal of the analysis, some features can be used as predictors (inputs), or explanatory variables, and others as responses (outputs) or responses. Sometimes we may refer to a variable as a label, the known value of the output (response) that we want a model to predict.\nOur dataset example can be viewed from a modelers point of view line in Figure 1.5.\n\n\n\n\n\n\nFigure 1.5: The way modellers see a dataset!\n\n\n\nIn the case each Y in Figure 1.5 is modelled alone we have an uni-response model and in the case they are modeled simultaneously we say it is multi-response.\nMachine learning tasks are classified in supervised, unsupervised, reinforcement and generative tasks. All methods share a general anatomy consisted of the data and motivational context, the task the algorithms perform, and measures of performance. Whenever studying a method we will use always describe it in this context of data, algorithm and performance.\n\n\n\n\n\n\nFigure 1.6: Anatomy of Machine Learning. All methods require data to be implemented, different algorithms as function of the kind of task. All methods need to have their performance evaluated via metrics.\n\n\n\nThe features are also classified into\n\n\n\n\n\n\n\n\n\nFeature Type\nDefinition\nExample from Model\nExplanation\n\n\n\n\nCategorical\nVariables that represent distinct groups or classes with no inherent order.\ncondition = {Healthy, HeartCondition}; activity = {Rest, Moderate}\nThese describe qualitative differences between individuals. One level is not “more” or “less” than another only different. Encoded typically as dummy variables (0/1).\n\n\nOrdinal\nVariables that have a clear, ranked order, but where differences between levels are not necessarily uniform.\ne.g., if activity were coded as {Rest &lt; Moderate &lt; Vigorous}\nHere, the order matters “Moderate” implies higher exertion than “Rest” but the distance between levels is undefined. These often appear in surveys or clinical scoring scales (e.g., pain level: mild &lt; moderate &lt; severe).\n\n\nContinuous\nVariables measured on a numeric scale with meaningful and consistent intervals.\nage (in years)\nThe differences are measurable: a 10-year increase has the same quantitative meaning regardless of where it occurs (e.g., 40→50 vs 60→70). Continuous variables allow arithmetic operations and smooth modeling (e.g., linear effects).\n\n\n\nIn the simulated dataset:\n\nheart_condition and activity are categorical.\nage_years is continuous.\nIf an ordered variable (like “disease severity” or “activity level”) were added, it would be ordinal.\n\n\n1.8.0.0.1 Supervised Learning\nUses labeled data: inputs (features) + known output (label).\n\nGoal: learn a mapping from features → label.\nExample: Predict heart_condition (Healthy vs HeartCondition) from activity, age_years, heart_rate_bpm, spo2_percent\nTypical algorithms: Logistic regression, random forest, SVM, neural networks\nOutput: class label or numeric value (classification/regression).\n\n\n\n1.8.0.0.2 Unsupervised Learning\nUses unlabeled data: only features, no target.\n\nGoal: find structure/patterns (clusters, components).\n\nExample: Cluster patients by heart_rate_bpm, spo2_percent, age_years to reveal low/high-risk profiles without using heart_condition.\n\nTypical algorithms: k-means, hierarchical clustering, Gaussian mixtures, PCA.\n\nOutput: groupings, embeddings, or density estimates.\n\n\n\n1.8.0.0.3 Reinforcement Learning\nAn agent interacts with an environment and learns via rewards/penalties over time.\n\nGoal: learn a policy that maximizes long-term reward.\n\nExample: A wearable adjusts activity level to keep spo2_percent and heart_rate_bpm in range; rewards for healthy ranges, penalties otherwise.\n\nTypical algorithms: Q-learning, DQN, policy gradients.\n\nOutput: policy for sequential decisions.\n\n\n\n1.8.0.0.4 Generative Learning\nLearns the data distribution to create new, realistic samples.\n\nGoal: model (p()) and generate synthetic data.\n\nExample: Create synthetic patient records with plausible heart_rate_bpm/spo2_percent/activity combinations for testing or augmentation.\n\nTypical algorithms: VAEs, GANs, transformer-based models.\n\nOutput: new samples; also used for simulation, augmentation, imputation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#introduction-to-model-comparison",
    "href": "introduction_AI.html#introduction-to-model-comparison",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.9 Introduction to model comparison",
    "text": "1.9 Introduction to model comparison\nSo far we have learnt that models are representations of a given reality, which in AI contexts, relate to supervised, unsupervised, reinforcement and generative tasks. Intuitively, we also learnt from Figure 1.1 that for a same given reality we can have different models. In this section we will address the matter of how to compare the different possible machine learning methods for the same situation.\nIn our efforts, we will consider that the best model is the one that most accurately learns and represents the reality it is intended to approximate. To assess this, we rely on performance metrics that quantify how closely a model’s outputs align with the true or expected outcomes.\nIn supervised learning, these metrics measure the discrepancy between predicted and observed values  for instance we use concepets like accuracy, precision–recall, ROC-AUC for classification, or RMSE and MAE for regression. In unsupervised learning, where there are no predefined labels, we instead evaluate how well the model captures underlying structure, using metrics such as silhouette score, cluster purity, or reconstruction error.\nIn reinforcement learning, performance is gauged by the cumulative reward obtained through interaction with the environment, reflecting how well the agent learns optimal decisions over time.\nFinally, in generative modeling, evaluation focuses on fidelity, diversity, and realism of the generated data, often through metrics like FID (Fréchet Inception Distance), likelihood, or domain-specific validation.\nIn the next chapters when discussing the different methods we will always refer to metrics of model performance and how they could be used to select best models for a given situation.\nWe also can compare models regarding they usage for inference and for answering research questions.\nIn general we should advocate for a balance between predictive performance and interpretability x inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#understanding-the-aiml-predictive-pipeline",
    "href": "introduction_AI.html#understanding-the-aiml-predictive-pipeline",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.10 Understanding the AI/ML Predictive Pipeline",
    "text": "1.10 Understanding the AI/ML Predictive Pipeline\nFigure 1.7 illustrates a simplified version of the predictive pipeline commonly used in Artificial Intelligence and Machine Learning. Although implementations may vary across domains and algorithms, most supervised learning workflows follow the same fundamental structure: a training stage, in which the model learns patterns from data, and a deployment stage, in which the trained model is applied to new cases.\n\n1.10.1 Training Phase\nThe process begins with a dataset containing observations of several variables. In the example, each row corresponds to a patient and includes both:\nPredictors (or features): such as heart_condition, activity, and age_years\nResponses (or labels): such as heart_rate_bpm and spo2_percent\nTo evaluate how well a model can generalize, the dataset is divided into two distinct subsets:\nTraining set\n\nThis subset is used to fit the model. It provides the examples from which the algorithm learns the relationship between predictors and responses. During this stage, the model adjusts its internal parameters to capture regularities in the training data.\nTest set\n\nThis subset is intentionally withheld during model fitting. It serves as an independent benchmark to assess the model’s performance on new, unseen data. By comparing predicted values with the true responses in the test set, we obtain an estimate of the model’s ability to generalize beyond the data from which it learned.\nThis separation between training and testing is essential; without it, the evaluation would be overly optimistic and would fail to indicate whether the model can perform reliably outside the initial dataset.\nOnce a model has been trained and its performance has been verified, it can be deployed. Deployment refers to the use of the model in real or simulated operational settings, where it is asked to make predictions for entirely new instances.\nIn the example, new patients (P06 to P100) arrive with predictor information (e.g., heart condition, activity, age), but their outcomes are unknown. The trained model processes these inputs and generates predictions for variables such as expected heart rate. These predictions illustrate the core function of machine learning models: using previously learned structure to inform decision-making in new contexts.\n\n\n\n\n\n\nFigure 1.7: The pipeline of developing AI/ML solutions\n\n\n\nIf well trained a models works well in new unseen data in other terms this models generalizes well.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#a-very-short-introduction-to-model-bias-overfitting-and-crossvalidation",
    "href": "introduction_AI.html#a-very-short-introduction-to-model-bias-overfitting-and-crossvalidation",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.11 A very short introduction to model bias, overfitting and crossvalidation",
    "text": "1.11 A very short introduction to model bias, overfitting and crossvalidation\nWhen we build a model, the goal is to approximate reality well enough to make useful predictions and inferences. Models must generalize from the data they see to new data they have not seen. Three core ideas help us judge this: bias, variance/overfitting, and cross-validation.\n\n1.11.1 Model bias\nBias is the systematic difference between a model’s predictions and the true values. High-bias models make strong simplifying assumptions and typically under fit the data, missing important structure, see Figure 1.9.\nExample: predicting heart rate using only age while ignoring activity level and heart condition is too rigid, so real patterns are missed.\n\n\n1.11.2 Variance and overfitting\nVariance is a model’s sensitivity to the specific training set. High-variance models are very flexible; they fit random noise as if it were signal and thus overfit great training performance, poor test performance.\nExample: a highly tuned model that perfectly matches small fluctuations in five patients’ heart-rate/SpO₂ readings but fails on a sixth.\n\n\n\n\n\n\nFigure 1.8\n\n\n\n\n\n1.11.3 Cross-validation, training and test datasets\nCross-validation estimates how well a model will generalize. Instead of training and testing on the same data, we split the dataset into several folds. We train on some folds and test on the remaining fold, then rotate until every fold has served as a test set.\nCommon choices:\n\nk-fold cross-validation: split into k parts (for example, k = 5). Average the metric across folds for a stable estimate.\nLeave-one-out (LOOCV): useful for very small datasets; train on all but one observation and test on the left-out one, repeating for all observations.\n\n\n\n\n1.11.4 Importance of cross-validating\nBias, overfitting, and cross-validation address the same question: is the model learning meaningful signal rather than noise? A good model balances flexibility (to capture structure) with restraint (to avoid noise). Cross-validation provides an honest estimate of out-of-sample performance before using models to inform real decisions in clinical, laboratory, or policy settings.\n\n\n\n\n\n\n\n\nFigure 1.9: Three situations side-by-side: Good fit (well-tuned), High bias (underfit), and Overfit (too complex). The dashed line is the true signal.\n\n\n\n\n\n\n\n1.11.5 Causal and non causal machine learning\nMachine learning models can be grouped according to how they treat the data-generating process (DGP). The main difference between causal and non-causal approaches lies in whether the model tries to represent how the data were produced or only what patterns appear in them.\n\n1.11.5.1 Non-causal (Associational) Machine Learning\nNon-causal machine learning assumes that the available data contain stable relationships between variables but does not try to represent the mechanisms that produced those data. Algorithms such as linear regression, random forest, and neural networks are trained to find statistical associations that best predict an outcome (Y) from a set of inputs (X):\n[ = f(X) ]\nThe goal is to minimize prediction error. However, the model ignores the process that generated (X) and (Y). If the environment changes, for example when an intervention or policy modifies the DGP, these models may perform poorly. This situation is known as a distributional shift or a non-stationarity problem.\n\n\n1.11.5.2 Causal Machine Learning\nCausal machine learning explicitly represents the mechanisms that generate the data. It seeks to estimate how outcomes would change if a variable were intervened upon, moving from correlation toward causal inference. This requires assumptions about the DGP, which are often expressed using a causal graph or a structural causal model (SCM):\n[ Y = f(X, U) ]\nwhere (U) represents unobserved causes. Causal machine learning answers questions such as: “What would happen to (Y) if (X) were changed while keeping everything else constant?”\n\n\n\n\n\n\n\n\nAspect\nNon-causal ML\nCausal ML\n\n\n\n\nMain goal\nAccurate prediction\nEstimation of causal effects\n\n\nData used\nObserved variables\nObserved and counterfactual variables\n\n\nDGP considered\nImplicit or ignored\nExplicitly modeled\n\n\nOutput\n( = f(X) )\nEffect of (X) on (Y)\n\n\nLimitations\nSensitive to confounding and bias\nRequires strong causal assumptions such as no hidden confounders\n\n\n\n\n\n1.11.5.3 Associational pattern without explicit DGP\n\n\n\n\n\nflowchart LR\n  X[X features] --&gt; Y[Outcome Y]\n  style X fill:#e8f1ff,stroke:#2f63c0\n  style Y fill:#e8f1ff,stroke:#2f63c0\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\nZ[Confounder Z] --&gt; X[Exposure X]\nZ --&gt; Y[Outcome Y]\nX --&gt; Y\nstyle X fill:#e8f1ff,stroke:#2f63c0\nstyle Y fill:#e8f1ff,stroke:#2f63c0\nstyle Z fill:#fff4d6,stroke:#c0902f\n\n\n\n\n\n\nThe first diagram at the top represents a purely associational pattern.\nIt shows that we can find a statistical relationship between a set of predictors ( X ) (or features) and an outcome ( Y ), even without knowing how these data were generated.\nThis is the standard setup of non-causal machine learning, where the goal is to learn a predictive mapping:\n[ = f(X) ]\nSuch models can predict ( Y ) given ( X ), but they do not explain why the relationship exists.\nIf the data-generating process changes (for example, after a new treatment policy or environmental shift), the learned pattern might no longer hold.\nThe second diagram introduces a causal structure with a confounder ( Z ) that affects both the exposure ( X ) and the outcome ( Y ).\nThis setup represents a situation where the association between ( X ) and ( Y ) is partly due to a third variable ( Z ) that influences both.\nTo correctly identify the effect of ( X ) on ( Y ), we must account for ( Z ).\nIf ( Z ) is not measured or controlled for, the observed relationship between ( X ) and ( Y ) can be biased this is known as confounding.\nInterpretation.\nTo estimate the true causal effect of ( X ) on ( Y ), the confounder ( Z ) must be included in the model.\nIgnoring ( Z ) can lead to misleading conclusions about the direction or strength of the effect.\nTakeaway.\n- Non-causal machine learning captures what tends to occur together.\n- Causal machine learning aims to estimate what would happen if we changed something, considering the data-generating process and possible confounders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#coding-and-version-control",
    "href": "introduction_AI.html#coding-and-version-control",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.12 Coding and Version Control",
    "text": "1.12 Coding and Version Control\nIn other to perform tasks machine learning algorithms and methods need to be implemented in computers. To communicate to the machine what it is necessary to do we use programming languages in the form of code. Below in Table 1.3 we have an example of code that creates a vector of numbers, calculates the averages of such numbers, prints a message and creates a simulated data frame.\n\n\n\nTable 1.3: A tiny example of coding\n\n\n# 1) Create a small numeric vector\nx &lt;- c(72, 75, 70, 78, 74)\n\n# 2) Compute a simple summary\navg &lt;- mean(x)\n\n# 3) Print a message that mixes text and a computed value\ncat(\"The average value is:\", avg, \"\\n\")\n\nThe average value is: 73.8 \n\n# 4) Make a tiny data frame and show the first rows\ndf &lt;- data.frame(id = 1:5, value = x)\nhead(df)\n\n  id value\n1  1    72\n2  2    75\n3  3    70\n4  4    78\n5  5    74\n\n\n\n\nWhen creating code we usually start with an initial version which often is modified or adapted, created by a single individual or by several people’s contributions which are using the same machines or not, in the same place or not among other details. These dynamics bring the need for mechanism that facilitate the control of code version, allowing for remote contributions and collaborative creation of code which are very well implemented in version control technologies like Git.\nGit is a version control system (VCS) designed to track changes in code and other text files over time. It allows developers to record, compare, and revert modifications, ensuring that every update is safely stored and that collaboration does not overwrite previous work. Each version of your project is stored as a commit, which captures the exact state of all tracked files at a given point in time, along with a descriptive message about what changed.\nWhile Git works locally on your computer, platforms such as GitHub, GitLab, and Bitbucket provide remote repositories, online spaces that store your project’s history, enable backup, and make team collaboration possible from anywhere.\nWe recommend the readers to set up a github account, install git in their system and integrate git with R and Rstudio.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#set-up-git-github-for-rrstudio-workflows",
    "href": "introduction_AI.html#set-up-git-github-for-rrstudio-workflows",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.13 Set up Git & GitHub (for R/RStudio workflows)",
    "text": "1.13 Set up Git & GitHub (for R/RStudio workflows)\nWe recommend that readers set up a GitHub account, install Git, and integrate it with R and RStudio to make their workflows versioned, collaborative, and reproducible.\n\n1.13.1 1) Create a GitHub account\n\nGo to https://github.com and create a free account.\nPick a concise username (becomes part of your repo URLs).\n(Optional) Enable 2FA: Settings → Password and authentication.\nLearn key concepts: repository, commit, branch, pull request.\n\n\n\n1.13.2 2) Install Git\nCheck if Git is already available:\ngit --version\nIf not, install it for your OS:\nWindows\n\nDownload and install: https://git-scm.com/download/win\n(or via Winget):\nwinget install --id Git.Git -e\n\nmacOS\n\nEasiest (Apple CLT):\nxcode-select --install\n(Optional) With Homebrew:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew install git\n\nLinux (Debian/Ubuntu)\nsudo apt update\nsudo apt install -y git\n\n\n1.13.3 3) Configure Git (one-time)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\ngit config --global init.defaultBranch main\n# Optional:\ngit config --global pull.rebase false   # use merge on pull\ngit config --global core.autocrlf input # good default on macOS/Linux (use \"true\" on Windows)\n\n\n1.13.4 4) Authenticate with GitHub (Personal Access Token)\nFrom R (recommended):\ninstall.packages(c(\"usethis\",\"gitcreds\"))\nusethis::create_github_token()  # opens browser to create a PAT (scopes: repo, read:org, workflow)\ngitcreds::gitcreds_set()        # paste the token once\ngitcreds::gitcreds_get()        # verify stored token\nTip: If you’re prompted for a “password” during git push, paste the PAT (not your GitHub password).\n\n\n1.13.5 5) Enable Git in RStudio\n\nRStudio → Tools → Global Options → Git/SVN\n\nEnsure Git executable is detected (browse to it if needed).\nTick Enable version control interface.\n\nClick Apply (restart RStudio if prompted).\n\n\n\n1.13.6 6) Create a repo and push to GitHub\nOption A via RStudio UI\n\nFile → New Project → choose a directory → tick Create a git repository.\nMake a change → Git tab → Commit → Push.\n\nOption B via terminal (works on both macOS and Windows)\n# Run inside your project folder\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin https://github.com/&lt;youruser&gt;/&lt;repo&gt;.git\ngit push -u origin main\n\n\n1.13.7 7) Daily workflow\ngit pull                      # get others' changes\ngit add .                     # stage your changes\ngit commit -m \"Meaningful message\"\ngit push                      # publish your work\n\n\n1.13.8 8) (Optional) Use SSH instead of HTTPS\nGenerate a key and add it to GitHub:\nmacOS/Linux\nssh-keygen -t ed25519 -C \"you@example.com\"\ncat ~/.ssh/id_ed25519.pub\nWindows (PowerShell)\nssh-keygen -t ed25519 -C \"you@example.com\"\ntype $env:USERPROFILE\\.ssh\\id_ed25519.pub\n\nCopy the .pub key → GitHub → Settings → SSH and GPG keys → New SSH key.\nPoint your repo to SSH:\n\ngit remote set-url origin git@github.com:&lt;youruser&gt;/&lt;repo&gt;.git\n\n\n1.13.9 9) Quick fixes\n\n“git: command not found” (macOS) → run xcode-select --install or brew install git.\nRStudio doesn’t see Git → set Git path in Tools → Global Options → Git/SVN.\nDivergent branches on pull → choose one:\ngit pull --no-rebase   # merge (default if configured)\n# or\ngit pull --rebase      # rebase to keep linear history\nToken expired → in R:\ngitcreds::gitcreds_set()\nWrong email in commits → update config and amend latest commit:\ngit config --global user.email \"you@newmail.com\"\ngit commit --amend --reset-author\ngit push --force-with-lease\n\n\n\n1.13.10 A minimal portfolio structure for data professionals\nEvery craft needs the right setup: carpenters need a bench and tools; data professionals need a basic, reliable workspace. If you’re starting in AI/ML and data science, invest early in a lean, well-organized portfolio. The simplest and most effective format is a set of GitHub repositories that showcase your work each with clear documentation, runnable code, and reproducible results. Over time, this portfolio becomes your toolkit and your calling card: easy to share, easy to maintain, and easy for others to trust.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#reproducibility",
    "href": "introduction_AI.html#reproducibility",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.14 Reproducibility",
    "text": "1.14 Reproducibility\n\nAs discussed in the previous sections of this chaoter, Data are our evidence about the world; models turn that evidence into information for decision-making. Most AI and ML projects follow the same steps: collect and organize data, build a model, evaluate performance and communicate results. Across all these stages, one principle matters above all: reproducibility, defined as the ability for others (or your future self) to repeat the process and obtain the same results for learning, auditing, or verification.\nReproducibility is best achieved with a minimal, transparent workflow: make data (when permissible) and code available so that someone else can rerun the analysis and arrive at the same outputs. Version control (e.g., Git) is essential here. Whenever data governance, intellectual property, and legal constraints allow, we recommend sharing both data and code to support reproducible science.\nIn health applications, reproducible analyses also underpin evidence-based regulation. National authorities issue guidelines for approving drugs, procedures, and other therapeutic technologies, typically grounded in literature and empirical evidence. Researchers and developers are therefore frequently called upon to produce transparent, well-documented analyses that regulatory boards can evaluate and trust.\n\n\n\n\nBell, David E., Howard Raiffa, and Amos Tversky, eds. 1988. Decision Making: Descriptive, Normative, and Prescriptive Interactions. Cambridge: Cambridge University Press. https://www.cambridge.org/core/books/decision-making/D29E12748C31B85FE83C9CEDEE9D4D88.\n\n\nHasin, Yoram, Michael Seldin, and Aldons Lusis. 2017. “Multi-Omics Approaches to Disease.” Genome Biology 18: 83. https://doi.org/10.1186/s13059-017-1215-1.\n\n\nHilbert, Martin, and Priscila López. 2011. “The World’s Technological Capacity to Store, Communicate, and Compute Information.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nHoward, Ronald A. 1988. “Decision Analysis: Practice and Promise.” Management Science 34 (6): 679–95.\n\n\nHripcsak, George, and David J. Albers. 2013. “Next-Generation Phenotyping of Electronic Health Records.” Journal of the American Medical Informatics Association 20 (1): 117–21. https://doi.org/10.1136/amiajnl-2012-001145.\n\n\nJensen, Peter B., Lars J. Jensen, and Søren Brunak. 2012. “Mining Electronic Health Records: Towards Better Research Applications and Clinical Care.” Nature Reviews Genetics 13 (6): 395–405. https://doi.org/10.1038/nrg3208.\n\n\nKarczewski, Konrad J., and Michael P. Snyder. 2018. “Integrative Omics for Health and Disease.” Nature Reviews Genetics 19: 299–310. https://doi.org/10.1038/nrg.2018.4.\n\n\nRaghupathi, Wullianallur, and Viju Raghupathi. 2014. “Big Data Analytics in Healthcare: Promise and Potential.” Health Information Science and Systems 2 (3): 1–10. https://doi.org/10.1186/2047-2501-2-3.\n\n\nSarti, Danilo Augusto. 2013. “Uncertainty Management Through Decision Analysis: Applications to Production Optimization and Uncertain Demands.” PhD thesis, Universidade de São Paulo.\n\n\nSpetzler, Carl, Hannah Winter, and Jennifer Meyer. 2016. Decision Quality: Value Creation from Better Business Decisions. Hoboken, NJ: John Wiley & Sons.\n\n\nStephens, Zachary D., Skylar Y. Lee, Faraz Faghri, R. Ilkayeva Campbell, et al. 2015. “Big Data: Astronomical or Genomical?” PLOS Biology 13 (7): e1002195. https://doi.org/10.1371/journal.pbio.1002195.\n\n\nTopol, Eric J. 2019. Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again. New York: Basic Books.\n\n\nWhite, Douglas John. 1970. Decision Theory. New Brunswick, NJ: Routledge / Transaction Publishers. https://www.routledge.com/Decision-Theory/White/p/book/9780202308982.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html",
    "href": "supervised_regression.html",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "",
    "text": "2.1 Setting up R\nRun this code in your R studio to make sure you have all the packages required installed!\n# Packages needed in this section\nreq_pkgs &lt;- c(\n  \"dplyr\",      # data wrangling\n  \"ggplot2\",    # plotting\n  \"tidyr\",      # tidying\n  \"readr\",      # read/write csv\n  \"tibble\",     # tibbles/printing\n  \"gridExtra\",  # simple plot grids\n  \"emmeans\",    # adjusted means / contrasts\n  \"effsize\",     # effect sizes (Cohen's d, Cliff's delta)\n \"DataExplorer\" ,#for missingness\n  \"glmnet\",\n \"lmtest\" ,# for checking the \n \"pROC\"\n )\n\n# Install any that are missing\nto_install &lt;- setdiff(req_pkgs, rownames(installed.packages()))\nif (length(to_install) &gt; 0) {\n  install.packages(to_install, dependencies = TRUE)\n}\n# Load all (silently)\ninvisible(lapply(req_pkgs, require, character.only = TRUE))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#recall",
    "href": "supervised_regression.html#recall",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.2 Recall",
    "text": "2.2 Recall\nIn the introduction to AI section we learnt that models are representations that provide information for evidence based decisions. We also learnt that for each machine learning task we may have different models that use different learning algorithms, their performance needs to be evaluated. Comparison between such models is better performed via metrics. We also learnt the importance of cross validation to avoid variance, bias and over-fitting.\nIn this chapter with a motivational example we will start with the first kind of supervised learning technique, in which the models will learn from the relation between continuous or binary labels and explanatory features, this kind of task is named regression.\nThe introductory chapter defined a generic representation of a model using an equation:\n\\[Y=f(x1,x2,...,xn)+error\\] Where Y is attribute considered as label or response and x’s are the explanatory features and error is due to random process os measure errors usually notate as \\(\\epsilon\\)\nIn this chapter we will give attention to a series of models in which this mathematical function f can be defined as:\n\\[Y= \\mu + w1 * x1 +.... wn *xn+ \\epsilon \\tag{2.1}\\] Once all \\(w_i\\)s are raised to the power of 1 we say this model is linear on the parameters or simply linear. \\(\\mu\\) and \\(w_i\\)s are named parameters which we can estimate using different algorithms. The \\(w_is\\) will be weighting the importance of feature \\(x_i\\) when explaining Y.\nIn this chapter we will cover three main classes of models and respective algorithms to determine the parameters as follows:\n\nlinear regression which parameters are estimated by least squares (which use the great idea of minimizing the errors between the real values and the predicted values)\n\nand two models that consider the notion of regularization for avoiding model complexity (using an idea of shrinkage - defined later)\n\nlasso regression, estimated via Least Absolute Shrinkage and Selection Operator (LASSO) and L1 regularization\nRidge regresion estimated via L2 norm.\n\nThe idea of Lasso and Ridge regression is making the values of \\(w_i\\)s smaller as possible (ridge) or zero (lasso) for the least important features.\nOnce we will be covering regression tasks we will also address the logistic regression model that is useful when we want to model a binary response variable Y= 0 or 1 using explanatory features.\nAfter running a machine learning method and estimating the parameters, also named weights, we can rewrite it as:\n\\[\\hat{Y}= \\hat{\\mu} + \\hat{w_1}* x1 +....+\\hat{w_n} *xn\\] The values with a hat are the estimatives for each parameter an assume values depending on the model fitted. The \\(\\hat{Y}\\) is named predicted value. It is the value returned by the modell fitted for a given set of explanatory features multiplied by the parameters estimated. We will further see the importance of this value for estimating measures of error and model performance.\nBefore seeing the details about regression models we will start by describing a particular case in which they will be applied.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#motivational-context-chemotherapy-clinical-trial-focus-on-variables-and-practical-interpretation",
    "href": "supervised_regression.html#motivational-context-chemotherapy-clinical-trial-focus-on-variables-and-practical-interpretation",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.3 Motivational context : chemotherapy clinical trial (focus on variables and practical interpretation)",
    "text": "2.3 Motivational context : chemotherapy clinical trial (focus on variables and practical interpretation)\nAs a motivational example we will use a simulated randomized clinical trial (1:1) ,a typical kind of experiment conducted to determine efficacy of drugs, in our case a chemoterapeutic one, including only patients with tumors.\nHalf of the patients receive chemotherapy (chemo), and the other half do not (no_chemo). The practical goal is to evaluate whether patients treated with chemotherapy show greater tumor reduction, and to understand how gene expression and clinical characteristics influence this response.\n\n2.3.1 What is measured (and how to interpret each variable)\nUnit of analysis: patients. We have 10000 thousand patients.\nTreatment - treatment (factor: no_chemo, chemo)\nRepresents whether the patient received the chemotherapy.\nIn regression models, the coefficient for chemo directly quantifies the average difference in tumor reduction compared with no_chemo.\nDose - dose_intensity (continuous, ~0 for no_chemo and 0.8–1.1 for chemo)\nMeasures how strong the chemotherapy was for treated patients.\nA positive regression coefficient means that higher dose intensity is associated with greater tumor shrinkage.\nOutcomes - baseline_tumor_mm (continuous, mm): tumor size before treatment\n- post_tumor_mm (continuous, mm): tumor size after treatment\n- response_percent (continuous, 0–100): percentage of tumor shrinkage,\ncalculated as 100 × (baseline − post) / baseline\n→ Higher values mean better therapeutic response.\n- high_response (binary, 0/1): equal to 1 if response_percent ≥ 30 (similar to RECIST clinical criteria) Eisenhauer et al. (2009).\n→ Used in logistic regression to model the probability of a strong response.\nThe RECIST (Response Evaluation Criteria in Solid Tumors) Eisenhauer et al. (2009) are standardized, internationally accepted criteria used to assess tumor response to treatment in clinical trials. Developed by an international collaboration between the EORTC, NCI (U.S.), and NCIC (Canada), RECIST provides quantitative guidelines for measuring changes in tumor size using imaging (typically CT or MRI). Under RECIST version 1.1 (2009), tumor response is classified as: Complete Response (CR): Disappearance of all target lesions. Partial Response (PR): ≥30% decrease in the sum of the diameters of target lesions (relative to baseline). Progressive Disease (PD): ≥20% increase in the sum of diameters (plus an absolute increase of ≥5 mm) or appearance of new lesions. Stable Disease (SD): Neither sufficient shrinkage nor sufficient increase to qualify as PR or PD.\nClinical profile - patient_age (continuous, years): older age may slightly reduce treatment effect.\n- tumor_grade (factor: G1, G2, G3): aggressiveness of the tumor.\nMore aggressive tumors may respond more strongly because of higher proliferation rates.\n- performance_score (ordinal, 0–2): functional status (ECOG-like scale).\nHigher scores indicate poorer condition and typically lower treatment benefit due to toxicity or fragility.\nGene expression (omics) - gene_01 … gene_20000 (continuous, log2-CPM normalized)\nRepresent quantitative gene expression levels.\nSome genes were simulated as causal (directly affecting response),\nothers as correlated (co-expressed with causal ones),\nand the rest as noise.\nThe above example will be considered the reality for which we want to create a model representation. In penalized models (LASSO or Ridge), we expect the causal and correlated genes to receive higher weights (non-zero coefficients), while noisy ones will shrink toward zero.\n\n\n2.3.2 Experimental objectives and practical questions\n\nMain question: Do patients treated with chemotherapy (chemo) show higher average tumor shrinkage than those who do not (no_chemo)?\n\nClinical modulators: Do age, tumor grade, or performance score modify this effect?\n\nMolecular biomarkers: Which genes are associated with treatment response?\n\nPredictive modeling: Given a patient’s clinical and molecular profile, how well can we predict their tumor reduction and probability of high response?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#reading-the-dataset-into-our-environment",
    "href": "supervised_regression.html#reading-the-dataset-into-our-environment",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.4 Reading the dataset into our environment",
    "text": "2.4 Reading the dataset into our environment\nLets read the dataset in our R studio and check its structure of the first 15 columns\n\ntrial_ct &lt;- readRDS(\"~/att_ai_ml/data/trial_ct_chemo_cont.rds\")\nstr(trial_ct[, 1:15])   # peek\n\n'data.frame':   10000 obs. of  15 variables:\n $ patient_id       : chr  \"P001\" \"P002\" \"P003\" \"P004\" ...\n $ treatment        : Factor w/ 2 levels \"no_chemo\",\"chemo\": 2 1 1 1 2 2 1 2 1 2 ...\n $ dose_intensity   : num  1.08 0 0 0 1.01 ...\n $ patient_age      : num  81 61 81 74 41 74 22 61 26 22 ...\n $ tumor_grade      : Factor w/ 3 levels \"G1\",\"G2\",\"G3\": 2 2 3 2 2 2 1 2 3 2 ...\n $ performance_score: int  1 1 1 0 1 1 2 0 1 0 ...\n $ baseline_tumor_mm: num  52.2 43.3 43.2 63.8 54.2 44.9 51.5 95.1 69.5 54.7 ...\n $ post_tumor_mm    : num  25.8 43.3 43.2 61.2 33.6 37 51.5 60.4 69.5 29.8 ...\n $ response_percent : num  50.6 0 0 4.2 38 17.5 0 36.5 0 45.6 ...\n $ high_response    : int  1 0 0 0 1 0 0 1 0 1 ...\n $ gene_01          : num  11.01 9.26 10.09 8.99 9.01 ...\n $ gene_02          : num  9.31 7.79 8.95 7.8 8.33 ...\n $ gene_03          : num  9.51 8.02 9.45 7.78 8.23 ...\n $ gene_04          : num  10.27 9.16 9.42 8.87 9.03 ...\n $ gene_05          : num  7.31 8.14 7.83 7.49 7.23 ...\n\nstr(trial_ct)\n\n'data.frame':   10000 obs. of  2010 variables:\n $ patient_id       : chr  \"P001\" \"P002\" \"P003\" \"P004\" ...\n $ treatment        : Factor w/ 2 levels \"no_chemo\",\"chemo\": 2 1 1 1 2 2 1 2 1 2 ...\n $ dose_intensity   : num  1.08 0 0 0 1.01 ...\n $ patient_age      : num  81 61 81 74 41 74 22 61 26 22 ...\n $ tumor_grade      : Factor w/ 3 levels \"G1\",\"G2\",\"G3\": 2 2 3 2 2 2 1 2 3 2 ...\n $ performance_score: int  1 1 1 0 1 1 2 0 1 0 ...\n $ baseline_tumor_mm: num  52.2 43.3 43.2 63.8 54.2 44.9 51.5 95.1 69.5 54.7 ...\n $ post_tumor_mm    : num  25.8 43.3 43.2 61.2 33.6 37 51.5 60.4 69.5 29.8 ...\n $ response_percent : num  50.6 0 0 4.2 38 17.5 0 36.5 0 45.6 ...\n $ high_response    : int  1 0 0 0 1 0 0 1 0 1 ...\n $ gene_01          : num  11.01 9.26 10.09 8.99 9.01 ...\n $ gene_02          : num  9.31 7.79 8.95 7.8 8.33 ...\n $ gene_03          : num  9.51 8.02 9.45 7.78 8.23 ...\n $ gene_04          : num  10.27 9.16 9.42 8.87 9.03 ...\n $ gene_05          : num  7.31 8.14 7.83 7.49 7.23 ...\n $ gene_06          : num  7.44 7.22 7.41 6.28 7.01 ...\n $ gene_07          : num  9.13 9.24 8.96 8.23 7.96 ...\n $ gene_08          : num  1.722 -1.205 -1.211 -0.574 0.824 ...\n $ gene_09          : num  7.23 4.41 6.01 4.67 5.36 ...\n $ gene_10          : num  6.94 6.52 6.99 5.92 5.97 ...\n $ gene_11          : num  7.21 7 7.23 6.81 7.42 ...\n $ gene_12          : num  5.81 6.78 6.54 7.85 7.44 ...\n $ gene_13          : num  9.33 8.92 8.48 9.07 9.69 ...\n $ gene_14          : num  1.344 -1.212 -0.934 -0.639 1.07 ...\n $ gene_15          : num  8.79 9.1 8.95 9.33 9.22 ...\n $ gene_16          : num  6.79 7.36 7.23 7.74 7.94 ...\n $ gene_17          : num  7.31 5.58 6.11 5.7 6.14 ...\n $ gene_18          : num  7.56 7.82 7.98 7.97 7.66 ...\n $ gene_19          : num  1.504 -1.567 -1.099 -1.003 0.616 ...\n $ gene_20          : num  10.1 10.7 10.8 11.2 11.1 ...\n $ gene_21          : num  9.42 8.45 8.92 7.9 9.4 ...\n $ gene_22          : num  9.68 11.12 10.65 9.82 10.17 ...\n $ gene_23          : num  8.95 10.27 9.73 9.33 9.36 ...\n $ gene_24          : num  10.34 9.81 9.98 9.37 9.3 ...\n $ gene_25          : num  10.37 10 9.73 10.4 10.87 ...\n $ gene_26          : num  6.69 6.77 6.88 7.61 7.91 ...\n $ gene_27          : num  7.91 5.81 6.16 6.38 6.81 ...\n $ gene_28          : num  7.91 8.34 7.18 7.39 6.98 ...\n $ gene_29          : num  8.34 8.97 8.75 8.97 8.69 ...\n $ gene_30          : num  7.89 7.97 7.81 7.64 7.98 ...\n $ gene_31          : num  12.4 11.5 12.5 11 12 ...\n $ gene_32          : num  8.8 8.66 9.46 8.28 8.03 ...\n $ gene_33          : num  6.61 6.65 6.79 7.05 7.23 ...\n $ gene_34          : num  8.43 8.08 8.87 7.6 7.54 ...\n $ gene_35          : num  8.34 8.18 8.06 8.79 8.8 ...\n $ gene_36          : num  7.84 8.3 8.2 7.82 8.05 ...\n $ gene_37          : num  8.43 9.52 9.07 8.28 8.88 ...\n $ gene_38          : num  8.78 9.98 9.22 9.27 8.98 ...\n $ gene_39          : num  6.87 6.19 6.3 5.78 6.11 ...\n $ gene_40          : num  11.1 10.7 10.8 10.7 11.4 ...\n $ gene_41          : num  9.21 7.15 8.6 5.2 5.9 ...\n $ gene_42          : num  9 6.96 7.74 7.21 7.66 ...\n $ gene_43          : num  7.39 6.07 6.79 6.7 6.46 ...\n $ gene_44          : num  9.48 8.83 8.49 8.74 10.09 ...\n $ gene_45          : num  4.78 7.44 7.09 6.89 7.13 ...\n $ gene_46          : num  7.84 9.75 8.41 9.7 9.39 ...\n $ gene_47          : num  9.17 8.53 8.83 7.74 7.46 ...\n $ gene_48          : num  8.42 8.41 8.95 7.38 7.06 ...\n $ gene_49          : num  7.18 7.25 7.45 6.99 6.94 ...\n $ gene_50          : num  7.93 7.18 7.6 7.37 7.6 ...\n $ gene_51          : num  5.43 5.76 5.72 6.2 5.69 ...\n $ gene_52          : num  6.68 6.6 6.35 7.35 6.98 ...\n $ gene_53          : num  8.27 7.81 8.48 7.42 8.01 ...\n $ gene_54          : num  7.73 7.84 7.61 9.21 8.28 ...\n $ gene_55          : num  7.65 7.43 7.61 6.57 7.12 ...\n $ gene_56          : num  9.82 11.92 10.52 12.27 11.62 ...\n $ gene_57          : num  9.97 9.99 9.66 10.4 10.09 ...\n $ gene_58          : num  7.87 5.55 6.54 4.29 4.83 ...\n $ gene_59          : num  9.06 10.87 10.13 9.65 9.71 ...\n $ gene_60          : num  6.87 7.84 8.28 7.42 7.47 ...\n $ gene_61          : num  6.91 5.87 6.01 6.52 6.67 ...\n $ gene_62          : num  8.68 8.81 8.92 9.36 8.77 ...\n $ gene_63          : num  5.12 4.97 5.13 4.89 5.17 ...\n $ gene_64          : num  10.66 8.63 9.46 8.34 8.96 ...\n $ gene_65          : num  8.93 8.82 8.35 8.07 7.76 ...\n $ gene_66          : num  9.62 8.71 8.94 8.74 8.56 ...\n $ gene_67          : num  6.77 6.92 6.94 6.64 6.64 ...\n $ gene_68          : num  9.85 8.11 9.24 8.39 8.94 ...\n $ gene_69          : num  6.43 6.69 6.63 5.66 5.91 ...\n $ gene_70          : num  7.23 6.44 7.12 6.02 6.42 ...\n $ gene_71          : num  8.98 8.83 8.74 9.01 8.74 ...\n $ gene_72          : num  9.38 8.19 8.7 8.47 8.84 ...\n $ gene_73          : num  8.33 8.69 9.39 9.03 7.84 ...\n $ gene_74          : num  8.87 10.39 10.11 9.81 9.93 ...\n $ gene_75          : num  9.42 7.99 8.5 7.07 7.32 ...\n $ gene_76          : num  8.83 8.06 8.09 7.54 7.83 ...\n $ gene_77          : num  5.13 6.01 5.6 5.5 5.26 ...\n $ gene_78          : num  6.5 7.77 7.28 6.79 6.29 ...\n $ gene_79          : num  7.19 7.43 8.1 8.45 8.17 ...\n $ gene_80          : num  7.84 7.63 7.82 8.5 8.5 ...\n $ gene_81          : num  7.53 8.77 7.86 7.33 7.27 ...\n $ gene_82          : num  10.29 6.95 8.81 7.3 8.14 ...\n $ gene_83          : num  7.21 6.96 6.79 6.25 6.76 ...\n $ gene_84          : num  8.53 8.66 8.53 7.92 8.42 ...\n $ gene_85          : num  8.72 7.01 7.91 7.4 7.52 ...\n $ gene_86          : num  9.76 7.75 8.29 9.36 9.29 ...\n $ gene_87          : num  7.36 6.59 6.37 7.87 7.61 ...\n $ gene_88          : num  8.1 8.88 8.67 9.1 9.03 ...\n $ gene_89          : num  9.56 7.91 8.79 7.01 7.39 ...\n  [list output truncated]\n\n\n\n2.4.1 Notes for analysis\nAn important first stage in any statistical or machine learning modelling is the exploratory analysis in which we use simple metrics of visualization tools to have a first glimpse of the dataset we have in hands. In this sense, we can use figures like histograms or boxplots that represent the distribution of continuous features and scatterplots that show the relationship between two continuous variables\nFor example, we can produce, Boxplots of response_percent by treatment, histograms of response_percent, and scatter plots of baseline_tumor_mm vs post_tumor_mm.\nIn summary, in our clinical trial example, response_percent is the main continuous endpoint, high_response is its clinically relevant binary version, treatment and dose_intensity define the intervention, and clinical plus gene variables explain for whom and how much the chemotherapy works.\nOur objective in this chapter will be:\nFor linear regression:\n\nReport estimated coefficients, standard errors, t-values, and p-values for each predictor.\nEvaluate model fit using metrics such as R² and adjusted R² to assess explained variability.\nCheck residual diagnostics (normality, homoscedasticity, and influential points) to verify model assumptions.\nPlot fitted vs observed values and residual vs fitted to visually inspect model adequacy.\n\n\n\nReport confidence intervals for main effects (e.g., treatment effect, dose effect).\n\n\n\nSummarize predicted responses or marginal means by treatment group for interpretation and communication of clinical impact.\n\n\n\nFor LASSO, Ridge and Elastic Net: use cross-validation to select the regularization parameter λ, and compare their stability and sparsity.\n\nIn your report, distinguish clearly between:\n\nAverage treatment effects (chemo vs no_chemo)\n\nModulating effects (genes, age, grade, performance, dose intensity)\n\nFor logistic regression: report odds ratios with 95% CIs and evaluate ROC/AUC and calibration.\n\nWe start usually by understanding if we have any bits of the data that may be missing. Using this code below we can see that our dataset is indeed complete. We will dedicate a section on missing data in future chapters. In the case of our motivational example we have a complete dataset.\n\ntype_map &lt;- tibble::tibble(\n  variable    = names(trial_ct),\n  class       = sapply(trial_ct, \\(x) paste(class(x), collapse = \"/\")),\n  n_missing   = sapply(trial_ct, \\(x) sum(is.na(x))),\n  pct_missing = round(100 * sapply(trial_ct, \\(x) mean(is.na(x))), 2),\n  n_unique    = sapply(trial_ct, \\(x) dplyr::n_distinct(x)),\n  example     = sapply(trial_ct, \\(x) paste(utils::head(unique(x), 3), collapse = \", \"))\n)\ntype_map %&gt;% arrange(desc(class))\n\n# A tibble: 2,010 × 6\n   variable          class   n_missing pct_missing n_unique example             \n   &lt;chr&gt;             &lt;chr&gt;       &lt;int&gt;       &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;               \n 1 dose_intensity    numeric         0           0      302 1.085, 0, 1.007     \n 2 patient_age       numeric         0           0       61 81, 61, 74          \n 3 baseline_tumor_mm numeric         0           0      912 52.2, 43.3, 43.2    \n 4 post_tumor_mm     numeric         0           0      869 25.8, 43.3, 43.2    \n 5 response_percent  numeric         0           0      665 50.6, 0, 4.2        \n 6 gene_01           numeric         0           0    10000 11.0132382528657, 9…\n 7 gene_02           numeric         0           0    10000 9.31355524977464, 7…\n 8 gene_03           numeric         0           0    10000 9.51020822690023, 8…\n 9 gene_04           numeric         0           0    10000 10.272629360758, 9.…\n10 gene_05           numeric         0           0    10000 7.30636755315514, 8…\n# ℹ 2,000 more rows\n\n\n\np + scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, .05)))\n\n\n\n\n\n\n\n\nAn important first stage in any statistical or machine learning modelling is the exploratory analysis in which we use simple metrics of visualization tools to have a first glimpse of the dataset we have in hands. In this sense, we can use figures like histograms or boxplots that represent the distribution of continuous features and scatterplots that show the relationship between two continuous variables\nFor example, we can produce, Boxplots of response_percent by treatment, histograms of response_percent, and scatter plots of baseline_tumor_mm vs post_tumor_mm.\nBelow we have the codes to produce some exploratory graphics\n\np1 &lt;- ggplot(trial_ct, aes(response_percent)) + geom_histogram(bins = 30) +\n  labs(x = \"Response percent\", y = \"Count\")\np2 &lt;- ggplot(trial_ct, aes(factor(high_response))) + geom_bar() +\n  labs(x = \"High response (≥30%)\", y = \"Count\")\np3 &lt;- ggplot(trial_ct, aes(baseline_tumor_mm)) + geom_histogram(bins = 30) +\n  labs(x = \"Baseline tumor (mm)\", y = \"Count\")\np4 &lt;- ggplot(trial_ct, aes(patient_age)) + geom_histogram(bins = 30) +\n  labs(x = \"Age (years)\", y = \"Count\")\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)\n\n\n\n\nDistributions of outcomes and key covariates.\n\n\n\n\n\nggplot(trial_ct, aes(baseline_tumor_mm, post_tumor_mm, color = treatment)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = 2) +\n  labs(x = \"Baseline (mm)\", y = \"Post (mm)\", color = \"Treatment\")\n\n\n\n\nBaseline vs post-treatment tumor size.\n\n\n\n\n\nggplot(trial_ct, aes(treatment, response_percent)) +\n  geom_boxplot() +\n  labs(x = \"Treatment\", y = \"Response percent\")\n\n\n\n\nResponse by treatment.\n\n\n\n\n\nggplot(trial_ct, aes(treatment, response_percent)) +\n  geom_boxplot(outlier.shape = NA, width = 0.6) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, fill = \"white\") +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", width = 0.15) +\n  labs(x = \"Treatment\", y = \"Response percent\")\n\n\n\n\nResponse by treatment with raw points and mean ± 95% CI.\n\n\n\n\nWe can also create some summaries of the dataset. For example we can summarise the reponse_percent feature in each of the groups.\n\ntrial_ct %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(\n    n      = n(),\n    mean   = mean(response_percent),\n    sd     = sd(response_percent),\n    median = median(response_percent),\n    q1     = quantile(response_percent, 0.25),\n    q3     = quantile(response_percent, 0.75)\n  )\n\n# A tibble: 2 × 7\n  treatment     n  mean    sd median    q1    q3\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 no_chemo   5078  4.83  6.81    0.8   0     8.2\n2 chemo      4922 37.6  10.9    37.3  30.2  44.8\n\n\nWe can also perform the calculation of some correlations to determine associations between features. Plotting this correlations is also important helping us to discuss the level of association between features. The code below calculates the correlation between the level of expression of each gene and the response_percent feature.\n\ngene_cols &lt;- grep(\"^gene_\", names(trial_ct), value = TRUE)\ncors &lt;- sapply(gene_cols, function(g) {\n  suppressWarnings(cor(trial_ct[[g]], trial_ct$response_percent, use = \"pairwise.complete.obs\"))\n})\ntop10_names &lt;- names(sort(abs(cors), decreasing = TRUE))[1:10]\ntop10_df &lt;- tibble::tibble(\n  gene = top10_names,\n  cor  = unname(cors[top10_names])\n) %&gt;%\n  dplyr::mutate(gene = reorder(gene, abs(cor)))\n\nggplot(top10_df, aes(x = gene, y = cor, fill = cor &gt; 0)) +\n  geom_col() +\n  coord_flip() +\n  guides(fill = \"none\") +\n  labs(x = \"Gene\", y = \"Correlation with response_percent\",\n       title = \"Top 10 genes by absolute correlation (signed)\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#metrics-for-model-comparison",
    "href": "supervised_regression.html#metrics-for-model-comparison",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.5 Metrics for model comparison",
    "text": "2.5 Metrics for model comparison\nBefore comparing regression methods, it is important to understand the metrics used to evaluate how well a model predicts continuous or binary outcomes. Each metric captures a different aspect of model performance: accuracy, precision, or calibration and helps interpret how reliable a model’s predictions are in practice.\nWe will focus on following main evaluation metrics commonly used for regression and classification.\n\n2.5.1 Error\nThe error (also called the residual) for each observation is:\n\\[\ne_i = \\hat{y}_i - y_i\n\\]\nWhere: (y_i) is the observed (true) value for subject (i); (_i) is the model prediction; (e_i) is the individual prediction error. Positive error = overestimation; negative error = underestimation.\nExample:\n\nsuppressPackageStartupMessages(library(ggplot2))\nset.seed(1)\ndf_err &lt;- data.frame(\n  patient   = 1:8,\n  observed  = c(20, 35, 40, 60, 75, 85, 95, 100),\n  predicted = c(18, 38, 37, 65, 71, 89, 92, 102)\n)\ndf_err$error &lt;- df_err$predicted - df_err$observed\n\nggplot(df_err, aes(x = patient)) +\n  geom_segment(aes(y = observed, yend = predicted, xend = patient), linewidth = 0.7) +\n  geom_point(aes(y = observed), size = 2) +\n  geom_point(aes(y = predicted), size = 2, shape = 17) +\n  labs(x = \"Patient\", y = \"Response (%)\",\n       title = \"Prediction error (eᵢ = ŷᵢ − yᵢ): observed vs predicted\")\n\n\n\n\n\n\n\nFigure 2.1\n\n\n\n\n\n\n\n2.5.2 Mean Absolute Error (MAE)\nThe mean absolute error summarizes the average magnitude of errors, ignoring their sign:\n\\[\n\\mathrm{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |e_i| = \\frac{1}{n}\\sum_{i=1}^{n} |\\hat{y}_i - y_i|\n\\]\nMAE gives equal weight to all errors and is less sensitive to outliers than MSE/RMSE.\nInterpretation: if MAE = 4, predictions are, on average, 4 units away from the observed values (e.g., 4 percentage points of tumor reduction).\n\n\n2.5.3 Mean Square Error (MSE)\nThe mean square error measures the average squared deviation:\n\\[\n\\mathrm{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n} e_i^2\n\\]\nBy squaring, large errors are penalized quadratically, making MSE more sensitive to occasional large misses useful when big mistakes are costly (for example, underestimating toxicity).\n\n\n2.5.4 Root Mean Square Error (RMSE)\nThe root mean square error is the square root of MSE, returning to the original units of the outcome:\n\\[\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2} = \\sqrt{\\mathrm{MSE}}\n\\]\nRMSE can be read as the typical magnitude (standard deviation) of prediction errors; it penalizes large errors more than MAE but remains directly interpretable.\n\n\n2.5.5 Compact comparison\n\n\n\n\n\n\n\n\n\n\nMetric\nFormula\nPenalizes large errors strongly?\nUnits\nPractical meaning\n\n\n\n\nError\n\\((e_i = \\hat{y}_i - y_i)\\)\n\nOutcome units\nDirection and size of each residual\n\n\nMAE\n\\((\\frac{1}{n}\\sum\\)\ne_i\n)\nNo (linear)\n\n\nMSE\n\\((\\frac{1}{n}\\sum e_i^2)\\)\nYes (quadratic)\nSquared units\nAverage squared deviation\n\n\nRMSE\n\\((\\sqrt{\\frac{1}{n}\\sum e_i^2})\\)\nYes (quadratic)\nOutcome units\nTypical error size (√MSE)\n\n\n\n\n\n2.5.6 Visual comparison (how metrics respond to error size)\n\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyr))\n\ndf_metrics &lt;- data.frame(error = seq(-10, 10, by = 0.25)) |&gt;\n  mutate(\n    abs_error = abs(error),\n    sq_error  = error^2,\n    rmse_unit = sqrt(error^2)  # same as abs(error), shown for relation to RMSE\n  ) |&gt;\n  pivot_longer(\n    cols = c(abs_error, sq_error, rmse_unit),\n    names_to = \"metric\", values_to = \"value\"\n  ) |&gt;\n  mutate(\n    # Ensure fixed factor level order so colors map correctly\n    metric = factor(metric, levels = c(\"abs_error\", \"sq_error\", \"rmse_unit\"))\n  )\n\nlabs_map &lt;- c(abs_error = \"MAE contribution |e|\",\n              sq_error  = \"MSE contribution e²\",\n              rmse_unit = \"RMSE unit √(e²)\")\n\n# Map colors by named vector to avoid level-order surprises\ncol_map &lt;- c(\n  abs_error = \"#2f63c0\",  # blue\n  sq_error  = \"#ff7f50\",  # orange/coral\n  rmse_unit = \"#66c2a5\"   # green\n)\n\nggplot(df_metrics, aes(x = error, y = value, color = metric, linetype = metric)) +\n  geom_line(linewidth = 1) +\n  scale_color_manual(values = col_map, breaks = names(labs_map), labels = labs_map) +\n  scale_linetype_manual(values = c(abs_error = \"solid\", sq_error = \"dashed\", rmse_unit = \"dotdash\"),\n                        breaks = names(labs_map), labels = labs_map) +\n  labs(x = \"Error (eᵢ = ŷᵢ − yᵢ)\", y = \"Contribution\", color = NULL, linetype = NULL,\n       title = \"How MAE, MSE, and RMSE respond to increasing error magnitude\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 2.2\n\n\n\n\n\n\n\n2.5.7 Worked numeric example to illustrate how to calculate errors, mae, mse and rmse and to show them in a dataframe.\n\nerrors &lt;- c(-5, 2, -3, 8, 1, -2, 4, -1)\nmae  &lt;- mean(abs(errors))\nmse  &lt;- mean(errors^2)\nrmse &lt;- sqrt(mse)\ndata.frame(Metric = c(\"MAE\",\"MSE\",\"RMSE\"),\n           Value  = c(mae, mse, rmse))\n\n  Metric     Value\n1    MAE  3.250000\n2    MSE 15.500000\n3   RMSE  3.937004\n\n\n\n\n2.5.8 Notes for analysis\n\nUse MAE when you want an intuitive average error magnitude, robust to outliers.\n\nUse RMSE when large errors must be penalized more heavily (and to keep units interpretable).\n\nReport both MAE and RMSE for a balanced view; and residual diagnostics (validity).\n\nAlways compute metrics with cross-validation to estimate out-of-sample performance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#fitting-simple-linear-regression-to-our-data",
    "href": "supervised_regression.html#fitting-simple-linear-regression-to-our-data",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.6 Fitting simple linear regression to our data",
    "text": "2.6 Fitting simple linear regression to our data\nWe will fit the following model for our motivational example\n\\[Y = w_0 + w_1 X_1 + w_2 X_2 + \\ldots + w_p X_p + \\varepsilon\\]\nThat in our particular case will be:\n\\[\n\\text{response\\_percent} =\nw_0 +\nw_1(\\text{compound\\_dose}) +\nw_2(\\text{patient\\_age}) +\nw_3(\\text{disease\\_type}) +\nw_4(\\text{gene\\_expression\\_1}) +\nw_5(\\text{gene\\_expression\\_2}) +\n\\ldots +\nw_{2000}(\\text{gene\\_expression\\_2000}) +\n\\varepsilon\n\\]\nThe response variable (outcome) is response_percent the continuous measure of how much the tumor shrank (efficacy minus toxicity). The explanatory variables (predictors) include everything else in the dataset (.) except: patient_id (just an identifier) high_response (the binary version of the same outcome)\nSo our explanatory variables include: Gene expression features (gene_expression_1 … gene_expression_20) Clinical variables (compound_dose, patient_age, disease_type, etc.)\n\n2.6.1 Model assumptions for linear regression\nWhen we fit a linear regression model, we make several assumptions about the relationship between the predictors (X’s) and the outcome (Y). These assumptions matter because they affect whether the estimated coefficients and statistical tests can be trusted.\n\nplot_pair &lt;- function(data_good, data_bad, xvar, yvar, title_good, title_bad, xlabel, ylabel) {\np1 &lt;- ggplot(data_good, aes({{xvar}}, {{yvar}})) +\ngeom_point(alpha = 0.7, color = \"#2f63c0\") +\ngeom_smooth(method = \"lm\", se = FALSE, color = \"#ff7f50\") +\nlabs(title = paste(\"Correct:\", title_good), x = xlabel, y = ylabel) +\ntheme_minimal()\np2 &lt;- ggplot(data_bad, aes({{xvar}}, {{yvar}})) +\ngeom_point(alpha = 0.7, color = \"#2f63c0\") +\ngeom_smooth(method = \"lm\", se = FALSE, color = \"#ff7f50\") +\nlabs(title = paste(\"Incorrect:\", title_bad), x = xlabel, y = ylabel) +\ntheme_minimal()\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n}\n\n\nLinearity The relationship between each predictor and the outcome is assumed to be linear (on the model’s scale). If the true relationship is curved or nonlinear, the model may systematically under- or over-predict. Check: residuals vs fitted values or vs individual predictors should look patternless (no curves).\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nIndependence of errors Residuals (errors) should be independent across observations. This is especially important for time series or clustered data (e.g., repeated measures, multi-center studies). Violation: autocorrelation or clustering inflates apparent precision.\n\n\n\n\n\n\nIndependence of errors: correct (i.i.d.) vs incorrect (strong AR(1)). Each row shows residuals over order and their ACF.\n\n\n\n\n\nHomoscedasticity (constant variance) The spread of residuals should be roughly constant across fitted values. If residuals fan out as predictions increase, that’s heteroskedasticity. Check: residuals vs fitted plot where spread should be similar across the range. Fixes: transform the outcome, use weighted least squares, or robust (HC) standard errors.\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nNormality of residuals Residuals should be approximately normally distributed around zero. This mainly underpins valid p-values and confidence intervals (less critical for pure prediction). Check: Q–Q plot or histogram of standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\nNo multicollinearity Predictors should not be highly correlated with each other. Severe collinearity makes individual coefficient estimates unstable and hard to interpret. Check: Variance Inflation Factor (VIF); values &gt; 5 (or &gt; 10) suggest issues.\n\n\n\n\n\n\n\n\n\n\n\nNo influential outliers A few extreme points should not unduly determine the fit. Check: leverage and Cook’s distance; large Cook’s D indicates influential observations worth investigation.\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhen these assumptions are reasonably met, Ordinary Least Squares (OLS) yields unbiased, efficient estimates and meaningful inference about how predictors relate to the response. In practice, diagnostic plots and simple tests help verify whether the model behaves well for the data at hand.\nWe know use the fabulous property of R to create functions and we will create three functions one to calculate mae and rmse which will be performed simultaneosly by the eval_perform function.\n\n# ---- 0) Metrics (MAE and RMSE) ----\n\nmae  &lt;- function(y, yhat) mean(abs(y - yhat))\nrmse &lt;- function(y, yhat) sqrt(mean((y - yhat)^2))\n\n# ---- evaluation function (MAE and RMSE) ----\neval_perf &lt;- function(y_true, y_pred) {\n  tibble(\n    MAE  = mae(y_true, y_pred),\n    RMSE = rmse(y_true, y_pred)\n  )\n}\n# alias to avoid mistyping\neval_perform &lt;- eval_perf\n\nAfter that we split the data set into training (70%) of the data and test (30%) of the data.\n\nset.seed(42)\n# ---- 1) Train/Test split (70/30) ----\nn  &lt;- nrow(trial_ct)\nix &lt;- sample.int(n, size = floor(0.7 * n))\ntrain &lt;- trial_ct[ix, , drop = FALSE]\ntest  &lt;- trial_ct[-ix, , drop = FALSE]\n\nThen we can remove some unecessary columns from the dataset.\n\n# ---- 2) Remove columns that must NOT enter the model ----\n# (ID and the binary endpoint; keeps the supervised task as a pure regression)\n\ndrop_cols &lt;- intersect(names(train), c(\"patient_id\", \"high_response\", \"baseline_tumor_mm\", \"post_tumor_mm\"))\ntrain_nopii &lt;- dplyr::select(train, -dplyr::all_of(drop_cols))\ntest_nopii  &lt;- dplyr::select(test,  -dplyr::all_of(drop_cols))\n\nAfter that we write some code that will inform to R the formula we want to consider for this model.\n\n# Common formula for OLS (and to derive terms/dummies for glmnet)\nf_ols &lt;- response_percent ~ .\n\nThe function for fitting a linear regression in R is `lm` to which we indicate the formula of the model and the training dataset to be used for construction of the model. When we write ols_tmp &lt;- lm(f_ols, data = train_nopii) we are fitting a temporary linear model on the trainingset only to let R learn the exact design it should use: which predictors are in the model, how factors are encoded (their levels and the chosen reference), and the precise structure of any interactions or transformations. Extracting ols_terms &lt;- terms(ols_tmp) gives us that blueprint of the design matrix.\nLater, when we build matrices with model.matrix(ols_terms, data = train_nopii) and model.matrix(ols_terms, data = test_nopii), both training and test data are transformed with the same blueprint. This prevents issues like “factor has new levels in test,” mismatched dummy columns, or different column ordering problems that would otherwise break penalized models (e.g., glmnet) or yield inconsistent predictions. In short: we lock in the training design so the test set is encoded identically, guaranteeing compatible inputs for all models.\n\nThese steps are important because we will use the same training and testing datasets for running Lasso and Ridge Regression later using the glmnet package.\n\n# ---- 3) Capture TRAIN terms to ensure identical dummies in TEST ----\n# (fit a temporary OLS only to extract terms & factor levels)\n\nols_tmp   &lt;- lm(f_ols, data = train_nopii)\nols_terms &lt;- terms(ols_tmp)\n\n\n# Consistent design matrices for glmnet (no intercept column)\nX_train &lt;- model.matrix(ols_terms, data = train_nopii)[, -1, drop = FALSE]\nX_test  &lt;- model.matrix(ols_terms, data = test_nopii)[,  -1, drop = FALSE]\ny_train &lt;- train_nopii$response_percent\ny_test  &lt;- test_nopii$response_percent\n\nNo we proceed with the Ordinary Least Square regression analysis of our motivational example.\nIn this block of code we are fitting and evaluating our baseline Ordinary Least Squares (OLS) regression model. The command mod_ols &lt;- lm(f_ols, data = train_nopii) fits a linear regression model using only the training data. The formula f_ols expresses that we want to predict response_percent, our continuous measure of tumor shrinkage, using all available explanatory variables except for patient_id and high_response. These two variables are removed because one is simply an identifier and the other is a binary version of the same outcome, which would leak information into the model. The resulting object mod_ols contains the estimated coefficients and fitted values for the training set.\nNext, pred_ols_train &lt;- predict(mod_ols, newdata = train_nopii) generates predictions for the same training data used to fit the model. These are the in-sample predictions. They allow us to evaluate how well the model fits the data it has already seen and to compute basic performance metrics such as the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE). They are also used to inspect diagnostic plots that reveal potential problems such as nonlinearity, heteroskedasticity, or outliers.\nFinally, pred_ols_test &lt;- predict(mod_ols, newdata = test_nopii) applies the trained OLS model to the independent test dataset, which was not used during model fitting. These predictions are used to assess the model’s ability to generalize to new data. Comparing the errors on the training and test sets helps us detect whether the model is overfitting (performing much better on training data than on unseen data) or underfitting. Because we ensured earlier that categorical variables and factor levels are defined consistently between training and test sets, the prediction step runs smoothly without level-mismatch errors. In summary, this three-step process fits the baseline OLS model, obtains fitted and predicted values, and provides the foundation for fair comparison with the regularized models (LASSO and Ridge) that will be trained using the same data split.\n\n# ---- 4) OLS (uses data.frame; glmnet uses X/y) ----\nmod_ols        &lt;- lm(f_ols, data = train_nopii)\npred_ols_train &lt;- predict(mod_ols, newdata = train_nopii)\npred_ols_test  &lt;- predict(mod_ols, newdata = test_nopii)\n\nIn this block we prepare and run standard OLS diagnostics to check whether the linear model assumptions look reasonable. First, we load helper packages: ggplot2 and dplyr for plotting and data handling, lmtest for tests like Breusch–Pagan and Durbin–Watson, sandwich for robust variance estimators, and car for tools such as variance inflation factors, component-plus-residual plots, and outlier checks. Then par(mfrow = c(2, 2)) tells base R to arrange four plots in a 2 by 2 grid. The call plot(mod_ols) draws the default diagnostic panel for a fitted lm object: Residuals vs Fitted to look for nonlinearity or heteroskedasticity, Normal Q-Q to assess approximate normality of residuals, Scale–Location to check whether residual spread is roughly constant across fitted values, and Residuals vs Leverage with Cook’s distances to flag influential observations. Together these plots provide a quick visual screening of model adequacy before we proceed to formal tests or alternative specifications.\n\n  #| label: ols-assumptions\n  #| message: false\n  #| warning: false\n  suppressPackageStartupMessages({\n    library(ggplot2)\n    library(dplyr)\n    library(lmtest)    # bptest(), dwtest()\n    library(sandwich)  # robust (HC) variance estimators\n    library(car)       # vif(), crPlots(), outlierTest()\n  })\n  \n  # --- 1) Quick base-R diagnostic panel (residuals, QQ, Scale-Location, Residuals vs Leverage)\nop &lt;- par(mfrow = c(2, 2))\nplot(mod_ols)\n\n\n\n\n\n\n\n\nResiduals vs Fitted\nThis plot examines whether the residuals are centered around zero and whether there is any systematic pattern. The points here are scattered roughly around the horizontal line with no obvious curve, suggesting that the relationship between predictors and outcome is reasonably linear. There is a slight spread increase for larger fitted values, but it does not seem severe. Overall, the assumption of linearity and constant variance appears acceptable.\nNormal Q–Q\nThe Q–Q plot compares the standardized residuals to what would be expected if they followed a normal distribution. Most points lie very close to the diagonal reference line, except for a few at the extreme tails. This indicates that the residuals are approximately normal, with only minor deviations that are unlikely to affect inference materially.\nScale–Location (Spread–Location)\nThis plot checks whether the variance of residuals is constant across the range of fitted values (homoskedasticity). The red line is nearly flat, and the spread of the points is fairly uniform across the x-axis. There is no strong funnel shape or trend, so the homoskedasticity assumption is reasonably satisfied.\nResiduals vs Leverage\nThis plot identifies influential cases observations that have both high leverage (unusual predictor combinations) and large residuals (poorly fitted). Most points lie within the Cook’s distance contours, indicating that no single case is exerting excessive influence on the fitted model. A few observations (such as those labeled 800 or 1920) have higher leverage, but they do not appear to distort the overall fit.\nOverall interpretation\nTaken together, these diagnostics suggest that the OLS model fits the data adequately. The linearity, normality, and constant-variance assumptions hold reasonably well, and there are no major outliers or influential points. Minor departures at the extremes are typical in real data and do not undermine the general validity of the model.\nThe next block of code is a compact toolkit to inspect, interpret, and summarize your fitted OLS model from several complementary angles. It starts with summary(mod_ols), which is the classic regression report. You get one row per coefficient with its estimate, standard error, t statistic, and p value, plus model-level diagnostics such as the residual standard error, R², adjusted R², and the F test for the null that all slopes are zero. Read this first to see direction and magnitude of effects and whether they are statistically distinguishable from zero after adjusting for the other variables in the model.\nNext it calls anova(mod_ols), which produces a Type I (sequential) ANOVA table. Here, sums of squares and p values are computed in the order that predictors enter the model. This is useful when there is a natural hierarchy or pre-specified entry order, but results can change if you reorder columns. If you need hypothesis tests that adjust for all other terms regardless of order (especially with factors and interactions), you would use car::Anova(mod_ols, type = 3) instead, which provides Type III tests.\nThen it builds a tidy coefficient table with broom::tidy(mod_ols, conf.int = TRUE). This converts the model output into a clean data frame that includes coefficient estimates, standard errors, test statistics, p values, and 95% confidence intervals. The code then arranges rows by p value and prints everything, which is convenient for scanning the most and least significant terms in a reproducible, table-friendly format.\nThe next step creates a quick ranking of “importance” by absolute t statistic. It removes the intercept, computes abs_t = |t|, sorts descending, and shows the top terms with their estimates, standard errors, test statistics, p values, and confidence intervals. This does not replace more formal variable-importance methods, but it is a fast way to see which coefficients have the strongest signal relative to their uncertainty within this linear specification.\nFinally, broom::glance(mod_ols) provides a one-row summary of overall fit metrics. You get R² and adjusted R² (explanatory power with and without a penalty for model size), sigma (residual standard deviation), the model F statistic and its p value, and information criteria such as AIC and BIC for comparing alternative models on a goodness-of-fit versus complexity trade-off.\nTaken together, these outputs let you check individual effects with uncertainty, evaluate sequential or fully adjusted hypothesis tests, order terms by signal-to-noise, and assess global model quality, all in tidy objects that can be reported or plotted downstream.\nIn the context of ANOVA, significance tells us whether a factor or variable has a real, measurable effect on the outcome, rather than the observed differences being just due to random chance.\nThe p-value is the probability of seeing a difference (or a larger one) in the data if, in reality, the factor had no effect at all that is, if the null hypothesis were true.\nWhen a p-value is very small (typically below 0.05), it means such a difference would be very unlikely to appear by random chance, so we have evidence to reject the null hypothesis and conclude that the variable probably does influence the outcome.\nIn simple terms, ANOVA uses p-values as a decision criterion:\n\nA small p-value (below 0.05) → the group differences or predictor effects are statistically significant.\nA large p-value (above 0.05) → the observed differences could easily occur by chance, so we do not have strong evidence of a real effect.\n\nSignificance does not measure the size or importance of the effect it only measures how confident we are that an effect exists at all.\nIn a regression model, the parameters (often called coefficients or betas) quantify how much the response variable changes when a given predictor changes, while holding all other predictors constant.\nEach parameter represents the direction and magnitude of that predictor’s influence on the outcome.\n\nA positive coefficient means that as the predictor increases, the response tends to increase as well.\nA negative coefficient means that higher values of the predictor are associated with lower response values.\nA coefficient close to zero suggests that the variable has little or no linear impact on the outcome, once the other predictors are accounted for.\n\nThe absolute value of the coefficient reflects how strong the relationship is (how sensitive the response is to changes in that variable), but the units of measurement matter: one unit of a gene-expression score does not mean the same as one year of age, so direct comparisons of raw coefficients can be misleading.\nWhen variables are standardized (converted to the same scale), larger absolute coefficients indicate stronger effects.\nStatistical tests (t values and p values) assess whether each coefficient is significantly different from zero. A small p value suggests that the estimated effect is unlikely to be due to random noise, providing evidence that this predictor truly contributes to explaining variation in the response.\nFollowing we have the code to generat the items above for our mod_ols object. We will avoid printing them here because each of the items would have thousands of elements… This brings to our minds the complexities of informing outputs of linear models with thousand of explanatories mantained by the model.\nThe next chunck of code evaluates how well the OLS model predicts the tumor response in both the training and test datasets, using two standard regression error metrics: MAE (Mean Absolute Error) and RMSE (Root Mean Square Error).\nThe first two lines extract the true outcome values the observed tumor shrinkage percentages (response_percent) from the training and testing subsets. The next two blocks call the function eval_perf(), which calculates MAE and RMSE by comparing the true values (y_train or y_test) to the model’s predictions (pred_ols_train or pred_ols_test).\nRecalling:\n\nMAE represents the average absolute difference between predicted and observed values it tells how far off the predictions are, on average, in the same units as the outcome (percentage points of tumor reduction).\nRMSE is similar but gives more weight to large errors, making it more sensitive to occasional poor predictions.\n\nThe results are stored in small tibbles and then combined into one table with the model name (“OLS”) and dataset origin (“Train” or “Test”).\n\n# --- Evaluate OLS with MAE and RMSE (using eval_perf) ---\n# y vectors (true values)\ny_train &lt;- train_nopii$response_percent\ny_test  &lt;- test_nopii$response_percent\n\nperf_ols_train &lt;- eval_perf(y_train, pred_ols_train) |&gt;\n  dplyr::mutate(Model = \"OLS\", Dataset = \"Train\")\n\nperf_ols_test &lt;- eval_perf(y_test, pred_ols_test) |&gt;\n  dplyr::mutate(Model = \"OLS\", Dataset = \"Test\")\n\n# Compact table\ndplyr::bind_rows(perf_ols_train, perf_ols_test)\n\n# A tibble: 2 × 4\n    MAE  RMSE Model Dataset\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  \n1  1.27  1.59 OLS   Train  \n2  1.83  2.28 OLS   Test   \n\n\nThe model fits the training data quite closely the average prediction error is about 1.3 percentage points. When applied to unseen data (the test set), the errors increase moderately to around 1.8–2.3 points.\nThis comparison is crucial because it shows generalization performance  how well the model performs on new data that were not used for training.\n\nThe training performance tells you how well the model explains patterns already seen during fitting.\nThe testing performance reveals how well those learned relationships extend to new, unseen patients.\n\nIf the test errors are only slightly higher than the training errors, as in your case, it suggests a good model that generalizes reasonably well. If test errors were much larger, it would indicate overfitting  the model memorized the training data instead of learning the general structure. Conversely, if both errors were high, it would point to underfitting, meaning the model is too simple to capture important relationships.\nIn summary, checking both training and test performance allows you to balance fit quality and predictive reliability, ensuring that the regression model is not only accurate on known data but also trustworthy for future predictions.\n\nAs a general guideline, whenever you a see a work quoting a linear regression try to understand if the cross validation technique was applied. You will be amazed on how many works do not bother about these details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#some-words-on-regularization",
    "href": "supervised_regression.html#some-words-on-regularization",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.7 Some words on regularization",
    "text": "2.7 Some words on regularization\nWe discussed in a previous section that the parameters of the model are important for interpretation because they tell us how each variable contributes to explaining the outcome. However, in practice, when we move from small, well-controlled models to modern biomedical or omics data, we often face a very different scenario: instead of a few predictors such as age, dose, or tumor grade, we may have hundreds or thousands of gene-expression features. In this context, interpreting the individual coefficients becomes nearly impossible. Many of them will be correlated with one another, some may carry redundant information, and others may simply represent random noise. Traditional linear regression tends to overfit in such high-dimensional settings it tries to give every variable a non-zero weight, which leads to unstable and unreliable estimates that generalize poorly to new data.\nTo address this, machine-learning methods introduce the idea of regularization, also called shrinkage. The key idea is to constrain or penalize the size of the coefficients so that the model prefers simpler explanations that still fit the data well. Regularization discourages the algorithm from assigning large weights to predictors that do not truly improve predictive accuracy.\nTwo common approaches are Ridge regression and LASSO regression. Ridge regression applies an L2 penalty, which shrinks all coefficients toward zero but rarely eliminates them completely; it is particularly effective when many predictors have small, distributed effects. LASSO regression, in contrast, uses an L1 penalty, which can shrink some coefficients exactly to zero, performing variable selection at the same time as estimation. This means that LASSO can automatically identify a smaller subset of genes or variables that carry most of the predictive signal, making the model both simpler and easier to interpret.### Ordinary Least Squares Sum of Squares\n\n2.7.1 Ordinary Least Squares Sum of Squares\nThe residual sum of squares (RSS) minimized by Ordinary Least Squares (OLS) is:\n\\[\n\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n           = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2\n\\]\nOLS estimates the parameters ( ) that minimize this quadratic error term, without any regularization.\n\n\n2.7.2 Ridge Regression (L2 regularization)\nRidge regression introduces a penalty proportional to the L2 norm of the coefficients:\n\\[\nL_{\\text{ridge}}(\\boldsymbol{\\beta})\n= \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2\n+ \\lambda \\sum_{j=1}^{p} \\beta_j^2\n\\]\nwhere ( ) controls the penalty strength.\nThe L2 term shrinks all coefficients toward zero but does not set them exactly to zero.\nIt is especially useful when predictors are highly correlated.\n\n\n2.7.3 Lasso Regression (L1 regularization)\nLasso regression adds a penalty proportional to the L1 norm of the coefficients:\n\\[\nL_{\\text{lasso}}(\\boldsymbol{\\beta})\n= \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2\n+ \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\]\nThe L1 term encourages sparsity, meaning that some coefficients are driven exactly to zero, effectively performing variable selection.\n\n\n2.7.4 Elastic Net (Combination of L1 and L2)\nThe Elastic Net combines both Ridge (L2) and Lasso (L1) penalties:\n\\[\nL_{\\text{elastic-net}}(\\boldsymbol{\\beta})\n= \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2\n+ \\lambda \\left[\n  \\alpha \\sum_{j=1}^{p} |\\beta_j|\n  + (1 - \\alpha) \\sum_{j=1}^{p} \\beta_j^2\n\\right]\n\\]\nwhere ( 0 ) controls the mix between L1 and L2 regularization:\n\n( = 1 ) → Lasso\n\n( = 0 ) → Ridge\n\n( 0 &lt; &lt; 1 ) → Elastic Net\n\nElastic Net is particularly effective when there are many correlated predictors: it keeps Ridge’s stability while still allowing Lasso-style variable selection.\nIn the next section we will see how to perform these three types of regression: LASSO, ridge regression, and elastic net.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#hyperparameters",
    "href": "supervised_regression.html#hyperparameters",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.8 Hyperparameters",
    "text": "2.8 Hyperparameters\nIn \\(Y=f\\left(x_1, x_2, \\ldots, x_n\\right)+\\) error, the parameters are the quantities inside the function \\(f\\) that the algorithm learns from the data to make \\(f\\) fit well (e.g., the \\(b\\) s in a linear model\nA hyperparameter is a setting that controls how f is learned, not something learned directly from the data by the usual fitting step. Hyperparameters define the shape/complexity of the function class you allow and how aggressively you search within it. They live outside f, but they constrain and guide the learning of f.\nHyperparameters are configuration choices that control the learning process and the capacity of the model class used to approximate \\(f(\\cdot)\\). They are not learned from the training loss directly; instead, they are selected (e.g., via cross-validation) to achieve good generalization, helping \\(f\\) capture the signal in \\(Y\\) without fitting the random error.\nIn supervised learning we seek to approximate an unknown function \\(f(\\cdot)\\) that links explanatory features \\(x_1, x_2, \\ldots, x_n\\) to a response variable \\(Y\\) :\n\\[\nY=f\\left(x_1, x_2, \\ldots, x_n\\right)+\\varepsilon,\n\\]\nwhere \\(\\varepsilon\\) captures random noise and unmeasured influences. When \\(f(\\cdot)\\) is assumed to be linear, the model becomes\n\\[\n\\hat{Y}=w_0+\\beta_1 x_1+\\cdots+w_p x_p,\n\\]\nand the learning task consists of estimating the coefficients \\(w_j\\) that minimize prediction error. These coefficients are the model parameters-they are learned directly from the data.\nHowever, in modern regression we often introduce an additional layer of control: hyperparameters, which determine how the coefficients are estimated and how much flexibility the model is allowed to have. Hyperparameters live outside the function \\(f(\\cdot)\\); they are not part of the fitted equation but instead regulate the learning process.\nOLS and the absence of hyperparameters Ordinary Least Squares (OLS) minimizes the Residual Sum of Squares (RSS):\n\\[\n\\mathrm{RSS}=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2 .\n\\]\nThe solution for \\(\\beta\\) has a closed analytical form and depends only on the data. Because there is no external control over model complexity, OLS has no hyperparameters. Its flexibility is entirely determined by the number of predictors in the model.\nWhile OLS is unbiased and efficient under ideal conditions, it becomes unstable when predictors are highly correlated or when \\(p\\) (number of variables) is large relative to \\(n\\).\nTo improve generalization, we introduce regularization-penalties that shrink coefficients toward zero and prevent overfitting.\nRidge, Lasso, and Elastic Net: controlling complexity with penalties Regularized regression modifies the OLS loss by adding a penalty term that constrains the magnitude of the coefficients.\nThe resulting objective function is\n\\[\n\\operatorname{RSS}_{\\text {penalized }}=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2+\\lambda P(\\boldsymbol{\\beta}),\n\\]\nwhere \\(P(\\boldsymbol{\\beta})\\) defines the type of penalty and \\(\\lambda&gt;0\\) is a hyperparameter that controls its strength.\n\n\n\n\n\n\n\n\n\nModel\nPenalty term ( P() )\nMain hyperparameters\nInterpretation\n\n\n\n\nRidge\n\\(( \\sum_j \\beta_j^2 )\\) (L2)\n\\(( \\lambda )\\)\nShrinks coefficients toward zero smoothly; keeps all variables.\n\n\nLasso\n\\(( \\sum_j |\\beta_j| )\\) (L1)\n\\(( \\lambda )\\)\nShrinks some coefficients exactly to zero → automatic variable selection.\n\n\nElastic Net\n\\(( (1-\\alpha)\\sum_j \\beta_j^2/2 + \\alpha\\sum_j |\\beta_j|)\\)\n\\(( \\lambda, \\alpha )\\)\nCombines both effects; balances stability (Ridge) and sparsity (Lasso).\n\n\n\nUnderstanding \\(\\lambda\\) : the bias-variance control knob The hyperparameter \\(\\lambda\\) regulates how strongly the model is penalized: - Small \\(\\lambda \\rightarrow\\) minimal penalty, coefficients close to OLS estimates.\nLow bias, high variance (risk of overfitting). - Large \\(\\lambda \\rightarrow\\) heavy penalty, coefficients shrink strongly.\nHigher bias, lower variance (risk of underfitting). Tuning \\(\\lambda\\) therefore manages the bias-variance trade-off, shaping the smoothness and generalization capacity of the learned function \\(f(\\cdot)\\).\nTuning and selection Unlike \\(\\beta\\), hyperparameters are not optimized by minimizing training error. If we simply fitted the model for the smallest training loss, \\(\\lambda\\) would always shrink toward zero (i.e., revert to OLS). Instead, hyperparameters are chosen using cross-validation, evaluating predictive error on unseen folds of the data.\nThe value of \\(\\lambda\\) (and \\(\\alpha\\) for Elastic Net) that minimizes cross-validated error-or is within one standard error of the minimum (the 1-SE rule)-is selected as optimal.\nAfter choosing the hyperparameters, the model is re-trained on the full training set to estimate the final coefficients.\nConceptual summary - Parameters ( \\(w_j\\) ) define the learned relationship \\(f(\\cdot)\\). - Hyperparameters ( \\(\\lambda, \\alpha\\) ) define how the relationship is learned-controlling model flexibility and generalization. - OLS has no hyperparameters; Ridge, Lasso, and Elastic Net introduce \\(\\lambda\\) (and possibly \\(\\alpha\\) ) to regularize the model. - Choosing good hyperparameters ensures \\(f(X)\\) captures the true signal in \\(Y\\) rather than noise in \\(\\varepsilon\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#fitting-lasso-regression",
    "href": "supervised_regression.html#fitting-lasso-regression",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.9 Fitting LASSO Regression",
    "text": "2.9 Fitting LASSO Regression\nThe next block of code uses the glmnet package to fit a LASSO regression and to choose its penalty strength by cross-validation. The call to cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10) runs a ten-fold cross-validation loop over a grid of lambda values with the L1 penalty, which defines the LASSO. For each lambda the algorithm fits the model on nine folds and evaluates prediction error on the remaining fold, then averages the error across folds. glmnet standardizes predictors internally by default, which is important so that the penalty treats variables on the same footing regardless of their scale.\nFrom this cross-validation object we extract lambda.min, the lambda that achieved the smallest mean cross-validated error. We then refit the model on the full training set with glmnet(X_train, y_train, alpha = 1, lambda = cv_lasso$lambda.min). This produces a single LASSO model whose coefficients have been shrunk toward zero. Many uninformative coefficients are exactly zero, which performs built-in variable selection and improves interpretability while controlling variance.\nFinally, we obtain predicted values for both the training and the test sets with predict(mod_lasso, newx = X_train)and predict(mod_lasso, newx = X_test). These predictions allow us to compute performance metrics such as MAE and RMSE on data used to fit the model and on held-out data. Evaluating both is useful because the training metrics show how well the model can fit observed samples, while the test metrics indicate how well the model generalizes to new patients. Some analysts also report lambda.1se, which is the largest lambda within one standard error of the minimum error. That choice usually yields a sparser model with similar predictive accuracy and can be attractive when parsimony is a priority.\nThe next three chunks’ outputs show the fitted LASSO regression model and how it performs on the training and testing data. Unlike ordinary least squares, which gives every variable a non-zero coefficient, LASSO includes a penalty that forces many coefficients exactly to zero. The result is a simpler model that keeps only the most informative predictors.\nThe table of coefficients lists all variables that remain active after regularization. The first line, the intercept (27.99), represents the baseline predicted tumor response (in percentage points) when all other predictors are at their reference or zero levels. The next few coefficients correspond to clinical factors. For instance, treatmentchemo = 3.11 means that, on average and holding other variables constant, patients who received chemotherapy are predicted to have about three percentage points higher tumor shrinkage than those who did not. The coefficient for dose_intensity = 3.34 indicates that stronger chemotherapy doses are associated with greater reductions in tumor size. Age has a small positive coefficient, suggesting a very mild increase in response with age, while the negative sign for performance_score shows that patients with worse functional status tend to respond less effectively to treatment. Tumor grade also has a clear effect, with higher grades showing larger coefficients and therefore stronger responses.\nAfter the clinical covariates, the list continues with gene expression variables. Only a fraction of the hundreds of available genes appear, meaning that the LASSO has automatically selected those with the most predictive value. For example, gene_08, gene_14, and gene_19 have relatively large positive coefficients (around five), identifying them as strong predictors of greater tumor reduction. In contrast, genes such as gene_05 or gene_1778 have large negative coefficients, implying that higher expression of these genes is associated with poorer therapeutic response. The many small coefficients near zero reflect genes with weak or marginal contributions that the model nonetheless retained under the chosen regularization strength. In situation where the number of features to be considered is giant LASSO returns a smaller reasonable amount of variables compared with the original number, making interpretation of the model more facilitated.\nThe last section of the output shows model performance using the eval_perf() function. The MAE (mean absolute error) and RMSE (root mean square error) values quantify how far, on average, the predictions are from the true tumor responses. The training errors are MAE = 1.49 and RMSE = 1.87, while the test errors are MAE = 1.57 and RMSE = 1.96. The similarity of these values suggests that the model generalizes well: it fits the training data closely but not excessively and maintains similar predictive accuracy on unseen test patients.\nOverall, this LASSO fit yields a parsimonious model that identifies a handful of relevant clinical variables and a limited subset of genes that together explain much of the variation in tumor response. The regularization penalty has successfully balanced interpretability and predictive performance by shrinking or eliminating irrelevant coefficients while preserving the key biological and clinical signals in the data.\n\n# ---- 5) LASSO (alpha = 1) with CV to choose lambda ----\n# Note: glmnet standardizes features by default (standardize = TRUE).\ncv_lasso  &lt;- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)\nmod_lasso &lt;- glmnet(X_train, y_train, alpha = 1, lambda = cv_lasso$lambda.min)\npred_lasso_train &lt;- as.numeric(predict(mod_lasso, newx = X_train))\npred_lasso_test  &lt;- as.numeric(predict(mod_lasso, newx = X_test))\n\n\n# Nonzero coefficients selected by LASSO\ncoef_lasso &lt;- coef(mod_lasso)\nnz &lt;- which(coef_lasso != 0)\nas.matrix(coef_lasso[nz, , drop = FALSE])\n\n                             s0\n(Intercept)       27.9894094231\ntreatmentchemo     3.1094150609\ndose_intensity     3.3438592381\npatient_age        0.0132882299\ntumor_gradeG2      0.1316015652\ntumor_gradeG3      0.5547952038\nperformance_score -0.2943083530\ngene_01            0.8494564076\ngene_05           -2.7310539064\ngene_08            5.1436742116\ngene_12            0.4288133459\ngene_14            5.1013463595\ngene_19            5.1746627223\ngene_27            0.0180460908\ngene_30           -0.0863467996\ngene_32           -0.0361982838\ngene_104           0.0211677652\ngene_116          -0.0218406711\ngene_117          -0.0052744538\ngene_123           0.0056347167\ngene_146           0.0498361359\ngene_157          -0.0279668435\ngene_165          -0.0080882163\ngene_187           0.0222580351\ngene_210          -0.0036941301\ngene_221           0.0096723727\ngene_225           0.0644057360\ngene_242           0.0684388670\ngene_253          -0.0077187155\ngene_287           0.0498245130\ngene_311          -0.0424858360\ngene_359          -0.0232784490\ngene_373          -0.0568150918\ngene_416           0.0008864799\ngene_459           0.0443358981\ngene_484           0.0190060279\ngene_490           0.0172107152\ngene_503          -0.0079158529\ngene_514          -0.0248283328\ngene_535          -0.0577584654\ngene_537          -0.0072821908\ngene_550           0.0375150155\ngene_576          -0.0704458018\ngene_600           0.0158202468\ngene_605           0.0196117982\ngene_627           0.0103392261\ngene_648          -0.0034662864\ngene_669           0.0008621344\ngene_670           0.0132388638\ngene_696          -0.0155218791\ngene_720          -0.0435232964\ngene_757           0.0366479160\ngene_760          -0.0310746046\ngene_795          -0.0038151634\ngene_804           0.0071026491\ngene_840           0.0162446672\ngene_864          -0.0570875844\ngene_874           0.0108188945\ngene_877          -0.0084290925\ngene_911           0.0303575087\ngene_916          -0.0135076583\ngene_956          -0.0240584781\ngene_969          -0.0054693674\ngene_976          -0.0569288581\ngene_980           0.0015706121\ngene_1013          0.0291775413\ngene_1024          0.0417414081\ngene_1051          0.0308411643\ngene_1059         -0.0030128212\ngene_1071         -0.0383864043\ngene_1084          0.0129566818\ngene_1096         -0.0226222064\ngene_1117          0.0243128673\ngene_1120          0.0600242242\ngene_1137         -0.0634424258\ngene_1150         -0.0576782902\ngene_1161          0.0050430799\ngene_1237          0.0121748732\ngene_1241          0.1164865560\ngene_1255          0.0025642628\ngene_1265         -0.0059853938\ngene_1292         -0.0114866186\ngene_1306          0.0069838046\ngene_1363         -0.0465489365\ngene_1376         -0.0113973153\ngene_1419          0.1117123380\ngene_1421          0.0343775734\ngene_1428         -0.1140875119\ngene_1437         -0.0166104156\ngene_1442          0.0553966751\ngene_1476         -0.0179387369\ngene_1490         -0.0272055274\ngene_1506          0.1071247869\ngene_1518         -0.0434573068\ngene_1556          0.0845555983\ngene_1564         -0.0539480912\ngene_1602         -0.0595800034\ngene_1610         -0.0390650740\ngene_1678          0.0569013521\ngene_1685         -0.0028281900\ngene_1691         -0.0119213530\ngene_1708          0.0561465041\ngene_1737          0.0029350844\ngene_1738          0.0040687069\ngene_1746          0.0015397073\ngene_1748         -0.0079077542\ngene_1761          0.0616664687\ngene_1763          0.0344047529\ngene_1775          0.0004665486\ngene_1778         -0.1198702861\ngene_1846         -0.0227816308\ngene_1858         -0.0464153923\ngene_1871         -0.0011798398\ngene_1892          0.0823542888\ngene_1927          0.0532531012\ngene_1976         -0.1120061133\ngene_1994          0.0020424817\n\n\n\n# Compare errors with your eval_perf()\nperf_lasso_train &lt;- eval_perf(y_train, pred_lasso_train) |&gt; dplyr::mutate(Model=\"LASSO\", Dataset=\"Train\")\nperf_lasso_test  &lt;- eval_perf(y_test,  pred_lasso_test)  |&gt; dplyr::mutate(Model=\"LASSO\", Dataset=\"Test\")\ndplyr::bind_rows(perf_lasso_train, perf_lasso_test)\n\n# A tibble: 2 × 4\n    MAE  RMSE Model Dataset\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  \n1  1.49  1.87 LASSO Train  \n2  1.57  1.96 LASSO Test",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#fitting-ridge-regression",
    "href": "supervised_regression.html#fitting-ridge-regression",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.10 Fitting Ridge Regression",
    "text": "2.10 Fitting Ridge Regression\nIn the following chunks of code we will implement a ridgre regression model.\nThe Ridge model is trained using cv.glmnet() with alpha = 0, which specifies the L2 penalty. The algorithm performs 10-fold cross-validation over a grid of possible penalty values (lambda) and identifies the one that minimizes the mean prediction error. The value of lambda.min = 1.83 is the penalty that achieved the lowest average cross-validation error, while lambda.1se = 2.01 corresponds to a slightly stronger penalty that still performs within one standard error of the minimum. The model is then refitted on the entire training dataset with this optimal lambda, and predictions are generated for both training and test sets.\nThe code then computes the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) for each model OLS, LASSO, and Ridge on both the training and test sets using the eval_perf() function. These metrics quantify how close the model’s predictions are to the observed tumor response. The Ridge model shows MAE = 1.43 and RMSE = 1.79 on the training data, and MAE = 1.77 and RMSE = 2.23 on the test data. Compared to OLS (MAE = 1.83, RMSE = 2.28 on the test set), Ridge performs slightly better, reducing both bias and variance without overfitting. Its performance is similar to that of LASSO but typically smoother, since Ridge keeps all variables in the model rather than setting some coefficients exactly to zero.\nThe Ridge model coefficients provide additional insight. In contrast to LASSO, which eliminates many predictors, Ridge keeps all coefficients non-zero (2,006 in this dataset) but shrinks them toward zero depending on their contribution strength. The list of the top fifteen coefficients shows that the variables with the strongest influence on tumor response are consistent with previous models: dose_intensity and treatmentchemo have the largest positive effects, indicating that higher doses and chemotherapy are associated with greater tumor shrinkage. Several genes such as gene_14, gene_19, and gene_08 also have strong positive associations, while gene_05 has a large negative coefficient, suggesting a detrimental effect on treatment response. Other predictors, such as tumor grade and performance score, have smaller but directionally meaningful coefficients that align with clinical expectations.\nOverall, these results illustrate the essence of Ridge regularization: instead of discarding predictors as LASSO does, it shrinks all coefficients toward zero, reducing overfitting and stabilizing estimates when many correlated variables (such as thousands of gene expressions) are present. The outcome is a model that generalizes better than OLS, retains all variables but with moderated influence, and highlights which features exert the most consistent effects across the dataset.\n\n# ---- 6) Ridge (alpha = 0) with CV ----\ncv_ridge  &lt;- cv.glmnet(X_train, y_train, alpha = 0, nfolds = 10)\nmod_ridge &lt;- glmnet(X_train, y_train, alpha = 0, lambda = cv_ridge$lambda.min)\npred_ridge_train &lt;- as.numeric(predict(mod_ridge, newx = X_train))\npred_ridge_test  &lt;- as.numeric(predict(mod_ridge, newx = X_test))\n\n\n# ---- Performance (MAE, RMSE) using eval_perform / eval_perf ----\nperf_ridge_train &lt;- eval_perform(y_train, pred_ridge_train) |&gt;\n  dplyr::mutate(Model = \"Ridge\", Dataset = \"Train\")\n\nperf_ridge_test &lt;- eval_perform(y_test, pred_ridge_test) |&gt;\n  dplyr::mutate(Model = \"Ridge\", Dataset = \"Test\")\n\ndplyr::bind_rows(perf_ridge_train, perf_ridge_test)\n\n# A tibble: 2 × 4\n    MAE  RMSE Model Dataset\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  \n1  1.43  1.79 Ridge Train  \n2  1.77  2.23 Ridge Test   \n\n# ---- Quick model “summary” for discussion ----\n# Lambdas chosen by CV\ncv_ridge$lambda.min\n\n[1] 1.82942\n\ncv_ridge$lambda.1se\n\n[1] 2.007787\n\n# Coefficients at lambda.min\nridge_coefs &lt;- as.matrix(coef(mod_ridge))\nridge_coef_tbl &lt;- tibble::tibble(\n  term     = rownames(ridge_coefs),\n  estimate = as.numeric(ridge_coefs[, 1])\n)\n\n# How many non-zero coefficients (excluding intercept)\nnnz &lt;- ridge_coef_tbl |&gt;\n  dplyr::filter(term != \"(Intercept)\", estimate != 0) |&gt;\n  nrow()\n\nnnz\n\n[1] 2006\n\n# Top coefficients by absolute magnitude (exclude intercept)\nridge_top &lt;- ridge_coef_tbl |&gt;\n  dplyr::filter(term != \"(Intercept)\") |&gt;\n  dplyr::mutate(abs_est = abs(estimate)) |&gt;\n  dplyr::arrange(dplyr::desc(abs_est)) |&gt;\n  dplyr::slice(1:15)\n## seeing the top 15 features regarding estimate\nridge_top\n\n# A tibble: 15 × 3\n   term              estimate abs_est\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n 1 dose_intensity       5.28    5.28 \n 2 treatmentchemo       5.03    5.03 \n 3 gene_14              4.33    4.33 \n 4 gene_19              4.30    4.30 \n 5 gene_08              4.29    4.29 \n 6 gene_05             -3.50    3.50 \n 7 gene_01              0.983   0.983\n 8 tumor_gradeG3        0.949   0.949\n 9 gene_12              0.526   0.526\n10 performance_score   -0.517   0.517\n11 tumor_gradeG2        0.283   0.283\n12 gene_1419            0.269   0.269\n13 gene_1506            0.233   0.233\n14 gene_1976           -0.215   0.215\n15 gene_907            -0.213   0.213\n\n\n\n# ---- 7) Elastic Net (tune alpha and lambda via CV) ----\nsuppressPackageStartupMessages({\n  library(glmnet)\n  library(dplyr)\n  library(tibble)\n  # assumes eval_perform() already defined and X_train, X_test, y_train, y_test exist\n})\n\nset.seed(42)\n\n# Grid of alpha values (0=ridge, 1=lasso)\nalpha_grid &lt;- seq(0.05, 0.95, by = 0.05)\n\n# For each alpha, run cv.glmnet to pick lambda; record CV error at lambda.min\ncv_list &lt;- lapply(alpha_grid, function(a) {\n  cv.glmnet(X_train, y_train, alpha = a, nfolds = 10)\n})\n\ncv_errors &lt;- sapply(cv_list, function(cv) min(cv$cvm))  # lower is better\nbest_idx  &lt;- which.min(cv_errors)\nbest_alpha &lt;- alpha_grid[best_idx]\nbest_cv    &lt;- cv_list[[best_idx]]\nbest_lambda_min &lt;- best_cv$lambda.min\nbest_lambda_1se &lt;- best_cv$lambda.1se\n\nbest_alpha\n\n[1] 0.15\n\nbest_lambda_min\n\n[1] 0.1811088\n\nbest_lambda_1se\n\n[1] 0.2883763\n\n# Fit final Elastic Net at best alpha/lambda\nmod_enet &lt;- glmnet(X_train, y_train, alpha = best_alpha, lambda = best_lambda_min)\n\n# Predictions\npred_enet_train &lt;- as.numeric(predict(mod_enet, newx = X_train))\npred_enet_test  &lt;- as.numeric(predict(mod_enet, newx = X_test))\n\n# Performance\nperf_enet_train &lt;- eval_perform(y_train, pred_enet_train) |&gt;\n  mutate(Model = \"Elastic Net\", Dataset = \"Train\", alpha = best_alpha, lambda = best_lambda_min)\n\nperf_enet_test &lt;- eval_perform(y_test, pred_enet_test) |&gt;\n  mutate(Model = \"Elastic Net\", Dataset = \"Test\", alpha = best_alpha, lambda = best_lambda_min)\n\nbind_rows(perf_enet_train, perf_enet_test)\n\n# A tibble: 2 × 6\n    MAE  RMSE Model       Dataset alpha lambda\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1  1.50  1.88 Elastic Net Train    0.15  0.181\n2  1.57  1.95 Elastic Net Test     0.15  0.181\n\n# Coefficients summary\nenet_coefs &lt;- as.matrix(coef(mod_enet))\nenet_coef_tbl &lt;- tibble(\n  term     = rownames(enet_coefs),\n  estimate = as.numeric(enet_coefs[, 1])\n)\n\n# Count non-zero (excluding intercept)\nenet_nnz &lt;- enet_coef_tbl |&gt;\n  filter(term != \"(Intercept)\", estimate != 0) |&gt;\n  nrow()\nenet_nnz\n\n[1] 87\n\n# Top coefficients by absolute value (exclude intercept)\nenet_top &lt;- enet_coef_tbl |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(abs_est = abs(estimate)) |&gt;\n  arrange(desc(abs_est)) |&gt;\n  slice(1:20)\n\nenet_top\n\n# A tibble: 20 × 3\n   term              estimate abs_est\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n 1 gene_19             5.08    5.08  \n 2 gene_08             5.01    5.01  \n 3 gene_14             4.98    4.98  \n 4 dose_intensity      3.54    3.54  \n 5 treatmentchemo      3.49    3.49  \n 6 gene_05            -2.92    2.92  \n 7 gene_01             0.863   0.863 \n 8 tumor_gradeG3       0.606   0.606 \n 9 gene_12             0.456   0.456 \n10 performance_score  -0.320   0.320 \n11 tumor_gradeG2       0.141   0.141 \n12 gene_1976          -0.110   0.110 \n13 gene_1419           0.107   0.107 \n14 gene_1241           0.106   0.106 \n15 gene_1428          -0.0990  0.0990\n16 gene_1778          -0.0947  0.0947\n17 gene_1506           0.0947  0.0947\n18 gene_1892           0.0793  0.0793\n19 gene_1556           0.0705  0.0705\n20 gene_30            -0.0676  0.0676",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#elastic-net",
    "href": "supervised_regression.html#elastic-net",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.11 Elastic Net",
    "text": "2.11 Elastic Net\nThe next chunk fits an Elastic Net regression and tunes its two key hyperparameters using cross-validation, then evaluates how well it predicts on unseen data and inspects the model’s coefficients.\nFirst, it defines a grid of candidate mixing parameters alpha from 0.05 to 0.95. Elastic Net blends Ridge and LASSO: alpha equal to 0 behaves like Ridge, alpha equal to 1 behaves like LASSO, values in between trade off between the two penalties. For each alpha in the grid, cv.glmnet() runs 10-fold cross-validation over a sequence of lambda values and records the mean cross-validated error. The code then picks the alpha that achieves the lowest error across its lambda path, identifies the corresponding best lambda at the minimum error (lambda.min) and also records lambda.1se which is a slightly stronger penalty within one standard error of the minimum. In your run, the best alpha is 0.15, with lambda.minabout 0.181 and lambda.1se about 0.288. This indicates that a model closer to Ridge than to LASSO gave the best cross-validated performance on this dataset.\nWith the best alpha and lambda.min fixed, the code refits a final Elastic Net model on the full training matrix and generates predictions for both training and test sets. It then computes MAE and RMSE via eval_perform(). Your results show MAE 1.50 and RMSE 1.88 on training, and MAE 1.57 and RMSE 1.95 on test. The similarity between train and test errors suggests good generalization with limited overfitting. Test performance is competitive with or slightly better than the individual Ridge and LASSO models you fitted earlier, which is a common outcome when Elastic Net can borrow strengths from both penalties in the presence of many correlated predictors.\nFinally, the code examines the coefficient vector. It converts the sparse coefficient matrix to a tibble, counts how many coefficients are non-zero excluding the intercept, and ranks predictors by absolute magnitude. You obtained 87 non-zero coefficients, which is far sparser than Ridge and far denser than a very aggressive LASSO, reflecting the balance enforced by alpha 0.15. The top effects are biologically and clinically interpretable: gene_19, gene_08, and gene_14 have the largest positive coefficients, dose_intensity and treatmentchemo are also strongly positive, while gene_05 is strongly negative, and performance_score is moderately negative. Tumor grade indicators and several additional genes appear with smaller but non-zero effects. This pattern is what Elastic Net is designed to produce in high-dimensional settings with correlated features: it keeps groups of correlated predictors, shrinks them toward zero to control variance, and still performs variable selection to improve interpretability and prediction.\n\n# ---- 7) Elastic Net (tune alpha and lambda via CV) ----\nsuppressPackageStartupMessages({\n  library(glmnet)\n  library(dplyr)\n  library(tibble)\n  # assumes eval_perform() already defined and X_train, X_test, y_train, y_test exist\n})\n\nset.seed(42)\n\n# Grid of alpha values (0=ridge, 1=lasso)\nalpha_grid &lt;- seq(0.05, 0.95, by = 0.05)\n\n# For each alpha, run cv.glmnet to pick lambda; record CV error at lambda.min\ncv_list &lt;- lapply(alpha_grid, function(a) {\n  cv.glmnet(X_train, y_train, alpha = a, nfolds = 10)\n})\n\ncv_errors &lt;- sapply(cv_list, function(cv) min(cv$cvm))  # lower is better\nbest_idx  &lt;- which.min(cv_errors)\nbest_alpha &lt;- alpha_grid[best_idx]\nbest_cv    &lt;- cv_list[[best_idx]]\nbest_lambda_min &lt;- best_cv$lambda.min\nbest_lambda_1se &lt;- best_cv$lambda.1se\n\nbest_alpha\n\n[1] 0.15\n\nbest_lambda_min\n\n[1] 0.1811088\n\nbest_lambda_1se\n\n[1] 0.2883763\n\n# Fit final Elastic Net at best alpha/lambda\nmod_enet &lt;- glmnet(X_train, y_train, alpha = best_alpha, lambda = best_lambda_min)\n\n# Predictions\npred_enet_train &lt;- as.numeric(predict(mod_enet, newx = X_train))\npred_enet_test  &lt;- as.numeric(predict(mod_enet, newx = X_test))\n\n# Performance\nperf_enet_train &lt;- eval_perform(y_train, pred_enet_train) |&gt;\n  mutate(Model = \"Elastic Net\", Dataset = \"Train\", alpha = best_alpha, lambda = best_lambda_min)\n\nperf_enet_test &lt;- eval_perform(y_test, pred_enet_test) |&gt;\n  mutate(Model = \"Elastic Net\", Dataset = \"Test\", alpha = best_alpha, lambda = best_lambda_min)\n\nbind_rows(perf_enet_train, perf_enet_test)\n\n# A tibble: 2 × 6\n    MAE  RMSE Model       Dataset alpha lambda\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1  1.50  1.88 Elastic Net Train    0.15  0.181\n2  1.57  1.95 Elastic Net Test     0.15  0.181\n\n# Coefficients summary\nenet_coefs &lt;- as.matrix(coef(mod_enet))\nenet_coef_tbl &lt;- tibble(\n  term     = rownames(enet_coefs),\n  estimate = as.numeric(enet_coefs[, 1])\n)\n\n# Count non-zero (excluding intercept)\nenet_nnz &lt;- enet_coef_tbl |&gt;\n  filter(term != \"(Intercept)\", estimate != 0) |&gt;\n  nrow()\nenet_nnz\n\n[1] 87\n\n# Top coefficients by absolute value (exclude intercept)\nenet_top &lt;- enet_coef_tbl |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(abs_est = abs(estimate)) |&gt;\n  arrange(desc(abs_est)) |&gt;\n  slice(1:20)\n\nenet_top\n\n# A tibble: 20 × 3\n   term              estimate abs_est\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n 1 gene_19             5.08    5.08  \n 2 gene_08             5.01    5.01  \n 3 gene_14             4.98    4.98  \n 4 dose_intensity      3.54    3.54  \n 5 treatmentchemo      3.49    3.49  \n 6 gene_05            -2.92    2.92  \n 7 gene_01             0.863   0.863 \n 8 tumor_gradeG3       0.606   0.606 \n 9 gene_12             0.456   0.456 \n10 performance_score  -0.320   0.320 \n11 tumor_gradeG2       0.141   0.141 \n12 gene_1976          -0.110   0.110 \n13 gene_1419           0.107   0.107 \n14 gene_1241           0.106   0.106 \n15 gene_1428          -0.0990  0.0990\n16 gene_1778          -0.0947  0.0947\n17 gene_1506           0.0947  0.0947\n18 gene_1892           0.0793  0.0793\n19 gene_1556           0.0705  0.0705\n20 gene_30            -0.0676  0.0676",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#comparing-ols-lasso-ridge-and-elastic-net-regression",
    "href": "supervised_regression.html#comparing-ols-lasso-ridge-and-elastic-net-regression",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.12 Comparing OLS, LASSO, Ridge and Elastic Net Regression",
    "text": "2.12 Comparing OLS, LASSO, Ridge and Elastic Net Regression\nThis section compares the performance and interpretability of the four regression models Ordinary Least Squares (OLS), LASSO, Ridge, and Elastic Net using the same training and testing datasets. The code constructs two tables, one for the training set and one for the test set, summarizing each model’s Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). These metrics quantify how close the predicted tumor responses are to the observed values: lower numbers indicate more accurate predictions.\nOn the training data, OLS achieves the smallest apparent errors (MAE = 1.27, RMSE = 1.59), which is expected because it freely adjusts all coefficients without any penalty. However, its flexibility can also lead to overfitting, meaning it might perform worse on new, unseen data. The Ridge model (MAE = 1.43, RMSE = 1.79) and the Elastic Net (MAE = 1.50, RMSE = 1.88) show slightly higher training errors, reflecting the effect of regularization that constrains coefficient size to prevent overfitting. LASSO (MAE = 1.49, RMSE = 1.87) behaves similarly, as it shrinks and even eliminates some coefficients.\nThe test set results are more revealing, since they reflect how well each model generalizes. OLS now has the highest errors (MAE = 1.83, RMSE = 2.28), showing a clear drop in performance compared with the training data. In contrast, the regularized models maintain nearly identical errors across training and test sets. LASSO and Elastic Net both achieve MAE around 1.57 and RMSE near 1.95, while Ridge performs slightly worse but still better than OLS. This stability across datasets indicates that the regularization terms have successfully reduced overfitting, producing models that generalize better to new observations.\nBeyond predictive accuracy, each method differs in interpretability and in how easily its results can be communicated. OLS is the simplest to interpret: every coefficient represents the independent effect of a predictor on the outcome, assuming all other variables are fixed. However, in high-dimensional data such as gene-expression studies, OLS becomes unstable and hard to explain because of multicollinearity and noise. LASSO improves interpretability by setting many coefficients exactly to zero, leaving only a small, focused subset of relevant predictors that can be examined biologically or clinically. Ridge, by contrast, keeps all variables but shrinks their effects toward zero, which stabilizes the estimates but makes it harder to identify a small list of “important” predictors. Elastic Net combines both approaches, preserving correlated groups of variables while still performing selection, offering a compromise between the simplicity of LASSO and the robustness of Ridge.\nIn practical terms, these results illustrate the trade-off between fit, generalization, and interpretability. OLS fits training data best but generalizes poorly. Ridge and Elastic Net offer more stable and realistic predictions in complex, high-dimensional problems. LASSO and Elastic Net, in particular, produce more interpretable models that highlight a concise subset of genes and clinical variables that drive tumor response, making them easier to communicate in biomedical contexts where explanation is as important as prediction.\n\n# ---- 8) Side-by-side performance tables (MAE and RMSE)   now with Elastic Net ----\n# Assumes you already computed:\n#   pred_ols_train,  pred_ols_test\n#   pred_lasso_train, pred_lasso_test\n#   pred_ridge_train, pred_ridge_test\n#   pred_enet_train,  pred_enet_test\n# And (optionally) best_alpha, best_lambda_min from your Elastic Net CV\n\n# Helper: safely carry alpha/lambda for ENet if they exist\nalpha_enet  &lt;- if (exists(\"best_alpha\")) best_alpha else NA_real_\nlambda_enet &lt;- if (exists(\"best_lambda_min\")) best_lambda_min else NA_real_\n\n# Build TRAIN table\nmetrics_train &lt;- tibble(\n  Model = c(\"OLS\", \"LASSO (λ.min)\", \"Ridge (λ.min)\", \"Elastic Net\")\n) |&gt;\n  bind_cols(\n    dplyr::bind_rows(\n      eval_perf(y_train, pred_ols_train),\n      eval_perf(y_train, pred_lasso_train),\n      eval_perf(y_train, pred_ridge_train),\n      eval_perf(y_train, pred_enet_train)\n    )\n  ) |&gt;\n  mutate(Alpha = c(NA, 1.00, 0.00, alpha_enet),\n         Lambda = c(NA, NA, NA, lambda_enet))\n\n# Build TEST table\nmetrics_test &lt;- tibble(\n  Model = c(\"OLS\", \"LASSO (λ.min)\", \"Ridge (λ.min)\", \"Elastic Net\")\n) |&gt;\n  bind_cols(\n    dplyr::bind_rows(\n      eval_perf(y_test, pred_ols_test),\n      eval_perf(y_test, pred_lasso_test),\n      eval_perf(y_test, pred_ridge_test),\n      eval_perf(y_test, pred_enet_test)\n    )\n  ) |&gt;\n  mutate(Alpha = c(NA, 1.00, 0.00, alpha_enet),\n         Lambda = c(NA, NA, NA, lambda_enet))\n\n# Print results\nmetrics_train\n\n# A tibble: 4 × 5\n  Model           MAE  RMSE Alpha Lambda\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 OLS            1.27  1.59 NA    NA    \n2 LASSO (λ.min)  1.49  1.87  1    NA    \n3 Ridge (λ.min)  1.43  1.79  0    NA    \n4 Elastic Net    1.50  1.88  0.15  0.181\n\nmetrics_test\n\n# A tibble: 4 × 5\n  Model           MAE  RMSE Alpha Lambda\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 OLS            1.83  2.28 NA    NA    \n2 LASSO (λ.min)  1.57  1.96  1    NA    \n3 Ridge (λ.min)  1.77  2.23  0    NA    \n4 Elastic Net    1.57  1.95  0.15  0.181\n\n\n\n\n\n\n\n\n\n\n\nModel\nPenalty norm\nVariable selection/Regularization\nExplanation\n\n\n\n\nOLS\n\nNo\nNo regularization , all variables retained.\n\n\nRidge\nL2\nNo\nShrinks coefficients but keeps all variables.\n\n\nLasso\nL1\nYes\nL1 penalty can drive some coefficients to exactly zero.\n\n\nElastic Net\nL1 + L2 (mix)\nPartial\nCombines both effects some zeroing, some shrinkage.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#organizing-our-study-questions",
    "href": "supervised_regression.html#organizing-our-study-questions",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.13 Organizing our study questions",
    "text": "2.13 Organizing our study questions\n\n2.13.1 Main treatment effect\nIn order to answer the first study question we use the following code:\n\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(broom)\n  library(stringr)\n  library(tibble)\n  \n})\n\n# OLS: adjusted main effect of treatment (chemo vs reference)\n\ncoef_ols &lt;- tidy(mod_ols, conf.int = TRUE) %&gt;%\nfilter(grepl(\"^treatment\", term))\ncoef_ols\n\n# A tibble: 1 × 7\n  term           estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 treatmentchemo     3.47     0.420      8.26 1.90e-16     2.65      4.30\n\n# Regularized models: report the treatment coefficient (main effect)\ngrab_coef &lt;- function(m, name) {\nb &lt;- as.matrix(coef(m))\nif (!name %in% rownames(b)) return(NA_real_)\nas.numeric(b[name, 1])\n}\n\nmain_effects_tbl &lt;- tibble(\nModel = c(\"OLS\",\"LASSO\",\"Ridge\",\"Elastic Net\"),\nTreatment_Beta = c(\ncoef(mod_ols)[[\"treatmentchemo\"]],\ngrab_coef(mod_lasso, \"treatmentchemo\"),\ngrab_coef(mod_ridge, \"treatmentchemo\"),\ngrab_coef(mod_enet,  \"treatmentchemo\")\n)\n)\nmain_effects_tbl\n\n# A tibble: 4 × 2\n  Model       Treatment_Beta\n  &lt;chr&gt;                &lt;dbl&gt;\n1 OLS                   3.47\n2 LASSO                 3.11\n3 Ridge                 5.03\n4 Elastic Net           3.49\n\n\nThe last table reports the estimated main treatment effect the adjusted average difference in tumor shrinkage between patients who received chemotherapy and those who did not across four models. The OLS estimate of 3.47 percentage points suggests a clear benefit of chemotherapy after accounting for other covariates. Introducing L1 regularization with LASSO yields a slightly smaller effect of 3.11, consistent with the tendency of sparsity penalties to dampen coefficients in the presence of multicollinearity or weak predictors. Ridge regression, which applies an L2 penalty but retains all variables, produces a larger estimate of 5.03, indicating that when correlated predictors are jointly shrunk rather than selected away, the treatment signal can be expressed more strongly. Elastic Net, blending L1 and L2 penalties, returns 3.49 essentially aligning with OLS while offering greater stability than an unpenalized fit. Taken together, the results are directionally consistent and clinically coherent given the fact that regardless of modeling strategy, chemotherapy is associated with higher average tumor reduction, with the magnitude ranging from roughly three to five percentage points depending on how the method handles correlation and complexity in the predictors.\n\n\n2.13.2 Clinical covariates:\nWhat are the main-effect associations of age, tumor grade, and performance score with tumor shrinkage, controlling for treatment?\nTo answer this question we run the code below which helps us to extract the parameters associated with the variables of interest.\n\n# --- Clinical (no interactions): OLS + LASSO + RIDGE + ENET ------------------\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(broom)\n  library(stringr)\n  library(tibble)\n})\n\n# Helper: make a regex that matches `patient age` with or without backticks (case-insensitive)\nage_regex &lt;- regex(\"^`?patient_age`?$\", ignore_case = TRUE)\n\n# 1) OLS clinical table (with CIs and p-values)\n#    Keep: patient age, performance_score, and ALL tumor_grade dummies\nols_tidy &lt;- tidy(mod_ols, conf.int = TRUE)\n\nols_clinical &lt;- ols_tidy %&gt;%\n  filter(\n    str_detect(term, age_regex) |\n    term == \"performance_score\" |\n    str_detect(term, \"^tumor_grade\")\n  ) %&gt;%\n  arrange(p.value) %&gt;%\n  rename(\n    beta_ols   = estimate,\n    se_ols     = std.error,\n    t_ols      = statistic,\n    p_ols      = p.value,\n    ci_low_ols = conf.low,\n    ci_high_ols= conf.high\n  )\n\n# 2) Helper to extract glmnet coefficients by term ----------------------------\nget_glmnet_coefs &lt;- function(mod_glmnet, keep_regex = NULL, keep_exact_regex = NULL, colname = \"beta\"){\n  B &lt;- as.matrix(coef(mod_glmnet))\n  tab &lt;- tibble(term = rownames(B), !!colname := as.numeric(B[, 1])) %&gt;%\n    filter(term != \"(Intercept)\")\n  tab %&gt;%\n    filter(\n      (if (!is.null(keep_exact_regex)) str_detect(term, keep_exact_regex) else FALSE) |\n      (if (!is.null(keep_regex))       str_detect(term, keep_regex)       else FALSE)\n    )\n}\n\n# Keep exactly patient age (with/without backticks) and performance_score;\n# plus everything that starts with tumor_grade\nkeep_exact_regex &lt;- paste0(\"(\", paste(c(\"performance_score\", age_regex), collapse=\"|\"), \")\")\nkeep_regex_grade &lt;- \"^tumor_grade\"\n\nlasso_tab &lt;- get_glmnet_coefs(mod_lasso, keep_regex = keep_regex_grade,\n                              keep_exact_regex = keep_exact_regex, colname = \"beta_lasso\")\nridge_tab &lt;- get_glmnet_coefs(mod_ridge, keep_regex = keep_regex_grade,\n                              keep_exact_regex = keep_exact_regex, colname = \"beta_ridge\")\nenet_tab  &lt;- get_glmnet_coefs(mod_enet,  keep_regex = keep_regex_grade,\n                              keep_exact_regex = keep_exact_regex, colname = \"beta_enet\")\n\n# 3) Join everything by term --------------------------------------------------\nclin_compare &lt;- ols_clinical %&gt;%\n  select(term, beta_ols, se_ols, t_ols, p_ols, ci_low_ols, ci_high_ols) %&gt;%\n  left_join(lasso_tab, by = \"term\") %&gt;%\n  left_join(ridge_tab, by = \"term\") %&gt;%\n  left_join(enet_tab,  by = \"term\") %&gt;%\n  mutate(\n    beta_lasso = coalesce(beta_lasso, 0),\n    beta_ridge = coalesce(beta_ridge, 0),\n    beta_enet  = coalesce(beta_enet,  0)\n  )\n\ndata.frame(clin_compare)\n\n               term    beta_ols      se_ols     t_ols        p_ols  ci_low_ols\n1       patient_age  0.01708261 0.001573078 10.859354 3.614794e-27  0.01399868\n2 performance_score -0.40130130 0.041437926 -9.684396 5.477501e-22 -0.48253783\n3     tumor_gradeG3  0.69494056 0.074639941  9.310572 1.860192e-20  0.54861349\n4     tumor_gradeG2  0.23598487 0.066305941  3.559030 3.756879e-04  0.10599610\n  ci_high_ols  beta_lasso  beta_ridge   beta_enet\n1  0.02016653  0.01328823  0.02179867  0.01439859\n2 -0.32006476 -0.29430835 -0.51698819 -0.32020568\n3  0.84126763  0.55479520  0.94884736  0.60613069\n4  0.36597363  0.13160157  0.28258697  0.14149931\n\n\nThe last table compares the estimated effects of key clinical predictors on tumor response across four regression approaches ordinary least squares (OLS), LASSO, Ridge, and Elastic Netfocusing on main effects. The coefficients represent how much the tumor shrinkage percentage is expected to change for a one-unit increase in each variable, holding all others constant. The OLS model serves as a baseline reference. The coefficient for patient_age is positive (0.017 ± 0.0016, p ≈ 3.6 × 10⁻²⁷), indicating that, on average, each additional year of age is associated with roughly a 0.017 percentage-point increase in tumor shrinkage. Although small in magnitude, this effect is highly significant, suggesting a subtle but consistent relationship between age and treatment response. The performance_score shows a strong negative association (β = –0.40, p ≈ 5.5 × 10⁻²²), implying that patients with poorer functional status tend to experience smaller reductions in tumor size. Tumor grade, in contrast, shows clear positive effects: compared with the reference category (grade 1), both grade 2 (β = 0.24, p ≈ 3.8 × 10⁻⁴) and grade 3 (β = 0.69, p ≈ 1.9 × 10⁻²⁰) are associated with progressively greater shrinkage, consistent with more aggressive tumors responding more markedly to treatment. When regularization is introduced, the three penalized methods yield broadly similar patterns but with slightly shrunk coefficients. LASSO reduces effect magnitudes toward zero, particularly for performance_score (–0.29) and tumor_gradeG2 (0.13), reflecting its tendency to suppress weaker signals. Ridge regression, which applies a smooth L2 penalty, retains all variables and yields somewhat larger estimates (e.g., tumor_gradeG3 = 0.95), showing how correlated predictors can share information without being zeroed out. Elastic Net, which blends LASSO and Ridge penalties, produces intermediate values that balance sparsity and stability (e.g., performance_score = –0.32, tumor_gradeG3 = 0.61). Across all models, the direction of effects remains stable: older age and higher tumor grade predict better tumor shrinkage, while poorer performance status predicts worse response. Regularization mainly reduces the absolute size of coefficients but does not alter their interpretation. These results reinforce the robustness of the main clinical signals age, performance status, and tumor grade as reliable determinants of treatment response, even under different modeling assumptions and penalty schemes.\n\n# --- Q3. Molecular biomarkers: which genes are associated with response? ----\n# Goal: identify gene predictors associated with continuous response (response_percent)\n# Models used: OLS (with p-values and BH-FDR), LASSO, Ridge, Elastic Net (coefficients)\n# Assumptions:\n#   - Genes are encoded as columns whose names start with \"gene_\" in X_train/X_test\n#   - No interactions in the model\n#   - mod_ols was fit with response_percent ~ clinical + genes (no interactions)\n#   - mod_lasso, mod_ridge, mod_enet were fit with glmnet on (X_train, y_train)\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(stringr)\n  library(tibble)\n  library(broom)\n  library(purrr)\n})\n\n# 0) Identify gene columns from the model matrix you used to fit glmnet\ngene_terms &lt;- colnames(X_train)[str_detect(colnames(X_train), \"^gene_\")]\n\n# --- Helper to extract glmnet coefs for a list of terms ----------------------\nget_glmnet_coefs &lt;- function(mod_glmnet, terms_keep, colname = \"beta\") {\n  B &lt;- as.matrix(coef(mod_glmnet))\n  tibble(term = rownames(B), !!colname := as.numeric(B[, 1])) %&gt;%\n    filter(term != \"(Intercept)\", term %in% terms_keep)\n}\n\n# 1) OLS gene table with t-stat and FDR ---------------------------------------\nols_genes &lt;- tidy(mod_ols, conf.int = TRUE) %&gt;%\n  filter(term %in% gene_terms) %&gt;%\n  mutate(\n    beta_ols = estimate,\n    se_ols   = std.error,\n    t_ols    = statistic,\n    p_ols    = p.value,\n    fdr_bh   = p.adjust(p_ols, method = \"BH\"),\n    imp_ols  = abs(t_ols)             # importance = |t|\n  ) %&gt;%\n  select(term, beta_ols, se_ols, t_ols, p_ols, fdr_bh, imp_ols)\n\ntop10_ols &lt;- ols_genes %&gt;%\n  arrange(desc(imp_ols)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(Model = \"OLS\") %&gt;%\n  select(Model, term, beta = beta_ols, importance = imp_ols, t_ols, p_ols, fdr_bh)\n\n# 2) LASSO / Ridge / Elastic Net gene coefficients (importance = |beta|) ------\nlasso_genes &lt;- get_glmnet_coefs(mod_lasso, gene_terms, colname = \"beta_lasso\") %&gt;%\n  mutate(imp_lasso = abs(beta_lasso))\nridge_genes &lt;- get_glmnet_coefs(mod_ridge, gene_terms, colname = \"beta_ridge\") %&gt;%\n  mutate(imp_ridge = abs(beta_ridge))\nenet_genes  &lt;- get_glmnet_coefs(mod_enet,  gene_terms, colname = \"beta_enet\")  %&gt;%\n  mutate(imp_enet  = abs(beta_enet))\n\ntop10_lasso &lt;- lasso_genes %&gt;%\n  arrange(desc(imp_lasso)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(Model = \"LASSO\") %&gt;%\n  select(Model, term, beta = beta_lasso, importance = imp_lasso)\n\ntop10_ridge &lt;- ridge_genes %&gt;%\n  arrange(desc(imp_ridge)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(Model = \"Ridge\") %&gt;%\n  select(Model, term, beta = beta_ridge, importance = imp_ridge)\n\ntop10_enet &lt;- enet_genes %&gt;%\n  arrange(desc(imp_enet)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(Model = \"Elastic Net\") %&gt;%\n  select(Model, term, beta = beta_enet, importance = imp_enet)\n\n# 3) Combine into a single tidy table -----------------------------------------\ntop10_all_models &lt;- bind_rows(\n  top10_ols,\n  top10_lasso,\n  top10_ridge,\n  top10_enet\n) %&gt;%\n  # rank within model by importance (1 = most important)\n  group_by(Model) %&gt;%\n  arrange(desc(importance), .by_group = TRUE) %&gt;%\n  mutate(rank = row_number()) %&gt;%\n  ungroup()\n\n# View the result (one long table with 10 rows per model)#\n#top10_all_models\n\n# Optional: a wide, side-by-side comparison (terms only) ----------------------\ntop10_wide_terms &lt;- top10_all_models %&gt;%\n  arrange(Model, rank) %&gt;%\n  select(Model, rank, term) %&gt;%\n  tidyr::pivot_wider(names_from = Model, values_from = term)\n\ndata.frame(top10_wide_terms)\n\n   rank Elastic.Net     LASSO       OLS     Ridge\n1     1     gene_19   gene_19   gene_14   gene_14\n2     2     gene_08   gene_08   gene_08   gene_19\n3     3     gene_14   gene_14   gene_19   gene_08\n4     4     gene_05   gene_05   gene_05   gene_05\n5     5     gene_01   gene_01   gene_01   gene_01\n6     6     gene_12   gene_12   gene_12   gene_12\n7     7   gene_1976 gene_1778 gene_1024 gene_1419\n8     8   gene_1419 gene_1241   gene_27 gene_1506\n9     9   gene_1241 gene_1428 gene_1419 gene_1976\n10   10   gene_1428 gene_1976 gene_1927  gene_907\n\n# Optional: barplot for a quick visual per model ------------------------------\n# (Make sure ggplot2 is loaded.)\n# ggplot(top10_all_models, aes(x = reorder(term, importance), y = importance)) +\n#   geom_col() +\n#   coord_flip() +\n#   facet_wrap(~ Model, scales = \"free_y\") +\n#   labs(x = \"Gene\", y = \"Importance (|t| for OLS; |β| for penalized models)\",\n#        title = \"Top 10 genes by model\") +\n#   theme_minimal(base_size = 12)\n\nThe last table compares the top ten genes most strongly associated with tumor response according to four regression models OLS, LASSO, Ridge, and Elastic Net based on the magnitude of each gene’s estimated effect. While the specific coefficient values are not shown here, the ranking reveals how different modeling strategies emphasize or penalize predictors in slightly distinct ways. A clear pattern emerges across all methods: several genes consistently appear among the most influential. gene_19, gene_08, gene_14, gene_05, gene_01, and gene_12 are common to nearly every model, suggesting that these markers carry robust predictive information about treatment response. Their presence across OLS (which estimates effects freely), LASSO and Elastic Net (which perform variable selection), and Ridge (which shrinks but retains all coefficients) indicates that these signals are not artifacts of a particular modeling assumption but stable features of the dataset. The agreement among Elastic Net and LASSO is especially notable. Both methods rely on sparsity-inducing penalties, and they rank genes gene_19, gene_08, gene_14, gene_05, gene_01, and gene_12 in nearly identical order. This consistency supports the interpretation that these genes represent the core set most predictive of tumor shrinkage. The few differences such as the inclusion of gene_1976 or gene_1241 in the Elastic Net list reflect the method’s ability to keep correlated predictors that LASSO might exclude. The Ridge model, which shrinks coefficients without setting any to zero, yields a slightly more diverse list. It retains the same leading genes (gene_14, gene_19, gene_08, gene_05, gene_01, gene_12) but also highlights additional candidates such as gene_1419, gene_1506, gene_1976, and gene_907. This broader selection reflects Ridge’s tendency to distribute importance among correlated features, capturing groups of genes that may be co-expressed or functionally linked. Finally, OLS, which lacks regularization, identifies a very similar top tier dominated by gene_14, gene_08, and gene_19. Its overlap with the penalized models suggests that these genes have both strong individual associations and stability under penalization key indicators of biological and statistical relevance. In summary, despite methodological differences, all models converge on a common biological signature: a small set of genes (notably gene_19, gene_08, gene_14, gene_05, gene_01, and gene_12) emerge as consistent predictors of tumor response. The regularized approaches (especially Elastic Net) reinforce their robustness while reducing noise from less informative or redundant variables. Together, these findings highlight a reproducible molecular profile potentially linked to therapeutic sensitivity.\n\n\n2.13.3 Predictive modeling:\nGiven a patient’s main-effect clinical and molecular profile, how well do OLS, Ridge, LASSO, and Elastic Net predict tumor shrinkage (MAE/RMSE on train/test).\nThe answert to this question relates directly with the comparison we did regarding OLS, Rige, LASSO and Elastic net MAE and RMSEs.\n\n# Print results\nmetrics_train\n\n# A tibble: 4 × 5\n  Model           MAE  RMSE Alpha Lambda\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 OLS            1.27  1.59 NA    NA    \n2 LASSO (λ.min)  1.49  1.87  1    NA    \n3 Ridge (λ.min)  1.43  1.79  0    NA    \n4 Elastic Net    1.50  1.88  0.15  0.181\n\nmetrics_test\n\n# A tibble: 4 × 5\n  Model           MAE  RMSE Alpha Lambda\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 OLS            1.83  2.28 NA    NA    \n2 LASSO (λ.min)  1.57  1.96  1    NA    \n3 Ridge (λ.min)  1.77  2.23  0    NA    \n4 Elastic Net    1.57  1.95  0.15  0.181",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#logistic-regression",
    "href": "supervised_regression.html#logistic-regression",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.14 Logistic Regression",
    "text": "2.14 Logistic Regression\nLogistic regression models the probability that a patient is a high responder using a logistic ( S -shaped) link. Instead of predicting a continuous shrinkage value, we now predict \\(P(Y=1 \\mid X)\\), where \\(Y=1\\) means a clinically meaningful tumor reduction. The model is linear on the log-odds scale:\n\\[\n\\operatorname{Pr}(Y=1 \\mid X)=\\frac{1}{1+\\exp \\left\\{-\\left(w_0+w_1 x_1+\\cdots+w_p x_p\\right)\\right\\}}\n\\]\nCoefficients remain interpretable: a positive \\(w_j\\) increases the log-odds, which increases the probability of high response, holding other variables fixed. We will fit a clean, leakage-free specification that excludes identifiers and any variables used to compute the continuous outcome.\nAn alternative representation for a logistic regression model is the following diagram that has a structure of a network. We will come back this in later chapters. Can you guess which kind of networks is this?\n\n\n\n\n\nflowchart LR\n    x1[\"x₁\"] --&gt; w1[\"× w₁\"]\n    x2[\"x₂\"] --&gt; w2[\"× w₂\"]\n    xp[\"xₚ\"] --&gt; wp[\"× wₚ\"]\n\n    w1 --&gt; SUM\n    w2 --&gt; SUM\n    wp --&gt; SUM\n\n    SUM[\"Σ (wⱼ xⱼ + b)\"] --&gt; ACT[\"σ ( · )  (sigmoid)\"]\n    ACT --&gt; y[\"ŷ\"]\n\n\n\n\n\n\n\nMetrics we will use to compare models We will evaluate models with threshold-based metrics and threshold-free curves.\n\nAccuracy: fraction of correct classifications.\nSensitivity (recall): fraction of true positives captured.\nSpecificity: fraction of true negatives correctly rejected.\nPrecision (PPV): among predicted positives, fraction that are truly positive.\nF1: harmonic mean of precision and recall.\nROC and AUC: trade-off between TPR and FPR across all thresholds; AUC summarizes discrimination from 0.5 (random) a 1.0 (perfeito).\nPrecision-Recall and AUC-PR: useful when we have unbalanced data, focus on performance in the rare set.\n\n\n2.14.1 Understanding ROC and AUC through examples\nThe ROC curve (Receiver Operating Characteristic) and the AUC (Area Under the Curve) are fundamental tools for evaluating binary classification models. The series of plots generated by the next blocks of code illustrates how ROC curves behave under different modeling situations, helping us understand what a “good,” “bad,” or “misleading” model looks like. The examples are from simulated data and not related to our motivational context.\nEach plot shows sensitivity (true positive rate) on the vertical axis and 1 – specificity (false positive rate) on the horizontal axis. A diagonal line represents the performance of a random classifier. Curves that rise sharply toward the upper-left corner correspond to models that discriminate well between positive and negative cases. The AUC value, printed in the title of each facet, summarizes this ability numerically.\nIn the first scenario, labeled Excellent model, the two classes are highly separable. T\nThe name “logistic” comes from the logit, or log-odds, transformation that the model uses. Instead of modeling the probability itself, it models the log of the odds of success (for example, the log of the odds of being a high responder). This logit is linear in the predictors, which means we can still use familiar regression ideas while keeping the predictions bounded between 0 and 1\n\n\n2.14.2 Logistic Regression in our motivating example\nOur response variable is now high_response (binary, 0/1): equal to 1 if response_percent ≥ 30(similar to RECIST clinical criteria) Eisenhauer et al. (2009).\nThe next chunk of code makes sure that the response is binary, and prepare the datasets for analysis and the formula we will use in the R function `glm` that will fit the logistic regression for us.\nIn logistic regression, the model does not minimize squared errors like ordinary least squares does. Instead, it estimates the coefficients \\(\\boldsymbol{\\beta}\\) that make the observed data most probable under the assumed binomial distribution, a distribution that models binary outcomes. This is achieved through Maximum Likelihood Estimation (MLE). In essence, MLE finds the set of parameters \\(\\beta\\) that maximize the log-likelihood function, which measures how well the model’s predicted probabilities align with the actual outcomes. For each observation, the model computes the probability of belonging to class 1 (success) as \\(\\hat{p}_i=\\frac{1}{1+e^{-\\left(x_i^T w\\right)}}\\); the log-likelihood then accumulates the logarithm of these probabilities across all samples. Maximizing this quantity ensures that the estimated coefficients produce predicted probabilities that are as consistent as possible with the observed binary responses.\n\n\n2.14.3 ROC and AUC\nBefore moving to the code that implements these methods, it is useful to understand two fundamental concepts for evaluating the performance of binary classification models: the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC).\nWhen a model predicts probabilities such as the likelihood that a patient will show a high tumor response it is not limited to a single classification threshold (for example, 0.5). Instead, the threshold can vary. For each possible threshold, the model produces a different balance between sensitivity (the proportion of true responders correctly identified) and specificity (the proportion of non-responders correctly rejected).\nThe ROC curve visualizes this trade-off. It plots sensitivity on the vertical axis against 1 – specificity on the horizontal axis, across all thresholds from 0 to 1. Each point on the curve represents a possible decision rule. A model that predicts perfectly has a curve that rises immediately to the top-left corner of the plot (sensitivity = 1, specificity = 1). A model that performs no better than random chance follows the diagonal line from (0, 0) to (1, 1).\nThe Area Under the ROC Curve (AUC) condenses this information into a single number between 0 and 1. An AUC of 1 indicates perfect discrimination: the model always ranks true responders above non-responders. An AUC of 0.5 corresponds to random guessing. Values between 0.7 and 0.8 are generally considered acceptable, 0.8 to 0.9 good, and above 0.9 excellent, though interpretation depends on the context and the consequences of errors.\nIn the context of our clinical trial, the ROC curve tells us how well the logistic regression model can distinguish patients who achieve a meaningful tumor reduction from those who do not. The AUC gives an overall summary of this discrimination ability, independent of any specific threshold. It is particularly useful in medicine, where the decision threshold may later be adjusted to achieve a desired balance between missing true responders (false negatives) and incorrectly labeling non-responders as high responders (false positives).\nhe ROC curve quickly approaches the top-left corner, and the AUC is close to 1. This means the model ranks nearly all true responders above non-responders, providing excellent discrimination.\nIn the second panel, Reasonable model, the curve still bows above the diagonal, but less dramatically. The AUC is around 0.8–0.9, which is common in real clinical prediction problems. This represents a solid model that balances sensitivity and specificity reasonably well.\nThe third example, Near random, has an ROC curve close to the diagonal with an AUC around 0.5. This model is essentially guessing; its predictions carry no discriminative information beyond random chance.\nThe fourth panel, Inverted score, shows a curve that falls below the diagonal. Here the model systematically reverses the ordering of cases high probabilities are assigned to negatives and low probabilities to positives. In practice, such a model can be “fixed” simply by reversing its decision rule, but its presence is a clear sign that something in the data or labeling is inverted.\nThe fifth case, Imbalanced classes, demonstrates a subtle but important limitation of the ROC curve. Even with a heavily skewed dataset (for example, many more non-responders than responders), the ROC may still appear fairly high. However, when the number of positive cases is small, precision the proportion of predicted positives that are correct can drop sharply even if the ROC looks good. For this reason, the code also produces Precision–Recall (PR) curves, which tend to give a clearer picture when class imbalance is severe. In these PR plots, precision is shown against recall, and a high-quality model maintains both values simultaneously at high levels.\nThe final figure illustrates the effect of changing the decision threshold. Each dot along the ROC curve corresponds to a different cutoff value for the predicted probability. Lowering the threshold increases sensitivity (the model captures more true responders) but also raises the false positive rate. Raising the threshold has the opposite effect: fewer false alarms, but more missed responders. Choosing the “best” threshold depends on clinical priorities whether we prefer to err on the side of overtreatment (high sensitivity) or undertreatment (high specificity).\nTaken together, these plots demonstrate that ROC curves and AUC values offer a concise but nuanced summary of model discrimination. A high AUC indicates that the model generally ranks positive cases above negative ones, but the exact trade-off between sensitivity and specificity still depends on how we set the classification threshold. Complementing ROC and AUC with PR curves and context-specific thresholds ensures a more complete understanding of predictive performance, especially in medical settings where the balance between false positives and false negatives carries real clinical implications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.14.4 Penalized logistic models: LASSO, Ridge, and Elastic Net\nHigh dimensionality and correlated features make it hard to interpret thousands of coefficients and can hurt generalization. Penalized logistic regression addresses this by shrinking coefficients. Ridge (L2) shrinks all coefficients toward zero without removing variables. LASSO (L1) can set some coefficients exactly to zero, performing embedded feature selection. Elastic Net blends both penalties and is helpful when many predictors are correlated. We fit all three using the binomial family so the comparison with logistic regression is fair. We always evaluate probabilities from these models in ROC and PR calculations.\n\n\n2.14.5 Fitting Logistic Regression and equivalent LASSO, RIDGE,and Elastic Net Approaches\nTo make the results reproducible by users with any kind of machine we will first create a smaller version of our dataset containing only 100 genes.\nThe next chunk reads the smaller version of the data. In this dataset we have a subset of the 1000 genes, this is the reason for genes names not being 1….2000.\n\n# === Load the reduced dataset and quick checks ================================\nsuppressPackageStartupMessages({\n  library(dplyr)\n})\n\nct_reduced &lt;- readRDS(\"~/att_ai_ml/data/ct_reduced_v1.rds\")\n\n# Basic sanity checks\ndim(ct_reduced)\n\n[1] 10000   106\n\nnames(ct_reduced)[1:min(20, ncol(ct_reduced))]\n\n [1] \"treatment\"         \"dose_intensity\"    \"patient_age\"      \n [4] \"tumor_grade\"       \"performance_score\" \"gene_1242\"        \n [7] \"gene_397\"          \"gene_1754\"         \"gene_355\"         \n[10] \"gene_308\"          \"gene_234\"          \"gene_1598\"        \n[13] \"gene_1551\"         \"gene_588\"          \"gene_779\"         \n[16] \"gene_599\"          \"gene_2000\"         \"gene_1190\"        \n[19] \"gene_1568\"         \"gene_1321\"        \n\nstr(ct_reduced)\n\n'data.frame':   10000 obs. of  106 variables:\n $ treatment        : Factor w/ 2 levels \"no_chemo\",\"chemo\": 2 1 1 1 2 2 1 2 1 2 ...\n $ dose_intensity   : num  1.08 0 0 0 1.01 ...\n $ patient_age      : num  81 61 81 74 41 74 22 61 26 22 ...\n $ tumor_grade      : Factor w/ 3 levels \"G1\",\"G2\",\"G3\": 2 2 3 2 2 2 1 2 3 2 ...\n $ performance_score: int  1 1 1 0 1 1 2 0 1 0 ...\n $ gene_1242        : num  9.15 6.34 7.55 4.64 6.12 ...\n $ gene_397         : num  6.34 8.44 7.13 9.8 9.67 ...\n $ gene_1754        : num  5.34 8.27 6.9 5.74 5.49 ...\n $ gene_355         : num  8.06 9.06 8.91 6.84 7.21 ...\n $ gene_308         : num  10.39 8.81 9.73 7.17 7.9 ...\n $ gene_234         : num  8.38 8.83 8.4 10.43 10.81 ...\n $ gene_1598        : num  7.56 11 9.45 9.79 9.41 ...\n $ gene_1551        : num  5.96 9.13 7.99 7.87 7.28 ...\n $ gene_588         : num  8.79 4.28 6.6 5.04 5.94 ...\n $ gene_779         : num  8.29 4.05 5.63 4.39 5.12 ...\n $ gene_599         : num  9.35 5.89 7.31 5.74 6.55 ...\n $ gene_2000        : num  9.63 7.43 8.14 6.3 6.44 ...\n $ gene_1190        : num  7.8 6.28 7.72 5.2 5.3 ...\n $ gene_1568        : num  8.62 7.09 8.41 5.41 5.97 ...\n $ gene_1321        : num  6.94 4.74 5.6 6.14 6.53 ...\n $ gene_1379        : num  7.63 4.72 5.91 4.12 4.64 ...\n $ gene_746         : num  6.62 9.59 8.43 9.21 9.36 ...\n $ gene_1480        : num  12.8 11.9 12.5 13.6 13.9 ...\n $ gene_1973        : num  7.73 9.63 9.2 11.02 10.55 ...\n $ gene_1448        : num  8.36 9.11 9.25 10.44 10.87 ...\n $ gene_1733        : num  8.62 9.31 9.51 7.86 8.09 ...\n $ gene_41          : num  9.21 7.15 8.6 5.2 5.9 ...\n $ gene_1668        : num  7.02 7.55 7.11 8.68 8.46 ...\n $ gene_1400        : num  7.15 9.14 9.23 7.51 7.77 ...\n $ gene_1716        : num  5.66 8.06 7.21 7 6.89 ...\n $ gene_1213        : num  8.31 9.42 9 6.85 7.21 ...\n $ gene_873         : num  8.9 9.47 8.53 10.9 10.53 ...\n $ gene_207         : num  8.2 4.94 6.85 4.77 5.29 ...\n $ gene_609         : num  9.53 5.43 7.58 7.2 7.43 ...\n $ gene_1923        : num  6.72 9.03 8.05 9.67 9.14 ...\n $ gene_772         : num  8.3 5.56 6.6 4.61 5.24 ...\n $ gene_1039        : num  6.4 2.99 3.99 4.44 4.66 ...\n $ gene_1291        : num  7.87 5.9 7.13 4.26 5.25 ...\n $ gene_58          : num  7.87 5.55 6.54 4.29 4.83 ...\n $ gene_1760        : num  8.53 11.48 10.64 11 10.96 ...\n $ gene_1232        : num  6.9 10.18 8.68 9.72 9.23 ...\n $ gene_1925        : num  6.74 9.46 8.76 9.63 9.39 ...\n $ gene_340         : num  8.04 5.27 6.25 4.5 5.5 ...\n $ gene_663         : num  6.73 5.7 5.92 7.5 6.88 ...\n $ gene_681         : num  9.33 11.85 10.72 11.88 11.28 ...\n $ gene_1596        : num  8.57 6.87 7.99 8.61 9.36 ...\n $ gene_821         : num  9.9 9 9.22 7.91 7.64 ...\n $ gene_94          : num  7.07 10.25 9.02 9.21 8.83 ...\n $ gene_814         : num  9.67 9.43 9.13 11.23 11.75 ...\n $ gene_719         : num  7.67 9.99 9.59 10.78 10.31 ...\n $ gene_122         : num  10.63 7.86 9.68 9.93 10.12 ...\n $ gene_1240        : num  8.11 9.81 9.22 7.75 7.62 ...\n $ gene_577         : num  7.99 7.24 7.91 5.92 5.57 ...\n $ gene_1233        : num  8.47 9.85 9.58 8.99 8.55 ...\n $ gene_1811        : num  5.15 9.11 6.91 7.83 7.81 ...\n $ gene_713         : num  9.17 9.19 9.81 8.37 7.77 ...\n $ gene_819         : num  6.83 6.82 7.52 5.99 5.88 ...\n $ gene_1196        : num  8.61 5.52 7.53 5.67 6.2 ...\n $ gene_56          : num  9.82 11.92 10.52 12.27 11.62 ...\n $ gene_1188        : num  7.96 7.07 7.63 5.88 5.97 ...\n $ gene_1319        : num  7.12 6.62 7.17 8.65 8.2 ...\n $ gene_1820        : num  9.55 9.21 9.55 7.33 7.9 ...\n $ gene_926         : num  6.4 8.88 7.85 8.8 8.17 ...\n $ gene_1698        : num  5.39 4.88 5.97 3.48 4.02 ...\n $ gene_1690        : num  8.9 7.24 8.09 6.53 7.3 ...\n $ gene_1758        : num  7.05 6.13 6.55 4.88 5.44 ...\n $ gene_1909        : num  8.39 11.61 10.65 11.21 10.83 ...\n $ gene_1003        : num  6.47 9.16 7.79 8.45 8.03 ...\n $ gene_890         : num  7.18 9.41 9.01 10.49 9.55 ...\n $ gene_186         : num  7.28 7.69 7.89 6.38 6.82 ...\n $ gene_1912        : num  7.82 5.25 7.02 4.84 5.09 ...\n $ gene_787         : num  8.28 4.86 6.78 5.39 6.16 ...\n $ gene_967         : num  5.7 6.21 6.14 4.4 4.78 ...\n $ gene_233         : num  7.88 5.22 6.81 7.11 7.25 ...\n $ gene_564         : num  8.28 10.65 9.85 9.52 9.29 ...\n $ gene_307         : num  9.96 10.27 10.56 7.9 8.56 ...\n $ gene_1099        : num  8.56 10.69 9.48 10.21 9.53 ...\n $ gene_1460        : num  6.46 3.58 4.58 3.36 4.57 ...\n $ gene_1830        : num  5.11 7 6.18 7.59 7.26 ...\n $ gene_289         : num  7.53 4.57 5.83 6.41 6.27 ...\n $ gene_1740        : num  10.31 8.9 10.31 7.95 8.67 ...\n $ gene_1116        : num  11.36 9.97 10.66 12.17 11.82 ...\n $ gene_1946        : num  10.12 9.64 9.98 7.82 8.42 ...\n $ gene_371         : num  10.82 8.71 9.29 10.47 10.59 ...\n $ gene_1033        : num  12.3 13.8 13.2 14.8 14.2 ...\n $ gene_579         : num  8.39 9.28 9.36 8.57 9.26 ...\n $ gene_333         : num  8.79 8.73 8.16 9.2 8.9 ...\n $ gene_1222        : num  6.78 7.3 7.32 6.71 6.57 ...\n $ gene_1139        : num  10.48 9.69 10.02 10.82 10.64 ...\n $ gene_1302        : num  8.87 6.51 7.46 7.85 8.06 ...\n $ gene_1215        : num  7.42 8.53 8.2 7.68 7.62 ...\n $ gene_655         : num  7.44 5.14 5.82 5.39 5.6 ...\n $ gene_50          : num  7.93 7.18 7.6 7.37 7.6 ...\n $ gene_1195        : num  7.31 8.83 8.35 9.25 8.63 ...\n $ gene_1380        : num  6.96 5.45 6.14 5.2 5.33 ...\n $ gene_24          : num  10.34 9.81 9.98 9.37 9.3 ...\n $ gene_1940        : num  7.64 6.54 7.27 6.62 6.88 ...\n $ gene_370         : num  4.34 5 4.65 5.19 5.09 ...\n $ gene_1687        : num  8.6 9.92 9.11 10.74 10.32 ...\n  [list output truncated]\n - attr(*, \"created_with\")= chr \"variance+random gene selection (80+20), target_total_genes=100\"\n - attr(*, \"created_at\")= POSIXct[1:1], format: \"2025-11-11 11:42:34\"\n\n\nFirst we force the data to be binary\n\n# === Ensure binary target and remove any accidental leakage ===================\n# ct_reduced SHOULD already include 'high_response' (0/1). We enforce 0/1 safely.\n\nto_binary01 &lt;- function(x) {\n  if (is.logical(x)) return(as.integer(x))\n  if (is.factor(x))  return(as.integer(as.numeric(x) == max(as.numeric(x))))\n  as.integer(x)\n}\n\nif (!\"high_response\" %in% names(ct_reduced)) {\n  stop(\"`high_response` is not in ct_reduced. Recreate ct_reduced with the target.\")\n}\n\nct_reduced &lt;- ct_reduced %&gt;%\n  mutate(high_response = to_binary01(high_response))\n\n# If any of these exist, drop them for modeling as potential leakage/IDs.\nleak_or_id &lt;- intersect(\n  c(\"patient_id\", \"response_percent\", \"baseline_tumor_mm\", \"post_tumor_mm\"),\n  names(ct_reduced)\n)\nif (length(leak_or_id)) {\n  message(\"Dropping potential leakage/ID columns: \", paste(leak_or_id, collapse = \", \"))\n  ct_reduced &lt;- dplyr::select(ct_reduced, -all_of(leak_or_id))\n}\n\n# Optional: remove zero-variance predictors (defensive)\nnzv &lt;- function(x) is.numeric(x) && (sd(x, na.rm = TRUE) == 0)\ndrop_nzv &lt;- names(ct_reduced)[vapply(ct_reduced, nzv, logical(1))]\nif (length(drop_nzv)) {\n  message(\"Dropping zero-variance numeric columns: \", paste(drop_nzv, collapse = \", \"))\n  ct_reduced &lt;- dplyr::select(ct_reduced, -all_of(drop_nzv))\n}\n\nAfter reading and preparing the data we proceed with the division of the dataset into training and testing dataset.\n\n\n\n# === Stratified train/test split (e.g., 70/30) ================================\nset.seed(42)\nprop_train &lt;- 0.70\n\nidx_train &lt;- ct_reduced %&gt;%\n  mutate(row_id = dplyr::row_number()) %&gt;%\n  group_by(high_response) %&gt;%\n  slice_sample(prop = prop_train) %&gt;%\n  ungroup() %&gt;%\n  pull(row_id)\n\ntrain_df &lt;- ct_reduced[idx_train, , drop = FALSE]\ntest_df  &lt;- ct_reduced[-idx_train, , drop = FALSE]\n\n# Quick balance check\ntable_train &lt;- table(train_df$high_response)\ntable_test  &lt;- table(test_df$high_response)\ntable_train; table_test\n\n\n   0    1 \n4385 2614 \n\n\n\n   0    1 \n1880 1121 \n\n\nNow we will build the model formula to be used by R\n\n# === Build the modeling formula (use everything except the target) ============\npredictors &lt;- setdiff(names(train_df), \"high_response\")\nrhs &lt;- paste(predictors, collapse = \" + \")\nf_logit &lt;- as.formula(paste(\"high_response ~\", rhs))\nf_logit\n\nhigh_response ~ treatment + dose_intensity + patient_age + tumor_grade + \n    performance_score + gene_1242 + gene_397 + gene_1754 + gene_355 + \n    gene_308 + gene_234 + gene_1598 + gene_1551 + gene_588 + \n    gene_779 + gene_599 + gene_2000 + gene_1190 + gene_1568 + \n    gene_1321 + gene_1379 + gene_746 + gene_1480 + gene_1973 + \n    gene_1448 + gene_1733 + gene_41 + gene_1668 + gene_1400 + \n    gene_1716 + gene_1213 + gene_873 + gene_207 + gene_609 + \n    gene_1923 + gene_772 + gene_1039 + gene_1291 + gene_58 + \n    gene_1760 + gene_1232 + gene_1925 + gene_340 + gene_663 + \n    gene_681 + gene_1596 + gene_821 + gene_94 + gene_814 + gene_719 + \n    gene_122 + gene_1240 + gene_577 + gene_1233 + gene_1811 + \n    gene_713 + gene_819 + gene_1196 + gene_56 + gene_1188 + gene_1319 + \n    gene_1820 + gene_926 + gene_1698 + gene_1690 + gene_1758 + \n    gene_1909 + gene_1003 + gene_890 + gene_186 + gene_1912 + \n    gene_787 + gene_967 + gene_233 + gene_564 + gene_307 + gene_1099 + \n    gene_1460 + gene_1830 + gene_289 + gene_1740 + gene_1116 + \n    gene_1946 + gene_371 + gene_1033 + gene_579 + gene_333 + \n    gene_1222 + gene_1139 + gene_1302 + gene_1215 + gene_655 + \n    gene_50 + gene_1195 + gene_1380 + gene_24 + gene_1940 + gene_370 + \n    gene_1687 + gene_170 + gene_643 + gene_1617 + gene_426 + \n    gene_934 + gene_1373\n\n\n\n\n2.14.6 Fitting logistic regression\nIn order to fit the logistic regression we use the R function `glm` that is the short for generalized linear models, a generic type of models from which logistic regression is a specific example.\n\n\n\n# === Fit logistic regression (glm) and get predictions =======================\nsuppressPackageStartupMessages({\n  library(broom)   # tidy output\n})\n\nmod_logit &lt;- glm(f_logit,\n                 data   = train_df,\n                 family = binomial(),\n                 control = glm.control(maxit = 100))  # a few more iterations can help\n\n# Predicted probabilities\np_train &lt;- predict(mod_logit, newdata = train_df, type = \"response\")\np_test  &lt;- predict(mod_logit, newdata = test_df,  type = \"response\")\n\n# Class predictions at a default 0.5 threshold (tune later as needed)\nthr &lt;- 0.5\ny_train &lt;- train_df$high_response\ny_test  &lt;- test_df$high_response\nyhat_train &lt;- as.integer(p_train &gt;= thr)\nyhat_test  &lt;- as.integer(p_test  &gt;= thr)\n\nNow we create some helper functions to get metrics from the model\n\n# === Metrics helpers (confusion-matrix stats, ROC AUC, PR AUC) ===============\nsuppressPackageStartupMessages({\n  library(pROC)       # ROC/AUC\n  suppressWarnings(require(PRROC))  # PR curves (optional)\n  library(tidyr)\n  library(ggplot2)\n})\n\ncm_metrics &lt;- function(y, yhat) {\n  tp &lt;- sum(y == 1 & yhat == 1)\n  tn &lt;- sum(y == 0 & yhat == 0)\n  fp &lt;- sum(y == 0 & yhat == 1)\n  fn &lt;- sum(y == 1 & yhat == 0)\n  acc  &lt;- (tp + tn) / (tp + tn + fp + fn)\n  sens &lt;- ifelse((tp + fn) &gt; 0, tp / (tp + fn), NA_real_)  # recall\n  spec &lt;- ifelse((tn + fp) &gt; 0, tn / (tn + fp), NA_real_)\n  prec &lt;- ifelse((tp + fp) &gt; 0, tp / (tp + fp), NA_real_)  # PPV\n  f1   &lt;- ifelse((prec + sens) &gt; 0, 2 * prec * sens / (prec + sens), NA_real_)\n  dplyr::tibble(TP = tp, TN = tn, FP = fp, FN = fn,\n                Accuracy = acc, Sensitivity = sens, Specificity = spec,\n                Precision = prec, F1 = f1)\n}\n\nauc_roc &lt;- function(labels, scores) {\n  as.numeric(pROC::auc(pROC::roc(labels, scores, quiet = TRUE)))\n}\n\nauc_pr &lt;- function(labels, scores, positive_class = 1) {\n  if (!\"PRROC\" %in% .packages()) return(NA_real_)\n  s &lt;- as.numeric(scores); y &lt;- as.integer(labels)\n  fg &lt;- s[y == positive_class]\n  bg &lt;- s[y != positive_class]\n  out &lt;- PRROC::pr.curve(scores.class0 = fg, scores.class1 = bg, curve = FALSE)\n  as.numeric(out$auc.integral)\n}\n\nLets calculate metrics of true positives true negatives false positives and false negatives for our example.\n\n# === Metrics table for glm on TRAIN/TEST ======================================\nlogit_train_metrics &lt;- cm_metrics(y_train, yhat_train) %&gt;%\n  mutate(Model = \"Logistic (glm)\", Dataset = \"Train\",\n         AUC_ROC = auc_roc(y_train, p_train),\n         AUC_PR  = auc_pr(y_train,  p_train))\n\nlogit_test_metrics &lt;- cm_metrics(y_test, yhat_test) %&gt;%\n  mutate(Model = \"Logistic (glm)\", Dataset = \"Test\",\n         AUC_ROC = auc_roc(y_test, p_test),\n         AUC_PR  = auc_pr(y_test,  p_test))\n\ndplyr::bind_rows(logit_train_metrics, logit_test_metrics)\n\n# A tibble: 2 × 13\n     TP    TN    FP    FN Accuracy Sensitivity Specificity Precision    F1 Model\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1  2406  4017   368   208    0.918       0.920       0.916     0.867 0.893 Logi…\n2  1005  1717   163   116    0.907       0.897       0.913     0.860 0.878 Logi…\n# ℹ 3 more variables: Dataset &lt;chr&gt;, AUC_ROC &lt;dbl&gt;, AUC_PR &lt;dbl&gt;\n\n\nMoving we can construct the ROC and AUC curves for our example\n\n\n\n# === ROC curves for glm (downsampled for plotting stability) ==================\nroc_train &lt;- pROC::roc(y_train, p_train, quiet = TRUE)\nroc_test  &lt;- pROC::roc(y_test,  p_test,  quiet = TRUE)\n\nroc_df &lt;- dplyr::bind_rows(\n  tibble::tibble(FPR = 1 - roc_train$specificities, TPR = roc_train$sensitivities, Dataset = \"Train\"),\n  tibble::tibble(FPR = 1 - roc_test$specificities,  TPR = roc_test$sensitivities,  Dataset = \"Test\")\n) %&gt;%\n  dplyr::group_by(Dataset) %&gt;%\n  dplyr::slice( unique(round(seq(1, dplyr::n(), length.out = pmin(400L, dplyr::n())))) ) %&gt;%\n  dplyr::ungroup()\n\nggplot(roc_df, aes(x = FPR, y = TPR, linetype = Dataset)) +\n  geom_abline(slope = 1, intercept = 0, alpha = 0.4) +\n  geom_line(linewidth = 1) +\n  coord_equal() +\n  labs(\n    title = \"ROC curves    Logistic regression\",\n    subtitle = sprintf(\"AUC Train = %.3f, AUC Test = %.3f\",\n                       as.numeric(pROC::auc(roc_train)), as.numeric(pROC::auc(roc_test))),\n    x = \"False Positive Rate\", y = \"True Positive Rate\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOptionally we can create a PR curve\n\n\n\n# === Optional PR curves for glm ===============================================\nif (\"PRROC\" %in% .packages()) {\n  fg_train &lt;- p_train[y_train == 1]; bg_train &lt;- p_train[y_train == 0]\n  fg_test  &lt;- p_test[y_test  == 1];  bg_test  &lt;- p_test[y_test  == 0]\n  pr_train &lt;- PRROC::pr.curve(scores.class0 = fg_train, scores.class1 = bg_train, curve = TRUE)\n  pr_test  &lt;- PRROC::pr.curve(scores.class0 = fg_test,  scores.class1 = bg_test,  curve = TRUE)\n\n  pr_df &lt;- dplyr::bind_rows(\n    tibble::tibble(recall = pr_train$curve[,1], precision = pr_train$curve[,2],\n                   Dataset = paste0(\"Train (AUC-PR = \", sprintf(\"%.3f\", pr_train$auc.integral), \")\")),\n    tibble::tibble(recall = pr_test$curve[,1],  precision = pr_test$curve[,2],\n                   Dataset = paste0(\"Test (AUC-PR = \",  sprintf(\"%.3f\", pr_test$auc.integral), \")\"))\n  )\n\n  ggplot(pr_df, aes(x = recall, y = precision, color = Dataset)) +\n    geom_line(linewidth = 1) +\n    labs(title = \"Precision–Recall curves    Logistic regression\",\n         x = \"Recall (Sensitivity)\", y = \"Precision (PPV)\") +\n    theme_minimal()\n}\n\n\n\n\n\n\n\n\n\nand get the coefficients from the models\n\n# === Coefficient table with Wald-style CIs (robust to confint() failures) =====\ncoef_tbl_logit &lt;- broom::tidy(mod_logit) %&gt;%\n  mutate(\n    conf.low  = estimate - qnorm(0.975) * std.error,\n    conf.high = estimate + qnorm(0.975) * std.error\n  ) %&gt;%\n  arrange(estimate)\n\nprint(coef_tbl_logit, n = min(25, nrow(coef_tbl_logit)))\n\n# A tibble: 107 × 7\n   term              estimate std.error statistic  p.value conf.low conf.high\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 performance_score   -0.676    0.0767    -8.82  1.13e-18   -0.826  -0.526  \n 2 gene_1379           -0.338    0.179     -1.89  5.89e- 2   -0.688   0.0126 \n 3 gene_1733           -0.335    0.170     -1.97  4.87e- 2   -0.668  -0.00196\n 4 gene_1598           -0.305    0.174     -1.75  7.93e- 2   -0.646   0.0357 \n 5 gene_426            -0.283    0.174     -1.63  1.04e- 1   -0.625   0.0579 \n 6 gene_1820           -0.280    0.175     -1.60  1.09e- 1   -0.622   0.0627 \n 7 gene_1760           -0.257    0.180     -1.43  1.54e- 1   -0.610   0.0964 \n 8 gene_719            -0.255    0.177     -1.45  1.48e- 1   -0.601   0.0906 \n 9 gene_170            -0.246    0.174     -1.42  1.57e- 1   -0.587   0.0945 \n10 gene_655            -0.237    0.177     -1.34  1.81e- 1   -0.584   0.110  \n11 gene_934            -0.230    0.174     -1.32  1.87e- 1   -0.572   0.112  \n12 gene_1909           -0.225    0.173     -1.30  1.94e- 1   -0.565   0.115  \n13 gene_819            -0.225    0.175     -1.29  1.99e- 1   -0.568   0.118  \n14 gene_1687           -0.208    0.177     -1.18  2.39e- 1   -0.555   0.138  \n15 gene_1925           -0.206    0.174     -1.19  2.34e- 1   -0.547   0.134  \n16 gene_1291           -0.199    0.174     -1.15  2.52e- 1   -0.540   0.142  \n17 gene_289            -0.198    0.176     -1.13  2.60e- 1   -0.543   0.147  \n18 gene_821            -0.183    0.175     -1.05  2.96e- 1   -0.526   0.160  \n19 gene_1099           -0.182    0.173     -1.05  2.93e- 1   -0.522   0.157  \n20 gene_577            -0.178    0.177     -1.00  3.16e- 1   -0.525   0.170  \n21 gene_1940           -0.176    0.178     -0.988 3.23e- 1   -0.524   0.173  \n22 gene_50             -0.172    0.178     -0.969 3.33e- 1   -0.521   0.176  \n23 gene_1740           -0.157    0.174     -0.900 3.68e- 1   -0.497   0.184  \n24 gene_41             -0.150    0.174     -0.862 3.89e- 1   -0.492   0.191  \n25 gene_1946           -0.148    0.176     -0.842 4.00e- 1   -0.493   0.197  \n# ℹ 82 more rows\n\n\n\n\n2.14.7 Understanding Log-Odds and Coefficient Interpretation in Logistic Regression\nIn linear regression, we predict a continuous outcome. In logistic regression, the outcome is binary (e.g., high_response = 1 for “high responder” vs 0 for “low responder”). To keep predicted probabilities between 0 and 1, we model the logarithm of the odds (the logit) of being a responder.\n\n2.14.7.1 Odds and Logit\nThe odds of success are: $[ = ] $If (p=0.75), then (=0.75/0.25=3) (three-to-one).\nThe logit (log-odds) is: \\(\\[ \\text{logit}(p) = \\log!\\left(\\frac{p}{1-p}\\right) \\]\\)\nLogistic regression is linear in the log-odds: \\(\\[ \\log!\\left(\\frac{p}{1-p}\\right)=w_0+w_1 x_1+\\cdots+w_k x_k \\]\\)\nEach coefficient ( $w_j $) is the change in log-odds for a one-unit increase in (x_j), holding other variables fixed.\n\n\n2.14.7.2 Odds Ratios\nExponentiating a coefficient yields an odds ratio (OR): \\(\\[ \\text{OR}\\_j = e\\^{w_j} \\] - (\\text{OR}\\_j \\&gt; 1)\\): increasing (x_j) raises the odds of response\n- (\\(\\text{OR}\\_j &lt; 1\\)): increasing (x_j) lowers the odds of response\n- (\\(\\text{OR}\\_j = 1\\)): no change in odds\nExample interpretations: - \\(( \\w=+0.80 \\Rightarrow \\text{OR}=e\\^{0.80}\\approx 2.22)\\): odds a bit more than double per unit increase.\n- \\(( w=-0.50 \\Rightarrow \\text{OR}\\approx 0.61)\\): odds drop by \\(\\~39%\\) per unit increase.\n\n\n2.14.7.3 From Log-Odds Back to Probability\n\\(\\[ p=\\frac{1}{1+e^{-(w_0+w_1 x_1+\\cdots+w_k x_k)}} \\]\\) If log-odds = (1.5), then (p). If log-odds = (-1.5), then (p).\n\n\n\n2.14.8 Compute Odds Ratios from the Fitted Model\n\n# Build odds–ratio table from the fitted model\n# (uses normal-approx CI; avoids profile-likelihood warnings/slowdowns)\nlibrary(dplyr)\nlibrary(broom)\n\nzcrit &lt;- qnorm(0.975)  # 1.96\n\nor_tbl &lt;- broom::tidy(mod_logit) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(\n    OR      = exp(estimate),\n    OR_low  = exp(estimate - zcrit * std.error),\n    OR_high = exp(estimate + zcrit * std.error)\n  ) %&gt;%\n  arrange(desc(abs(estimate)))\n\n# --- Ways to *see* the OR columns clearly ---\n\n# A) Select and print only the columns you care about\nor_tbl %&gt;%\n  select(term, estimate, std.error, statistic, p.value, OR, OR_low, OR_high) %&gt;%\n  slice_head(n = 12)\n\n# A tibble: 12 × 8\n   term            estimate std.error statistic  p.value      OR  OR_low OR_high\n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 treatmentchemo     6.78     0.658      10.3  6.35e-25 882.    243.    3.20e+3\n 2 dose_intensity     3.74     0.595       6.29 3.14e-10  42.2    13.1   1.35e+2\n 3 tumor_gradeG3      1.24     0.143       8.68 4.08e-18   3.47    2.62  4.59e+0\n 4 performance_sc…   -0.676    0.0767     -8.82 1.13e-18   0.509   0.438 5.91e-1\n 5 gene_1319          0.450    0.175       2.57 1.03e- 2   1.57    1.11  2.21e+0\n 6 gene_1039          0.435    0.178       2.44 1.45e- 2   1.55    1.09  2.19e+0\n 7 tumor_gradeG2      0.402    0.124       3.25 1.14e- 3   1.50    1.17  1.91e+0\n 8 gene_1302          0.355    0.177       2.01 4.48e- 2   1.43    1.01  2.02e+0\n 9 gene_1379         -0.338    0.179      -1.89 5.89e- 2   0.714   0.503 1.01e+0\n10 gene_1733         -0.335    0.170      -1.97 4.87e- 2   0.715   0.513 9.98e-1\n11 gene_1598         -0.305    0.174      -1.75 7.93e- 2   0.737   0.524 1.04e+0\n12 gene_58            0.292    0.177       1.65 9.93e- 2   1.34    0.946 1.90e+0\n\n# B) Print all columns without truncation\nprint(or_tbl, n = 12, width = Inf)\n\n# A tibble: 106 × 8\n   term              estimate std.error statistic  p.value      OR  OR_low\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 treatmentchemo       6.78     0.658      10.3  6.35e-25 882.    243.   \n 2 dose_intensity       3.74     0.595       6.29 3.14e-10  42.2    13.1  \n 3 tumor_gradeG3        1.24     0.143       8.68 4.08e-18   3.47    2.62 \n 4 performance_score   -0.676    0.0767     -8.82 1.13e-18   0.509   0.438\n 5 gene_1319            0.450    0.175       2.57 1.03e- 2   1.57    1.11 \n 6 gene_1039            0.435    0.178       2.44 1.45e- 2   1.55    1.09 \n 7 tumor_gradeG2        0.402    0.124       3.25 1.14e- 3   1.50    1.17 \n 8 gene_1302            0.355    0.177       2.01 4.48e- 2   1.43    1.01 \n 9 gene_1379           -0.338    0.179      -1.89 5.89e- 2   0.714   0.503\n10 gene_1733           -0.335    0.170      -1.97 4.87e- 2   0.715   0.513\n11 gene_1598           -0.305    0.174      -1.75 7.93e- 2   0.737   0.524\n12 gene_58              0.292    0.177       1.65 9.93e- 2   1.34    0.946\n    OR_high\n      &lt;dbl&gt;\n 1 3202.   \n 2  135.   \n 3    4.59 \n 4    0.591\n 5    2.21 \n 6    2.19 \n 7    1.91 \n 8    2.02 \n 9    1.01 \n10    0.998\n11    1.04 \n12    1.90 \n# ℹ 94 more rows\n\n# C) Nicely formatted table (if in a report)\n# knitr::kable(\n#   or_tbl %&gt;%\n#     transmute(\n#       term,\n#       `Estimate (β)` = estimate,\n#       `Std. Error`    = std.error,\n#       `z`             = statistic,\n#       `p`             = p.value,\n#       `OR = exp(β)`   = OR,\n#       `OR low`        = OR_low,\n#       `OR high`       = OR_high\n#     ) %&gt;%\n#     slice_head(n = 12),\n#   digits = 3, align = \"lrrrrrrr\",\n#   caption = \"Top coefficients by |β| with odds ratios and 95% CI\"\n# )\n\n# D) If you want rounded values for readability\nor_tbl_rounded &lt;- or_tbl %&gt;%\n  mutate(\n    across(c(estimate, std.error, statistic), ~round(.x, 3)),\n    across(c(p.value), ~signif(.x, 3)),\n    across(c(OR, OR_low, OR_high), ~round(.x, 2))\n  )\n\nor_tbl_rounded %&gt;%\n  select(term, estimate, std.error, statistic, p.value, OR, OR_low, OR_high) %&gt;%\n  slice_head(n = 12)\n\n# A tibble: 12 × 8\n   term              estimate std.error statistic  p.value     OR OR_low OR_high\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 treatmentchemo       6.78      0.658     10.3  6.35e-25 882.   243.   3202.  \n 2 dose_intensity       3.74      0.595      6.29 3.14e-10  42.2   13.2   135.  \n 3 tumor_gradeG3        1.24      0.143      8.68 4.08e-18   3.47   2.62    4.59\n 4 performance_score   -0.676     0.077     -8.82 1.13e-18   0.51   0.44    0.59\n 5 gene_1319            0.45      0.175      2.57 1.03e- 2   1.57   1.11    2.21\n 6 gene_1039            0.435     0.178      2.44 1.45e- 2   1.55   1.09    2.19\n 7 tumor_gradeG2        0.402     0.124      3.25 1.14e- 3   1.5    1.17    1.91\n 8 gene_1302            0.355     0.177      2.01 4.48e- 2   1.43   1.01    2.02\n 9 gene_1379           -0.338     0.179     -1.89 5.89e- 2   0.71   0.5     1.01\n10 gene_1733           -0.335     0.17      -1.97 4.87e- 2   0.72   0.51    1   \n11 gene_1598           -0.305     0.174     -1.76 7.93e- 2   0.74   0.52    1.04\n12 gene_58              0.292     0.177      1.65 9.93e- 2   1.34   0.95    1.9 \n\n\nThe treatment variable (chemo vs. no_chemo) shows by far the strongest association, with an estimated coefficient of 6.78, corresponding to an odds ratio (OR) of approximately 885. This means that, holding other predictors constant, patients who received chemotherapy had odds of achieving a high tumor response nearly 900 times greater than those who did not receive it.\nThe dose_intensity variable also exerts a very large positive effect (w = 3.74; OR ≈ 42). Each one-unit increase in dose intensity multiplies the odds of response by roughly 42, indicating a steep dose–response relationship.\nTumor-related characteristics also influence response probability. A higher tumor_grade (e.g., G2 or G3 relative to the baseline category) is associated with an estimated b of 1.24 (OR ≈ 3.46), suggesting about a three-and-a-half-fold increase in the odds of response. A smaller positive contrast (w = 0.40; OR ≈ 1.49) indicates that other grade categories also contribute modestly to improved response rates.\nIn contrast, performance_score has a negative coefficient (w = −0.676; OR ≈ 0.51), meaning that each additional point on this score reflecting poorer clinical performance reduces the odds of a favorable response by about half.\nSeveral genes show smaller but biologically interesting effects. gene_1598 (w = +0.45; OR ≈ 1.57) and gene_1551 (w = +0.44; OR ≈ 1.55) both display modest positive associations, where higher expression slightly increases the probability of response. gene_779 (w = +0.36; OR ≈ 1.43) and gene_588 (w = +0.29; OR ≈ 1.34) show similarly mild but consistent trends toward higher response odds.\nConversely, gene_565 (w = −0.34; OR ≈ 0.71), gene_323 (w = −0.34; OR ≈ 0.72), and gene_1183 (w = −0.31; OR ≈ 0.74) are negatively associated with high response, suggesting that higher expression of these genes slightly decreases the odds of tumor reduction, possibly reflecting resistance pathways.\n\n\n2.14.9 Benchmarking Logistic regression with LASSO, RIDGE and ELASTIC NET counterparts\n\n# === Penalized logistic: LASSO, Ridge, Elastic Net ============================\nsuppressPackageStartupMessages({\n  library(glmnet)\n})\n\n# Freeze factor levels/dummies using the TRAIN design implied by f_logit\nmf_train  &lt;- model.frame(f_logit, data = train_df)\nterms_log &lt;- terms(mf_train)\n\nX_train &lt;- model.matrix(terms_log, data = train_df)[, -1, drop = FALSE]\nX_test  &lt;- model.matrix(terms_log, data = test_df)[,  -1, drop = FALSE]\n\ny_train &lt;- train_df$high_response\ny_test  &lt;- test_df$high_response\n\nset.seed(42)\n\n# LASSO (alpha = 1)\ncv_lasso  &lt;- cv.glmnet(X_train, y_train, family = \"binomial\", alpha = 1, nfolds = 10)\nmod_lasso &lt;- glmnet(X_train, y_train, family = \"binomial\", alpha = 1, lambda = cv_lasso$lambda.min)\np_lasso_train &lt;- as.numeric(predict(mod_lasso, newx = X_train, type = \"response\"))\np_lasso_test  &lt;- as.numeric(predict(mod_lasso, newx = X_test,  type = \"response\"))\n\n# Ridge (alpha = 0)\ncv_ridge  &lt;- cv.glmnet(X_train, y_train, family = \"binomial\", alpha = 0, nfolds = 10)\nmod_ridge &lt;- glmnet(X_train, y_train, family = \"binomial\", alpha = 0, lambda = cv_ridge$lambda.min)\np_ridge_train &lt;- as.numeric(predict(mod_ridge, newx = X_train, type = \"response\"))\np_ridge_test  &lt;- as.numeric(predict(mod_ridge, newx = X_test,  type = \"response\"))\n\n# Elastic Net (alpha = 0.15 as an example)\nalpha_en   &lt;- 0.15\ncv_enet    &lt;- cv.glmnet(X_train, y_train, family = \"binomial\", alpha = alpha_en, nfolds = 10)\nmod_enet   &lt;- glmnet(X_train, y_train, family = \"binomial\", alpha = alpha_en, lambda = cv_enet$lambda.min)\np_enet_train &lt;- as.numeric(predict(mod_enet, newx = X_train, type = \"response\"))\np_enet_test  &lt;- as.numeric(predict(mod_enet, newx = X_test,  type = \"response\"))\n\nLet’s now compare the models\n\n\n\n# === Compare models on TEST ====================================================\ncompare_test &lt;- tibble::tibble(\n  Model = c(\"Logistic (glm)\", \"LASSO-logit\", \"Ridge-logit\", \"ElasticNet-logit\"),\n  Prob  = list(p_test,        p_lasso_test,  p_ridge_test,  p_enet_test)\n) %&gt;%\n  dplyr::rowwise() %&gt;%\n  dplyr::mutate(\n    yhat    = list(as.integer(unlist(Prob) &gt;= 0.5)),\n    Metrics = list(cm_metrics(y_test, unlist(yhat))),\n    AUC_ROC = auc_roc(y_test, unlist(Prob)),\n    AUC_PR  = auc_pr(y_test,  unlist(Prob))\n  ) %&gt;%\n  tidyr::unnest(Metrics) \n\ncompare_test\n\n# A tibble: 4 × 14\n  Model     Prob  yhat     TP    TN    FP    FN Accuracy Sensitivity Specificity\n  &lt;chr&gt;     &lt;lis&gt; &lt;lis&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 Logistic… &lt;dbl&gt; &lt;int&gt;  1005  1717   163   116    0.907       0.897       0.913\n2 LASSO-lo… &lt;dbl&gt; &lt;int&gt;  1019  1716   164   102    0.911       0.909       0.913\n3 Ridge-lo… &lt;dbl&gt; &lt;int&gt;  1034  1697   183    87    0.910       0.922       0.903\n4 ElasticN… &lt;dbl&gt; &lt;int&gt;  1019  1712   168   102    0.910       0.909       0.911\n# ℹ 4 more variables: Precision &lt;dbl&gt;, F1 &lt;dbl&gt;, AUC_ROC &lt;dbl&gt;, AUC_PR &lt;dbl&gt;\n\n\nand plot metrics of performance\n\n\n\n# === Faceted ROC across models (TEST) =========================================\nroc_facets &lt;- function(models, labels,\n                       suptitle = \"ROC curves by model (TEST)\",\n                       xlaw = \"False positive rate (1 - specificity)\",\n                       ylaw = \"True positive rate (sensitivity)\",\n                       caption = NULL) {\n  df_list &lt;- vector(\"list\", length(models))\n  model_names &lt;- names(models)\n  if (is.null(model_names)) model_names &lt;- paste0(\"Model \", seq_along(models))\n  for (i in seq_along(models)) {\n    roc_i &lt;- pROC::roc(labels, models[[i]], quiet = TRUE)\n    auc_i &lt;- as.numeric(pROC::auc(roc_i))\n    pts   &lt;- tibble::tibble(\n      fpr   = 1 - roc_i$specificities,\n      tpr   = roc_i$sensitivities,\n      model = sprintf(\"%s (AUC = %.3f)\", model_names[i], auc_i)\n    )\n    df_list[[i]] &lt;- pts\n  }\n  roc_df &lt;- dplyr::bind_rows(df_list)\n\n  ggplot(roc_df, aes(x = fpr, y = tpr)) +\n    geom_path(linewidth = 1) +\n    geom_abline(slope = 1, intercept = 0, linetype = 2) +\n    facet_wrap(~ model) +\n    labs(title = suptitle, x = xlab, y = ylab, caption = caption) +\n    theme_minimal(base_size = 12)\n}\n\nmodels_scores &lt;- list(\n  \"Logistic (glm)\" = p_test,\n  \"LASSO-logit\"    = p_lasso_test,\n  \"Ridge-logit\"    = p_ridge_test,\n  \"ENet-logit\"     = p_enet_test\n)\n\nroc_facets(models_scores, labels = y_test,\n           suptitle = \"ROC by model on TEST (ct_reduced)\")\n\n\n\n\n\n\n\n\n\n# === Faceted PR across models (TEST, optional) ================================\nif (\"PRROC\" %in% .packages()) {\n  pr_facets &lt;- function(models, labels,\n                        positive_class = 1,\n                        suptitle = \"Precision–Recall curves by model (TEST)\",\n                        caption = NULL) {\n    df_list &lt;- vector(\"list\", length(models))\n    model_names &lt;- names(models)\n    if (is.null(model_names)) model_names &lt;- paste0(\"Model \", seq_along(models))\n    for (i in seq_along(models)) {\n      s &lt;- as.numeric(models[[i]])\n      y &lt;- as.integer(labels)\n      s_pos &lt;- s[y == positive_class]\n      s_neg &lt;- s[y != positive_class]\n      pr &lt;- PRROC::pr.curve(scores.class0 = s_pos, scores.class1 = s_neg, curve = TRUE)\n      tmp &lt;- tibble::tibble(\n        recall    = pr$curve[, 1],\n        precision = pr$curve[, 2],\n        model     = sprintf(\"%s (AUC-PR = %.3f)\", model_names[i], pr$auc.integral)\n      )\n      df_list[[i]] &lt;- tmp\n    }\n    pr_df &lt;- dplyr::bind_rows(df_list)\n\n    ggplot(pr_df, aes(recall, precision)) +\n      geom_path(linewidth = 1) +\n      facet_wrap(~ model) +\n      labs(title = suptitle, x = \"Recall (sensitivity)\", y = \"Precision (PPV)\", caption = caption) +\n      theme_minimal(base_size = 12)\n  }\n\n  pr_facets(models_scores, labels = y_test,\n            suptitle = \"Precision–Recall by model on TEST (ct_reduced)\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\nEisenhauer, Elizabeth A., Patrick Therasse, Jan Bogaerts, Lawrence H. Schwartz, Daniel Sargent, Rebecca Ford, Janet Dancey, et al. 2009. “New Response Evaluation Criteria in Solid Tumours: Revised RECIST Guideline (Version 1.1).” European Journal of Cancer 45 (2): 228–47. https://doi.org/10.1016/j.ejca.2008.10.026.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "three_methods.html",
    "href": "three_methods.html",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "",
    "text": "3.1 Setting up R\n# Core data/plot\nreq_pkgs &lt;- c(\n  \"dplyr\", \"ggplot2\", \"tidyr\", \"readr\", \"tibble\", \"gridExtra\",\n  # Trees\n  \"rpart\", \"rpart.plot\", \"partykit\",\n  # Random Forests (pick one or use both)\n  \"ranger\",        # fast RF (recommended)\n  \"randomForest\",  # classic RF implementation\n  # Gradient Boosting\n  \"xgboost\", \"Matrix\",  # Matrix for sparse design matrices\n  # Model interpretation (optional but handy)\n  \"vip\",   # variable importance plots\n  \"pdp\",   # partial dependence\n  \"iml\"    # ICE/SHAP-like tools (optional)\n)\n\n# Install any missing\nto_install &lt;- setdiff(req_pkgs, rownames(installed.packages()))\nif (length(to_install) &gt; 0) {\n  install.packages(to_install, dependencies = TRUE)\n}\n\n# Load all (silently)\ninvisible(lapply(req_pkgs, require, character.only = TRUE))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#recalling",
    "href": "three_methods.html#recalling",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.2 Recalling",
    "text": "3.2 Recalling\nIn the last chapter we studied models that allow us to perform supervised learning regression tasks. We used linear models and different approaches for estimation including OLS, LASSO, Ridge and Elastic Net to predict labels of interest. We did that using an example of a clinical trial in which patients were randomized to receive or not chemotherapy. In the experiment we also collected other features and the expression of 2000 gens. We then fitted and compared OLS, LASSO, Ridge and Elastic Net models and compared regarding model outputs and prediction capacity. In this chapter we will still use the chemotherapy trial example to learn the concept of trees, random forests and XGboost techniques.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#reading-the-dataset",
    "href": "three_methods.html#reading-the-dataset",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.3 Reading the dataset",
    "text": "3.3 Reading the dataset\n\n# ================================\n# Read and prepare TRAIN/TEST (no saving)\n# ================================\n# 0) Load\ntrial_ct &lt;- readRDS(\"~/att_ai_ml/data/trial_ct_chemo_cont.rds\")\nstr(trial_ct[, 1:15])   # quick peek\n\n'data.frame':   10000 obs. of  15 variables:\n $ patient_id       : chr  \"P001\" \"P002\" \"P003\" \"P004\" ...\n $ treatment        : Factor w/ 2 levels \"no_chemo\",\"chemo\": 2 1 1 1 2 2 1 2 1 2 ...\n $ dose_intensity   : num  1.08 0 0 0 1.01 ...\n $ patient_age      : num  81 61 81 74 41 74 22 61 26 22 ...\n $ tumor_grade      : Factor w/ 3 levels \"G1\",\"G2\",\"G3\": 2 2 3 2 2 2 1 2 3 2 ...\n $ performance_score: int  1 1 1 0 1 1 2 0 1 0 ...\n $ baseline_tumor_mm: num  52.2 43.3 43.2 63.8 54.2 44.9 51.5 95.1 69.5 54.7 ...\n $ post_tumor_mm    : num  25.8 43.3 43.2 61.2 33.6 37 51.5 60.4 69.5 29.8 ...\n $ response_percent : num  50.6 0 0 4.2 38 17.5 0 36.5 0 45.6 ...\n $ high_response    : int  1 0 0 0 1 0 0 1 0 1 ...\n $ gene_01          : num  11.01 9.26 10.09 8.99 9.01 ...\n $ gene_02          : num  9.31 7.79 8.95 7.8 8.33 ...\n $ gene_03          : num  9.51 8.02 9.45 7.78 8.23 ...\n $ gene_04          : num  10.27 9.16 9.42 8.87 9.03 ...\n $ gene_05          : num  7.31 8.14 7.83 7.49 7.23 ...\n\n\nLet’s divide the dataset into training and testing like we did in the chapter about regression tasks.\n\n# 2) Stratified 70/30 split by treatment\nset.seed(42)\nsplit_strat &lt;- function(df, strat_col, p_train = 0.7) {\n  idx_tr &lt;- unlist(tapply(seq_len(nrow(df)), df[[strat_col]], function(ix) {\n    sample(ix, size = floor(p_train * length(ix)))\n  }))\n  list(train = sort(idx_tr), test = setdiff(seq_len(nrow(df)), idx_tr))\n}\n\nsp &lt;- split_strat(trial_ct, strat_col = \"treatment\", p_train = 0.7)\ntrain &lt;- trial_ct[sp$train, , drop = FALSE]\ntest  &lt;- trial_ct[sp$test,  , drop = FALSE]\n\nThe next chunk will make sure we will use only the columns that make sense.\n\n# 3) Drop columns that should NOT enter models\n#    - patient_id: identifier only\n#    - high_response: binary version of the outcome (leakage if modeling response_percent)\n#    - baseline_tumor_mm, post_tumor_mm: strongly deterministically related to response_percent\ndrop_cols &lt;- intersect(\n  names(train),\n  c(\"patient_id\", \"high_response\", \"baseline_tumor_mm\", \"post_tumor_mm\")\n)\ntrain_nopii &lt;- dplyr::select(train, -dplyr::all_of(drop_cols))\ntest_nopii  &lt;- dplyr::select(test,  -dplyr::all_of(drop_cols))\n\n# 4) Check outcome presence\n#stopifnot(\"response_percent\" %in% names(train_nopii))\n\nNow that we have the training and testing datasets organized we will create the matrices required for running models later. We will keep the same strategy we used for the previous chapters so we can compare the results of today with the ones of that class.\n\n# 5) Build a consistent design (for glmnet / xgboost, etc.)\n#    Use TRAIN to “freeze” factor levels and dummy columns\nf_ols   &lt;- response_percent ~ .\nols_tmp &lt;- lm(f_ols, data = train_nopii)\nols_terms &lt;- terms(ols_tmp)\n\n# Model matrices (no intercept column)\nX_train &lt;- model.matrix(ols_terms, data = train_nopii)[, -1, drop = FALSE]\nX_test  &lt;- model.matrix(ols_terms, data = test_nopii)[,  -1, drop = FALSE]\ny_train &lt;- train_nopii$response_percent\ny_test  &lt;- test_nopii$response_percent",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#loading-some-helper-functions",
    "href": "three_methods.html#loading-some-helper-functions",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.4 Loading some helper functions",
    "text": "3.4 Loading some helper functions\nAs in the other chapter\n\n# 6) Small helpers for later evaluations\nmae  &lt;- function(y, yhat) mean(abs(y - yhat))\nrmse &lt;- function(y, yhat) sqrt(mean((y - yhat)^2))\neval_perf &lt;- function(y_true, y_pred) {\n  tibble::tibble(MAE = mae(y_true, y_pred), RMSE = rmse(y_true, y_pred))\n}\n\nBefore reading the rest of the chapter make sure you have the following objects loaded in your R environment\n\n# ---- Objects now available (in-memory) ----\n# train, test                      # full splits (for inspection/plots)\n# train_nopii, test_nopii          # safe for tree models (RF) with data.frame API\n# X_train, X_test, y_train, y_test # matrices for glmnet / xgboost\n# eval_perf(), mae(), rmse()       # metric helpers",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#model",
    "href": "three_methods.html#model",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.5 Model",
    "text": "3.5 Model\nI the previous classes we learn that predictive AI, also named, machine learning can help us to predict labels using explanatory features, in simple mathematical terms we will have\n\\[\nY=f(X)+\\epsilon\n\\]\n\\[Y=f(x1,x2,...,xn)+error\\]\nIn the previous chapter we explored models in which f is said to have a linear behaviour\n\\[Y= \\mu + b1 * x1 +.... bn *xn+ \\epsilon \\tag{3.1}\\]\nwhen using the explanatory features to predict the responses.\nIn this chapter we will learn models that learn the function \\(f(X)\\) via (decision trees and their ensembles), named Random Forests and XGboost. The trees will be performing regression tasks or classification tasks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#trees",
    "href": "three_methods.html#trees",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.6 Trees",
    "text": "3.6 Trees\nBefore applying tree-based models to real clinical data, it is useful to build intuition with a simple synthetic example. Decision trees are, at their core, collections of if-else rules that partition the feature space into smaller, homogeneous regions. Each region (or leaf) represents a group of observations that share similar predicted values. To understand how such rules emerge from data, we will simulate a small dataset where the true underlying relationship between predictors and the outcome is explicitly governed by ifelse logic.\nIn this example, we create two explanatory variables- \\(X_1\\) and \\(X_2\\)-and one response variable \\(Y\\). The response depends on threshold-based rules involving these features, plus a small amount of random noise:\n\\[\nY= \\begin{cases}5+\\varepsilon, & \\text { if } X_1 \\leq 4 \\\\ 10+\\varepsilon, & \\text { if } X_1&gt;4 \\text { and } X_2 \\leq 0 \\\\ 14+\\varepsilon, & \\text { if } X_1&gt;4 \\text { and } X_2&gt;0\\end{cases}\n\\]\nwhere \\(\\varepsilon \\sim \\mathcal{N}\\left(0,0.8^2\\right)\\) adds a small random deviation around each mean value. This structure defines three regions in the predictor space-each corresponding to one “rule” that determines the value of \\(Y\\).\nThe following R code generates this dataset:\n\n# ===== 1) TOY EXAMPLE: a small, interpretable tree =====\n# Two features with threshold structure, plus noise\nn &lt;- 200\nX1 &lt;- runif(n, 0, 10)            # e.g., \"age-like\"\nX2 &lt;- rnorm(n, 0, 1)             # e.g., \"biomarker-like\"\n# Piecewise rule used to generate y (ground truth if/else)\n# IF X1 &lt;= 4      THEN y ~  5 + noise\n# ELSE IF X2 &lt;= 0 THEN y ~ 10 + noise\n# ELSE                 y ~ 14 + noise\ny  &lt;- ifelse(X1 &lt;= 4, 5, ifelse(X2 &lt;= 0, 10, 14)) + rnorm(n, 0, 0.8)\ntoy &lt;- tibble::tibble(X1 = X1, X2 = X2, y = y)\nhead(toy)\n\n# A tibble: 6 × 3\n     X1      X2     y\n  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 7.56   0.570  13.1 \n2 1.08   1.01    5.79\n3 9.04  -0.346   9.59\n4 5.55  -0.249  10.1 \n5 9.33   0.213  14.8 \n6 0.759 -0.0242  5.53\n\n\nThe resulting dataset toy contains 200 simulated observations, each described by two predictors ( X1 , X2 ) and a numeric outcome ( y ). The piecewise constant nature of the data mimics a situation where a target variable depends on threshold effects-for instance, a biomarker that changes behavior only above a certain age or concentration level.\nThis simple simulation is pedagogically powerful: when we fit a decision tree to these data, the model will recover rules very similar to the ones used to generate \\(Y\\). Each split in the tree corresponds to a decision node, where the algorithm asks a question of the form “Is \\(X_j \\leq s\\) ?”. Depending on the answer, the observation moves to a child node on either the left or right branch. The process continues until no further improvement in prediction can be achieved, producing terminal nodes (leaves) that store the average predicted value of \\(Y\\) for that region.\nBy visualizing and interpreting this tree, we can clearly see how decision trees learn and represent piecewise-constant approximations of complex, nonlinear relationships using only simple, interpretable rules.\nIn this chapter we will learn how to represent datasets like this in the form of the next figure\nOne possibility could be approximating the y values using the if else rules expressed in the tree\n\n\ny is  5.0 when\n          X1 &lt; 4\n\ny is  9.8 when\n          X1 &gt;= 4\n          X2 &lt; -0.031\n\ny is 14.0 when\n          X1 &gt;= 4\n          X2 &gt;= -0.031\n\n\nWhich can be visualised as with the usage of the Figure\n\n\n\n\n\n\n\n\n\nThis means the fitted model partitions the predictor space into three rectangular regions, each defined by simple threshold conditions on \\(X_1\\) and \\(X_2\\) : 1. Region 1 (Left branch):\nWhen \\(X_1&lt;4\\), the model predicts \\(\\hat{y}=5.0\\). This corresponds to the “younger” or “low-X1” group in our simulation, and reproduces the first rule of the data-generating process. 2. Region 2 (Middle branch):\nWhen \\(X_1 \\geq 4\\) and \\(X_2&lt;-0.03\\), the model predicts \\(\\hat{y}=9.8\\). This reflects the second rule-if \\(X_1\\) is large but the second biomarker is low, \\(y\\) is around 10 . 3. Region 3 (Right branch):\nWhen \\(X_1 \\geq 4\\) and \\(X_2 \\geq-0.03\\), the model predicts \\(\\hat{y}=14.0\\). This captures the third rule-both features are high, so the predicted outcome rises further. Each “when” statement defines a path from the root to a leaf, and each leaf holds the average of the training responses that fall into that region. In this simple example, the regression tree recovered the same three rules used to generate the data, showing how trees naturally express models as collections of logical conditions (if/else) rather than algebraic equations.\n\n3.6.1 Non vegetable anatomy of a tree\nThe following annotated Figure 1.6 depicts the anatomy of a computer science (algorithm) trees\n\n\n\n\n\n\nFigure 3.1\n\n\n\nA decision tree is composed of a small number of fundamental building blocks that work together to partition the predictor space and generate predictions. The annotated figure highlights six key elements each serving a distinct role in the tree’s logic and interpretability.\nThe root node is the starting point of the tree and contains the entire dataset below it. It summarises the outcome distribution before any splitting occurs and represents the baseline prediction if no further structure were learned. All decision paths originate from this node.\nAn internal node is any node that performs a further split. It contains a rule such as X1 &lt; 4 or X2 ≥ –0.03, chosen to maximize homogeneity in the resulting subgroups. These nodes divide the data into more refined regions and define the hierarchical structure of the model.\nBetween every parent and child node lies a splitting rule, which acts as the conditional logic directing observations left or right. This is the “if–else” mechanism of the tree.\n\nBetween-node rules partition the feature space and determine how each observation flows through the model, creating a sequence of decisions that progressively increases predictive precision. Each node contains a compact summary of the data reaching that point, typically including:\n\nthe predicted value (in regression),\nthe number of observations in the node, and\nthe percentage of the sample represented. This within-node information describes the characteristics of the subgroup created by previous splits and forms the foundation for the node’s prediction.\n\nA leaf node is a node with no further splits.\n\nLeaf nodes provide the model’s final predictions. They correspond to the most homogeneous subgroups discovered during training, each representing a rule-defined region of the predictor space. In regression trees, the leaf value is the mean outcome of that subgroup. Every split produces two child nodes, each inheriting all conditions from its ancestors.\n\nThese nodes represent progressively more detailed subdivisions of the dataset.\n\nA child node can either become another internal node (if it contains meaningful further structure) or a leaf node (if splitting stops).\n\n\n3.6.2 Attributes of trees\nDecision trees are among the most intuitive and versatile models in machine learning. They combine the logic of if-else reasoning with the ability to approximate complex, nonlinear functions. The structure of a tree provides both a visual and conceptual bridge between human decision-making and predictive modeling.\nHierarchical and rule-based structure\nA decision tree represents a sequence of binary decisions. At each internal node, the algorithm tests a condition of the form\n\\[\nx_j \\leq s,\n\\]\nwhere \\(x_j\\) is one explanatory variable and \\(s\\) is a threshold chosen to maximize predictive homogeneity the resulting subgroups.\nEach path from the root to a leaf corresponds to a complete logical rule that defines a rectangular region of the predictor space. Leaves store a single value (for regression) or a class probability (for classification), so the tree acts as a collection of piecewise rules.\nOther important attributes include the following items:\nLocal modeling and nonlinearity:\nUnlike linear regression, which assumes a single global relationship between predictors and the response, trees build local models. Each branch captures relationships that may differ across subsets of the data. This allows trees to represent sharp thresholds, discontinuities, and strong interactions between variables without explicitly defining them in advance.\nAutomatic handling of interactions:\nBecause each new split is conditional on previous \\(\\bigcirc\\) s, trees naturally model interactions between predictors.\nFor example, a second-level split on \\(X_2\\) applies only to observations that already satisfy a condition on \\(X_1\\). This hierarchical conditioning is equivalent to including interaction terms in a regression model, but it emerges automatically from the recursive partitioning process.\nScale and data type robustness:\nDecision trees are invariant to feature scaling-splits depend only on ordering, not on variable magnitude. They also handle both numeric and categorical variables seamlessly. In many implementations, missing values can be directed through surrogate splits, allowing a model to make predictions even with incomplete data.\nInterpretability and transparency: Each decision path can be read as an explicit rule such as “if tumor size \\(&lt;3 \\mathrm{~cm}\\) and biomarker \\(\\geq 1.2\\) then predict high response.” This makes trees particularly appealing in health and life-science contexts, where interpretability is essential for clinical validation and regulatory transparency. A tree’s visual representation helps communicate how specific variables drive predictions in different patient subgroups.\nDespite their interpretability, single trees can be unstable: small perturbations in the data may change the chosen splits and produce very different trees. Their predictions are also piecewise constant, creating abrupt jumps between regions. To improve stability and predictive accuracy, modern practice often aggregates many trees into ensembles such as Random Forests or Gradient I \\(\\downarrow\\) sted Trees, which we will explore next.\n\n\n3.6.3 A very short introduction to interactions\nIn predictive modeling, an interaction occurs when the effect of one explanatory variable on the outcome depends on the value of another variable. Formally, two variables \\(X_1\\) and \\(X_2\\) interact if the change in the response \\(Y\\) associated with \\(X_1\\) varies according to the level of \\(X_2\\).\nIn linear models, this relationship must be specified explicitly by adding a product term (e.g., \\(\\beta_3 X_1 X_2\\) ). In decision trees, interactions emerge automatically: each split is conditional on previous decisions, so the model can represent different relationships between \\(X_1\\) and \\(Y\\) depending on the branch defined by \\(X_2\\).\nIn other words, trees learn interactions hierarchically rather than algebraically-each branch of the tree corresponds to a different interaction context.\nExamples of interactions include:\n\nClinical example - drug efficacy and age\n\nThe effectiveness of a chemotherapy drug ( \\(X_1\\) ) may depend on the patient’s age ( \\(X_2\\) ). The treatment might be highly effective in younger patients but less so in older ones due to metabolism or organ function.\nIn this case, the effect of the drug is conditional on age - an interaction between treatment and age. 2. Biomarker example - gene expression and tumor grade\nA specific gene expression score ( \\(X_1\\) ) could predict tumor response only for patients with highgrade tumors ( \\(X_2\\) ). For low-grade tumors, the same biomarker might have little to no effect. This represents a biological interaction: the prognostic value of the biomarker changes across tumor grades. 3. Behavioral or physiological example - dose and physical condition\nThe relationship between drug dose ( \\(X_1\\) ) and therapeutic response ( \\(Y\\) ) may differ between patients with good and poor performance status ( \\(X_2\\) ). The slope of the dose-response curve is steeper in one group and flatter in the other, illustrating an interaction between dose intensity and baseline health.\n\nGeneric data-science example - temperature and humidity\n\nIn environmental modeling, the effect of temperature ( \\(X_1\\) ) on energy consumption ( \\(Y\\) ) depends on humidity ( \\(X_2\\) ).\nHigh temperatures increase consumption only when humidity is also high, due to greater airconditioning load - another clear interaction.\n\nGenotype by environment interactions\n\nIn genetics usually its important to understand how the interaction between genomic and environmental information defines a given phenotype.\nWe learnt what trees are and their characteristics. We will now understand how to implement them with R and how they learn the if else rules.\nInteractions can be explored graphically with the usage of interaction plots.\n\nIn the case of no interactions the graphic will be like the one shown in Figure 3.2, in which the lines connecting the mean response by dose intensity level are parallel, meaning that the means response does not change when we change the dose intensity level.\n\nggplot(\n  agg_no_inter,\n  aes(dose_intensity, mean_resp, color = tumor_grade, group = tumor_grade)\n) +\n  geom_line(linewidth = 1.3) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Interaction Plot (NO Interaction Example)\",\n    subtitle = \"Parallel lines → effect of dose is the same across tumor grades\",\n    x = \"Dose intensity\",\n    y = \"Mean response (%)\",\n    color = \"Tumor grade\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\nFigure 3.2\n\n\n\n\n\nOn the other hand when we have the presence of an interaction, like in our chemotherapy example, we can produce the interaction plot using the following code. We can see that the lines in the interaction plot are not parallel indicating the presence of interaction, in other words, the level o mean tumor response changes according to the level of dose intensity. This is shown in FIgure Figure 3.3\n\n# Interaction-style plot\nggplot(agg, aes(dose_bin, mean_resp, color = tumor_grade, group = tumor_grade)) +\n  geom_line(linewidth = 1.3) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Interaction Plot (Observed Data Only)\",\n    subtitle = \"How tumor grade modifies the dose–response pattern\",\n    x = \"Dose intensity (binned)\",\n    y = \"Mean tumor response (%)\",\n    color = \"Tumor grade\"\n  ) +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\nFigure 3.3\n\n\n\n\n\n\n3.6.3.1 How a tree is built\nA regression tree approximates the unknown function \\(f(\\mathbf{X})\\) by a piecewise-constant model:\n\\[\n\\hat{f}(\\mathbf{X})=\\sum_{m=1}^M c_m \\mathbf{1}\\left\\{\\mathbf{X} \\in R_m\\right\\},\n\\]\nwhere each leaf (region \\(R_m\\) ) predicts a constant \\(c_m\\) (usually the mean \\(y\\) in that region).\nThis formulation shows that, although trees are non-linear in the inputs, they are linear in the indicator functions that define the regions. In other words, a tree can be viewed as a linear model on a transformed feature space-one where the original variables have been replaced by a collection of binary indicators representing the hierarchical if-else splits.\n\\[\n\\hat{f}(X)=c_1 \\mathbf{1}_{R_1}(X)+c_2 \\mathbf{1}_{R_2}(X)+\\cdots+c_M \\mathbf{1}_{R_M}(X) .\n\\]\nFrom this perspective, decision trees extend the concept of a linear model by allowing the basis functions ( \\(\\mathbf{1}_{R_m}\\) ) to be learned from data rather than predefined. Each new split creates a new “basis” that isolates a subset of the data with distinct local behavior.\nWe can compare linear and tree models regarding some geometrical and conceptual interpretation:\n\nIn a linear model, the function \\(f(X)\\) defines a single plane (or hyperplane) across the feature space. Predictions vary smoothly and continuously with \\(X\\).\nIn a decision tree, the feature space is divided into rectangular regions, within which predictions are constant. The function \\(f(X)\\) therefore takes a piecewise-constant form, producing a step-like approximation to the true relationship.\n\nVisually, a tree can be seen as a function that “jumps” at each decision boundary, instead of tilting like a plane. This enables trees to capture sharp thresholds, nonlinearities, and interactions that linear models cannot express without manual feature engineering.\nLearning = splitting to reduce impurity. At a node containing samples \\(S\\), the CART algorithm chooses a feature \\(j\\) and threshold \\(s\\) that minimize the total squared error after splitting:\n\\[\n\\left(j^*, s^*\\right)=\\arg \\min _{j, s}\\left[\\sum_{i \\in S_L}\\left(y_i-\\bar{y}_L\\right)^2+\\sum_{i \\in S_R}\\left(y_i-\\bar{y}_R\\right)^2\\right],\n\\]\nequivalently maximizing variance reduction:\n\\[\n\\Delta I=I(S)-\\frac{\\left|S_L\\right|}{|S|} I\\left(S_L\\right)-\\frac{\\left|S_R\\right|}{|S|} I\\left(S_R\\right), \\quad I(S)=\\frac{1}{|S|} \\sum_{i \\in S}\\left(y_i-\\bar{y}_S\\right)^2 .\n\\]\n\n\n3.6.3.2 Choosing the best split: impurity and information gain\nAt every node, the tree algorithm searches for the feature and cut-point that most reduce the node’s impurity that is, the heterogeneity of responses within the node.\nFor regression tasks such as predicting response_percent, impurity is measured by the within-node variance:\n\\[\nI(S) = \\frac{1}{|S|} \\sum_{i \\in S} (y_i - \\bar{y}_S)^2 .\n\\]\nThe chosen split \\((j^*, s^*)\\) maximizes the reduction in impurity:\n\\[\n\\Delta I = I(\\text{parent})\n- \\frac{n_L}{n_{\\text{parent}}} I(\\text{left})\n- \\frac{n_R}{n_{\\text{parent}}} I(\\text{right}).\n\\]\nIntuitively, the algorithm prefers splits that make the child nodes more homogeneous in response_percent.\nFor classification tasks, rpart() uses the Gini index or entropy instead of variance.\n\n\n\n3.6.4 Continuous vs. categorical predictors in rpart\nrpart handles numeric and factor variables differently when proposing binary splits at a node. Continuous predictors (e.g., dose_intensity , gene_14) - Procedure: sort unique values of \\(x_j\\); evaluate candidate thresholds at midpoints between adjacent values.\n\nFor each threshold \\(s\\), form left/right nodes \\(\\left(x_j \\leq s\\right)\\) vs. \\(\\left(x_j&gt;s\\right)\\) and compute the impurity reduction (variance for regression; Gini/entropy for classification).\nChoose the \\(s\\) that maximizes \\(\\Delta I\\).\nConsequences: no scaling needed (splits depend on order, not magnitude); trees naturally create step-functions and thresholds (e.g., a cut at gene_14 \\(\\geq 0.98\\) in our chemo example).\n\nCategorical predictors (e.g., treatment \\(\\in\\{\\) no_chemo , chemo \\(\\}\\), tumor_grade \\(\\in\\{\\mathrm{G} 1, \\mathrm{G} 2, \\mathrm{G} 3\\}\\) )\n\nBinary factors: trivial split (one level left, the other right).\nMulti-level factors: in principle there are \\(2^{m-1}-1\\) groupings of \\(m\\) levels; rpart avoids brute force by ordering levels by their node statistics and then testing only adjacent two-group splits along that order:\nClassification: order levels by class composition; test adjacent partitions; pick the one with largest Gini/entropy reduction.\nRegression: order levels by the mean response; test adjacent partitions; pick the best variance reduction.\nResult: efficient search that still finds strong groupings (e.g., tumor_grade \\(\\in\\{G 2, G 3\\}\\) vs. \\(\\{G 1\\}\\) if those two higher grades share similar response patterns).\n\nOrdered factors\n\nIf a variable is an ordered factor, rpart treats it like a numeric rank and proposes threshold splits along that order (behaves like a continuous variable).\n\nMissing values \\(\\&\\) surrogates\n\nIf the primary split variable is missing for a case (e.g., missing gene_14), rpart can route it using surrogate splits-backup variables that mimic the primary partition (controlled by usesurrogate, maxsurrogate).\n\nPractical notes for our dataset\n\nKeep true categorical variables as factors (e.g., treatment , tumor_grade ), and keep gene expression and doses numeric.\nHigh-cardinality categorical variables can make splits unstable; if you have such variables, consider sensible grouping beforehand.\nNo need for one-hot encoding or scaling; rpart handles both types natively.\n\n\n3.6.4.1 Stopping & pruning.\nTo avoid overfitting, trees stop growing (e.g., maxdepth , minsplit ) and/or are pruned via costcomplexity:\n\\[\n\\operatorname{Score}(T)=\\operatorname{RSS}(T)+\\alpha|T|, \\quad \\alpha \\geq 0,\n\\]\nselecting the smallest subtree within 1-SE of the minimum cross-validated error.\nThe model is built by recursively partitioning the feature space into smaller and more homogeneous regions.\nEach split introduces a new if-else rule, and the process continues until no further improvement is possible or a stopping rule is reached.\nIn some future sections we will see the concept of ensembles and Link trees to ensembles. When we combine many trees-as in Random Forests and Gradient Boosted Trees (XGBoost)-the model becomes a sum of multiple tree functions:\n\\[\n\\hat{f}(X)=\\sum_{b=1}^B \\hat{f}_b(X),\n\\]\nwhere each \\(\\hat{f}_b(X)\\) is a tree trained on a different subset or residual of the data. This ensemble structure restores smoothness and reduces variance while keeping the interpretability and flexibility of the tree-based representation.\n\n\n3.6.4.2 Bias–variance perspective\nA fully grown tree fits every training case perfectly but generalizes poorly its variance is high.\nPruning or using constraints (minsplit, cp, maxdepth) increases bias slightly but drastically reduces variance, improving predictive stability.\nIn the chemotherapy trial, an unpruned tree would memorize gene-expression idiosyncrasies of a few patients, giving near-zero error in training but poor test performance.\nThe pruned tree achieves a better bias–variance balance, capturing major response patterns while ignoring random noise.\n\n\n3.6.4.3 Stages to build a tree\nThe construction of a tree can be described in several stages: initialization, growing, splitting, stopping, pruning, and prediction.\n\nInitialization\n\n\nStart with the full training dataset at the root node.\n\nEvery observation belongs to this node, and the model computes a simple summary: - for regression: the mean response \\(\\bar{y}\\); - for classification: the most frequent class. - This initial value is the baseline prediction before any split occurs.\n\nSearching for the best split (growing phase)\n\nAt each step, the algorithm evaluates all possible binary splits of all features. For a numeric variable \\(x_j\\), it considers thresholds \\(s\\) such that the data are divided into\n\\[\n\\text { Left: } x_j \\leq s, \\quad \\text { Right: } x_j&gt;s .\n\\]\nFor categorical features, the split divides categories into two subsets.\nFor each candidate split, the algorithm calculates the impurity reduction, which measures how much the new partition improves the homogeneity of the outcome:\n\\[\n\\Delta I=I(\\text { parent })-\\frac{n_L}{n_{\\text {parent }}} I(\\text { left })-\\frac{n_R}{n_{\\text {parent }}} I(\\text { right }),\n\\]\nwhere \\(I(\\cdot)\\) is the impurity index: - variance for regression, - Gini or entropy for classification.\nThe split that yields the largest reduction in impurity is selected. 3. Creating new nodes (splitting)\nOnce the best feature and threshold are chosen: - The parent node is replaced by two child nodes (left and right). - Each child node now represents a subset of the data that satisfies one side of the if/else rule. - The prediction for each child node is recalculated as the mean (regression) or class proportion (classification).\nThis process is repeated recursively for each child node. At each step, the algorithm searches again for the best local split to further reduce impurity within that node.\n\nStopping criteria (when to stop growing)\n\nThe tree keeps expanding until one or more stopping conditions are met: - The node contains fewer than a minimum number of observations ( minsplit , minbucket ). - The reduction in impurity from a new split is below a threshold ( cp in rpart ). - The tree has reached a maximum depth ( maxdepth ). - All observations in the node have identical responses.\nWhen a node can no longer be split under these rules, it becomes a terminal node or leaf. 5. Cost-complexity pruning (simplifying the tree)\nFully grown trees tend to overfit: they capture noise as if it were signal. To restore generalization, CART uses cost-complexity pruning. This involves fitting a large tree first, then sequentially removing the least useful branches according to the criterion\n\\[\nR_\\alpha(T)=\\operatorname{RSS}(T)+\\alpha|T|,\n\\]\nwhere \\(|T|\\) is the number of terminal nodes and \\(\\alpha \\geq 0\\) penalizes complexity. Cross-validation identifies the optimal penalty \\(\\alpha^*\\). The 1-SE rule then selects the smallest subtree whose error is within one standard error of the minimum, balancing accuracy and simplicity. 6. Prediction\nOnce the tree is built (and possibly pruned), prediction for a new observation is simple: 1. Start at the root. 2. Follow the if/else path defined by its feature values (e.g., “if dose_intensity &lt; 0.9 go left, else go right”). 3. When a leaf is reached, return the value stored there: - for regression: the mean response in that region; - for classification: the majority class or probability.\nThis process is deterministic and interpretable: each prediction can be traced to a specific logical path. 7. Visual summary of the process\n\n\n\n\n\n\n\n\nStage\nDescription\nTypical R function element\n\n\n\n\nInitialization\nRoot node with all data\nautomatic in rpart()\n\n\nGrowing\nRepeatedly searches for the split minimizing node impurity\ncontrolled by minsplit, maxdepth, cp\n\n\nSplitting\nPartitions data and creates left/right child nodes\nrecursive calls within rpart()\n\n\nStopping\nHalts if no improvement or nodes are too small\nminbucket, cp threshold\n\n\nPruning\nRemoves overfitted branches using cost–complexity pruning\nprune() + cross-validation\n\n\nPrediction\nApplies learned rules to new data\npredict()\n\n\n\nNote: Single decision trees can easily overfit the training data; we’ll use pruning (and later ensembles) to control this.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#trees-for-regression-and-classification-cart",
    "href": "three_methods.html#trees-for-regression-and-classification-cart",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.7 Trees for regression and classification (CART)",
    "text": "3.7 Trees for regression and classification (CART)\nDecision trees can solve both regression and classification tasks within a single framework historically called CART (Classification and Regression Trees). CART is a trademark. The idea is to approximate the unknown function \\(f(\\mathbf{X})\\) by splitting the feature space into rectangular regions and predicting a constant within each region (a leaf).\nFor regression, as in our chemotherapy example where the response is continuous ( response_percent ), the tree chooses splits that reduce the within-node variance. If a node contains samples \\(S\\), its impurity is\n\\[\nI_{\\mathrm{reg}}(S)=\\frac{1}{|S|} \\sum_{i \\in S}\\left(y_i-\\bar{y}_S\\right)^2,\n\\]\nand the “best” split is the one that maximizes the reduction in impurity (variance) after partitioning \\(S\\) into left/right children.\nFor classification, e.g., if we instead predict the binary label high_response ( \\(\\geq 30 \\% \\mathrm{vs}&lt;30 \\%\\) ), CART typically measures impurity using the Gini index (or entropy). If a node has class proportions \\(\\left\\{p_k\\right\\}_{k=1}^K\\),\n\\[\nI_{\\mathrm{cls}}^{\\mathrm{Gini}}(S)=1-\\sum_{k=1}^K p_k^2, \\quad I_{\\mathrm{cls}}^{\\mathrm{Ent}}(S)=-\\sum_{k=1}^K p_k \\log _2 p_k .\n\\]\nThe algorithm selects the feature and threshold that yield the largest impurity reduction (a.k.a. information gain for entropy, Gini gain for Gini). Although trees are nonlinear in the inputs, they are linear in indicator functions of regions:\n\\[\n\\hat{f}(\\mathbf{X})=\\sum_{m=1}^M c_m \\mathbf{1}\\left\\{\\mathbf{X} \\in R_m\\right\\}\n\\]\nwith one constant \\(c_m\\) per leaf \\(R_m\\). In our context, each path (“if dose_intensity \\(\\leq 0.9\\) and gene_14 \\(&gt;\\) 1.0 then …”) maps to a region with a clinically interpretable average prediction-mean tumor shrinkage for regression, or class probability for classification.\n\n3.7.1 How the rpart algorithm learns a tree\nHow the rpart algorithm learns a tree The rpart algorithm (recursive partitioning) is an open-source implementation of the CART family. It learns a tree greedily, one split at a time, choosing at each node the feature and cut-point that most improve node purity.\nSplit selection (objective). At a node with samples \\(S\\), rpart scans all predictors \\(x_j\\) and candidate split points \\(s\\), evaluates the impurity of the left child \\(S_L=\\left\\{i: x_{i j} \\leq s\\right\\}\\) and right child \\(S_R=\\left\\{i: x_{i j}&gt;s\\right\\}\\), and picks \\(\\left(j^*, s^*\\right)\\) that maximizes the impurity reduction\n\\[\n\\Delta I=I(S)-\\frac{\\left|S_L\\right|}{|S|} I\\left(S_L\\right)-\\frac{\\left|S_R\\right|}{|S|} I\\left(S_R\\right) .\n\\]\n\nFor regression (our response_percent target), \\(I(\\cdot)\\) is variance.\nFor classification (e.g., high_response), \\(I(\\cdot)\\) is usually Gini (default) or entropy.\n\nContinuous vs. categorical predictors. - Continuous variables (e.g., dose_intensity , gene_14 ) are split at thresholds. Practically, rpart considers cut-points at midpoints between sorted unique values; each candidate produces a left/right partition, and the best \\(\\Delta I\\) wins. - Categorical variables (e.g., tumor_grade with levels G1/G2/G3) are handled by creating a binary partition of levels. rpart orders levels by class composition (for classification) or node means (for regression) and evaluates adjacent two-group splits without trying all \\(2^{m-1}-1\\) combinations. - No scaling is required: decisions depend on orderings, not magnitudes.\nFrom our examples. - In the toy regression, the true rule is: if \\(X_1 \\leq 4\\) then \\(y \\approx 5\\); else if \\(X_2 \\leq 0\\) then \\(y \\approx 10\\); else \\(y \\approx\\) 14. rpart rediscovers this by choosing the vari \\(\\downarrow\\), e-reducing thresholds on \\(X_1\\) and \\(X_2\\). - In the clinical dataset, rpart selected thresholds on gene_14, gene_08, and gene_19 that create leaves whose means trace a clinically plausible gradient of tumor shrinkage.\nStopping and pruning. Left unchecked, greedy splitting makes overly deep, high-variance trees. rpart controls complexity in two ways: - Top-down stops: hyperparameters such as minsplit , minbucket , and maxdepth prevent tiny or overly deep nodes. - Cost-complexity pruning: rpart grows a large tree, computes a sequence of subtrees indexed by the complexity parameter cp , and uses internal cross-validation to estimate the error ( printcp , plotcp). Selecting the 1-SE cp yields a simpler subtree whose error is within one standard error of the minimum, improving generalization.\nMissing data & surrogate splits. If a case is missing the variable used at a node (say gene_14 ), rpart can route it using surrogate splits -backup variables that tend to make the same partition (controlled by usesurrogate, maxsurrogate ). This is practical in clinical datasets with sporadic biomarker gaps.\nWhy this matters for our course.\n\nFor regression (our primary task), trees optimize variance reduction, giving piecewise-constant predictions that capture thresholds and interactions without manual feature engineering.\nFor classification (secondary task with high_response), trees optimize Gini/entropy, naturally handling multi-class extensions and unscaled predictors.\nUnderstanding rpart’s split logic (variance, Gini), candidate generation (midpoints, level partitions), and pruning (cp & 1-SE) demystifies how the if/else rules are learned from data.\n\n\n\n3.7.2 Hyperparameters for decision trees (rpart): what, why, how\nTree growth is intentionally greedy and, left unconstrained, will overfit. In rpart, model complexity is governed by a small set of hyperparameters-values that control learning but are not learned from the data. Tuning them is crucial for generalization.\nConsider again the equation\n\\[Y=f\\left(x_1, x_2, \\ldots, x_n\\right)+ error\\]\nA hyperparameter is a setting that controls how \\(f\\) is learned, not something learned directly from the data by the usual fitting step. Hyperparameters define the shape/complexity of the function class you allow and how aggressively you search within it. They live outside \\(f\\), but they constrain and guide the learning of \\(f\\).\nWhat they control (regression \\(\\&\\)classification)\n\nminsplit\n\nMinimum number of cases required in a node to consider a split. Larger values make the tree more conservative (fewer splits). - minbucket\nMinimum number of cases in each terminal node (leaf). Often set near floor(minsplit/3). - maxdepth\nMaximum number of splits along any root-to-leaf path (tree height). Caps interactions/complexity. - cp (complexity parameter)\nMinimum relative improvement required to add a split. Also indexes the cost-complexity pruning sequence (printcp , plotcp). - xval\nNumber of folds for rpart’s internal cross-validation to estimate out-of-sample error along the pruning path. - parms (classification only)\nSplit criterion: parms = list(split = “gini”) (default) or parms = list(split = “information”) (entropy). - usesurrogate, maxsurrogate\nHandling of missing data via surrogate splits (useful in clinical/omics tables with sporadic NAs).\nWhich impurity? - Regression ( method = “anova” ): variance reduction. - Classification ( method = “class” ): Gini (default) or entropy via parms.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#running-a-tree-for-our-chemotherapy-example",
    "href": "three_methods.html#running-a-tree-for-our-chemotherapy-example",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.8 Running a tree for our chemotherapy example",
    "text": "3.8 Running a tree for our chemotherapy example\nWe will use the function rpart of the rpart package to implement the tree for our chemothreapy case. Recall that we will be explaining response_percente in terms of the all other variables in the data that are of our interest response_percent ~ .\nThe code in the next two chunks first grows a CART regression tree to predict response_percent from all available predictors in train_nopii. We use method = “anova”, which means each split is chosen to reduce within-node variance of the continuous outcome. The control list sets three guardrails that shape the initial tree: cp = 0.001 allows the tree to keep growing as long as each additional split reduces the resubstitution error by at least 0.1%; maxdepth = 8 caps the number of successive decisions along any root-to-leaf path; and minsplit = 30 prevents the algorithm from splitting nodes that hold fewer than 30 training cases. With these settings the model is encouraged to discover structure but not to chase tiny, sample-specific patterns. After fitting, printcp(ct_tree) reports the cost–complexity pruning path. Each row summarizes a candidate subtree obtained by pruning the large tree at a given complexity parameter CP CP. The columns have specific meanings. nsplit is the number of internal splits in that subtree. rel error is the training (resubstitution) error of that subtree relative to the root node error. xerror is the cross-validated error (here from rpart’s built-in K-fold procedure), again on the same relative scale, and xstd is its standard error across folds. In your run, the root node error is shown at the top (“Root node error: 2448864/6999 = 349.89”), which is simply the total sum of squared residuals divided by n n when predicting the overall mean. As the tree grows from nsplit = 0 to nsplit = 13, rel error falls monotonically, while xerror drops quickly and then flattens, indicating diminishing returns from additional complexity. To select a final model that favors parsimony without sacrificing predictive accuracy, the code applies the 1-SE rule. It locates the row with the minimum cross-validated error (xerr_min) and then chooses the smallest subtree whose xerror is within one standard error (xstd_min) of that minimum. The corresponding CP value (cp_1se) defines how aggressively the tree is pruned. prune(ct_tree, cp = cp_1se) returns this compact subtree as ct_pruned. Conceptually, this step trims away branches that improve apparent fit on the training set but do not demonstrably improve out-of-sample performance beyond sampling variability. The two rpart.plot calls draw the grown and pruned trees. Internal nodes display the chosen split conditions; leaves display the predicted outcome (the mean response_percent among training cases in that terminal region) and support counts. Comparing the “grown” and “pruned” plots makes the effect of pruning tangible: superfluous lower-level branches disappear, leaving a smaller set of clinically interpretable rules. For a human-readable summary of those rules, rpart.rules(ct_pruned, style = “tallw”) prints each root-to-leaf path as a nested “when … then …” statement. Your output shows that the pruned tree uses three molecular predictors gene_14, gene_08, and gene_19 to partition the cohort. Thresholds on these continuous features define rectangular regions of the predictor space, and each region has an associated predicted response. For example, when gene_14 &lt; -0.728 and gene_19 &lt; -0.65, the model predicts a mean shrinkage of about 0.97 percentage points; when gene_14 ≥ 0.977 and gene_08 ≥ 2.03, it predicts about 62.68 percentage points. The intermediate leaves trace a graded pattern: as gene_14 and gene_08 thresholds increase and gene_19 crosses its cut-points the predicted response_percent rises through roughly 6.31, 12.83, 20.28, 28.76, 37.38, 47.20, and 53.02 before reaching the top stratum. This staircase behavior reflects the piecewise-constant nature of a regression tree: within each leaf the prediction is constant, and it jumps at the learned split boundaries. Finally, the evaluation chunk applies the pruned tree to both training and test data and summarizes predictive error via MAE and RMSE. The training set errors (MAE ≈ 2.31, RMSE ≈ 3.03) and the held-out test set errors (MAE ≈ 2.43, RMSE ≈ 3.21) are close in magnitude, which is the hallmark of a model that generalizes reasonably well without obvious overfitting. The slight increase on the test set is expected; the absence of a large gap suggests that the 1-SE pruning achieved a good bias–variance balance for this dataset. These results can be obtained running the next chunks.\n\n# Grow a CART regression tree\nct_tree &lt;- rpart(\n  response_percent ~ .,\n  data = train_nopii,\n  method = \"anova\",\n  control = rpart.control(cp = 0.001, maxdepth = 8, minsplit = 30)\n)\n\n# Prune via 1-SE rule\nprintcp(ct_tree)\n\n\nRegression tree:\nrpart(formula = response_percent ~ ., data = train_nopii, method = \"anova\", \n    control = rpart.control(cp = 0.001, maxdepth = 8, minsplit = 30))\n\nVariables actually used in tree construction:\n[1] gene_08 gene_14 gene_19\n\nRoot node error: 2448864/6999 = 349.89\n\nn= 6999 \n\n          CP nsplit rel error   xerror       xstd\n1  0.7924745      0  1.000000 1.000227 0.01081767\n2  0.0826816      1  0.207526 0.209410 0.00397193\n3  0.0455275      2  0.124844 0.126561 0.00236147\n4  0.0159802      3  0.079316 0.081182 0.00179474\n5  0.0138112      4  0.063336 0.067376 0.00134407\n6  0.0057965      5  0.049525 0.052308 0.00106611\n7  0.0036277      6  0.043729 0.046388 0.00096049\n8  0.0031558      7  0.040101 0.042604 0.00091233\n9  0.0030375      8  0.036945 0.039700 0.00087026\n10 0.0029545      9  0.033908 0.038201 0.00083974\n11 0.0022781     10  0.030953 0.033671 0.00067939\n12 0.0012786     11  0.028675 0.031293 0.00062538\n13 0.0010818     12  0.027396 0.030282 0.00060776\n14 0.0010000     13  0.026315 0.029300 0.00059060\n\nbest_row &lt;- which.min(ct_tree$cptable[,\"xerror\"])\nxerr_min &lt;- ct_tree$cptable[best_row, \"xerror\"]\nxstd_min &lt;- ct_tree$cptable[best_row, \"xstd\"]\ncp_1se   &lt;- ct_tree$cptable[ct_tree$cptable[,\"xerror\"] &lt;= xerr_min + xstd_min, \"CP\"][1]\nct_pruned &lt;- prune(ct_tree, cp = cp_1se)\n\n# Human-readable rules (if/else)\nrpart.plot::rpart.rules(ct_pruned, style = \"tallw\")\n\nresponse_percent is  0.97 when\n                          gene_14 &lt; -0.728\n                          gene_19 &lt; -0.65\n\nresponse_percent is  6.31 when\n                          gene_14 is -0.728 to 0.014\n                          gene_19 &lt; -0.65\n\nresponse_percent is  8.17 when\n                          gene_14 &lt; -0.523\n                          gene_08 &lt; -0.37\n                          gene_19 &gt;= -0.65\n\nresponse_percent is 12.83 when\n                          gene_14 is -0.523 to 0.014\n                          gene_08 &lt; -0.37\n                          gene_19 &gt;= -0.65\n\nresponse_percent is 15.29 when\n                          gene_14 &lt; 0.014\n                          gene_08 &gt;= -0.37\n                          gene_19 is -0.65 to -0.18\n\nresponse_percent is 20.28 when\n                          gene_14 &lt; 0.014\n                          gene_08 &gt;= -0.37\n                          gene_19 &gt;= -0.18\n\nresponse_percent is 23.26 when\n                          gene_14 is 0.014 to 0.977\n                          gene_08 &lt; 0.25\n                          gene_19 &lt; 0.46\n\nresponse_percent is 28.76 when\n                          gene_14 is 0.014 to 0.977\n                          gene_08 &gt;= 0.25\n                          gene_19 &lt; 0.46\n\nresponse_percent is 32.26 when\n                          gene_14 is 0.014 to 0.977\n                          gene_08 &lt; 0.71\n                          gene_19 &gt;= 0.46\n\nresponse_percent is 37.38 when\n                          gene_14 is 0.014 to 0.977\n                          gene_08 &gt;= 0.71\n                          gene_19 &gt;= 0.46\n\nresponse_percent is 41.61 when\n                          gene_14 &gt;= 0.977\n                          gene_08 &lt; 1.56\n                          gene_19 &lt; 1.25\n\nresponse_percent is 47.20 when\n                          gene_14 &gt;= 0.977\n                          gene_08 &lt; 1.56\n                          gene_19 &gt;= 1.25\n\nresponse_percent is 53.02 when\n                          gene_14 &gt;= 0.977\n                          gene_08 is 1.56 to 2.03\n\nresponse_percent is 62.68 when\n                          gene_14 &gt;= 0.977\n                          gene_08 &gt;= 2.03\n\n\n\n3.8.1 Printing the trees\n\n# Visualize grown and pruned trees\nrpart.plot(ct_tree, type = 2, extra = 101, box.palette = \"Blues\",\n           main = \"Clinical trial tree (grown)\")\n\n\n\n\n\n\n\nrpart.plot(ct_pruned, type = 2, extra = 101, box.palette = \"GnBu\",\n           main = \"Clinical trial tree (pruned, 1-SE)\")\n\n\n\n\n\n\n\n\n\n3.8.1.1 Interpreting the grown and pruned trees\nThe two plots visualize successive stages of the same model. The first represents the grown tree-the full structure obtained when the algorithm keeps splitting the data as long as it finds any measurable reduction in node impurity. The second shows the pruned tree, obtained after applying the 1-SE rule of cost-complexity pruning, which removes branches that do not materially improve cross-validated performance.\nIn the grown tree, the structure is deeper and more branched. Each internal node corresponds to a binary decision of the form “Is gene \\({ }_{(\\mathrm{j})} \\leq \\mathrm{s}\\) ?”, and each terminal node (leaf) stores the mean value of response_percent among observations satisfying that sequence of conditions. The abundance of splits reflects the model’s flexibility: by recursively partitioning the data into small, homogeneous subsets, the tree can achieve very low training error. However, such detailed partitioning often adapts to random noise or idiosyncratic fluctuations in the sample-a phenomenon known as overfitting. The grown tree therefore fits the training set extremely well but may generalize poorly to new patients.\nThe pruned tree, by contrast, is shallower and more compact. Pruning starts from the large tree and iteratively removes branches that contribute the least to predictive accuracy, as measured by crossvalidated error. The 1-SE rule selects the simplest subtree whose error is within one standard error of the minimum observed, trading a negligible increase in bias for a substantial reduction in variance. In practice, this yields a model that performs almost as well on unseen data but is far more stable and interpretable. In the chemotherapy dataset, the pruned tree retains only the strongest and most reproducible thresholds-those involving, for example, gene_14 , gene_08 , and gene_19 -which together define a hierarchy of molecular conditions associated with different levels of tumor response.\nEach box in the plots displays the predicted mean (the constant \\(\\hat{y}\\) for that region) and the number of cases supporting that leaf. In the pruned tree, leaves are larger (more patients per region) and predictions vary more smoothly, reflecting a coarser but more reliable partition of the predictor space. The disappearance of lower-level branches illustrates how pruning merges overly specific regions back into their parents, simplifying the decision rules.\nThe human-readable output produced by\nrpart.rules(ct_pruned, style = \"tallw\")\ntranslates each root-to-leaf path into an explicit if-else statement, such as:\nWhen gene_14 &lt; -0.73 and gene_19 &lt; -0.65, predict response_percent \\(\\approx 1.0\\); when gene_14 \\(\\geq 0.98\\) and gene_08 \\(\\geq 2.03\\), predict \\(\\approx 62.7\\).\nThese rules correspond exactly to the leaves of the pruned tree and can be read as localized predictive statements: each describes a subpopulation with a characteristic mean response.\nIn summary, the grown tree shows everything the recursive partitioning algorithm could learn from the data, while the pruned tree shows what it should retain to balance interpretability and predictive reliability. The pruned version embodies the principle of parsimonious generalization-capturing the dominant structure in the data without chasing noise-an essential practice in applying machinelearning models to clinical and therapeutic contexts.\n\npred_tr &lt;- predict(ct_pruned, newdata = train_nopii)\npred_te &lt;- predict(ct_pruned, newdata = test_nopii)\n\nperf &lt;- dplyr::bind_rows(\n  tibble(Split=\"Train\", MAE=mae(train_nopii$response_percent, pred_tr),\n         RMSE=rmse(train_nopii$response_percent, pred_tr)),\n  tibble(Split=\"Test\",  MAE=mae(test_nopii$response_percent,  pred_te),\n         RMSE=rmse(test_nopii$response_percent,  pred_te))\n)\nperf\n\n# A tibble: 2 × 3\n  Split   MAE  RMSE\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Train  2.31  3.03\n2 Test   2.43  3.21\n\n\nIt is useful to build the following graphic about the model too\n\nggplot(\n  tibble(truth = test_nopii$response_percent, pred = pred_te),\n  aes(truth, pred)\n) +\n  geom_point(alpha = 0.35) +\n  geom_abline(slope = 1, intercept = 0, linetype = 2) +\n  labs(x = \"Observed response_percent\", y = \"Tree prediction\",\n       title = \"Pruned tree: observed vs predicted (test)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe diagonal dashed line marks perfect calibration. The cloud of points hugging this line indicates that, on average, the pruned tree predicts within a small error margin on unseen data, consistent with the MAE/RMSE reported earlier. The horizontal bands are characteristic of regression trees: within each leaf the prediction is a single constant, so many cases share the same ŷ even when their observed values differ; each band corresponds to one leaf’s mean response. Vertical spread around the diagonal within a band reflects the within-leaf variance (irreducible noise plus any misspecification). At low and high ends you may notice slight deviations due to boundary effects (responses constrained near 0 or high shrinkage). Overall, proximity to the 45° line and relatively balanced dispersion across the range suggest the pruned tree achieves a good bias–variance trade-off on the test set.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#solutions-for-trees-problems",
    "href": "three_methods.html#solutions-for-trees-problems",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.9 Solutions for trees problems",
    "text": "3.9 Solutions for trees problems\nDecision trees are among the most intuitive predictive models in machine learning. They partition the predictor space using simple, rule-based splits, producing a structure that is easy to interpret and explain. This transparency makes trees attractive in settings where clarity and decision logic matter such as clinical environments, quality assurance, or regulatory discussions. However, the strengths of a single tree are also its weaknesses. Trees are unstable: small fluctuations in the training data can lead to large changes in the learned structure. A model that splits first on tumour grade in one sample might split first on age or dose intensity in another, even if the underlying population signal is the same. Decision trees are also prone to overfitting, especially when grown deep. They can “memorize” noise, rare patterns, or outliers, achieving excellent accuracy on the training set while generalizing poorly to new patients. These limitations appear clearly in our running example: predicting response_percent in a chemotherapy trial using clinical covariates plus approximately 2,000 gene expression variables (after removing the columns you excluded earlier). A single tree can indeed learn meaningful rules dose intensity thresholds, gene expression activation points, or interactions between tumour grade and age but it may also capture highly specific patterns that do not repeat in unseen data. As complexity grows, the risk increases that the tree fits patient-specific noise rather than biologically grounded structure. A powerful solution is to move from relying on one tree to combining many. This shift from a solitary model to a coordinated collection of models is known as ensemble learning. Ensembles reduce instability, limit overfitting, and yield predictions that are more reliable and accurate. They leverage the simple idea that while individual trees may be noisy or inconsistent, the aggregate of many trees can reveal the true underlying signal.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#ensemble-techniques",
    "href": "three_methods.html#ensemble-techniques",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.10 Ensemble techniques",
    "text": "3.10 Ensemble techniques\nEnsemble methods are a family of strategies in which multiple models often weak, unstable, or low-capacity learners are trained and then combined to produce a stronger predictor. The guiding principle is straightforward:\n\nInstead of trusting a single model’s view of the data, we consult many models and integrate their insights.\n\nThis idea mirrors how groups often make decisions: diverse perspectives, when aggregated sensibly, tend to outperform any single viewpoint. In machine learning, ensembles achieve this by reducing variance, reducing bias, or exploiting complementary strengths across different algorithms.\nWhile ensemble learning can be applied to any type of model, trees are particularly well suited for it. Their instability makes them ideal candidates: many slightly different trees, trained on varied samples or focused on different aspects of the data, can collectively smooth out one another’s mistakes.\nA central tool behind many ensemble methods is bootstrapping drawing multiple datasets by sampling with replacement from the original training data. Each bootstrap sample contains a slightly different mix of observations: some patients appear multiple times, some not at all. Training a model on each of these bootstrap samples produces a collection of slightly different learners. When we aggregate their predictions (for example, by averaging in regression), random fluctuations tend to cancel out, and the ensemble becomes more stable than any single model.\nThis combination of bootstrap sampling and aggregation is known as bootstrap aggregating, or bagging. Bagging is particularly effective for high-variance models such as decision trees: instead of one tree that may overreact to idiosyncrasies in the data, we obtain many trees, each seeing a slightly different world, and we average their predictions to reduce variance.\nEnsemble strategies come in several forms, with three major families commonly used in practice:\n\nBagging, which uses bootstrap sampling to create many resampled versions of the training set, trains a separate model on each, and then aggregates their predictions. This primarily reduces variance, stabilizing unstable learners such as trees. This approach is used by random forest models.\nBoosting, which reduces bias (and often variance) by training models sequentially, each one focusing on the errors or residuals of the current ensemble and gradually improving performance.\nStacking, which learns how to combine the predictions of diverse algorithms through a meta-model that takes their outputs as inputs and learns an optimal way to blend them.\n\nBefore exploring these families in detail, it is crucial to understand that all ensemble methods share the same foundational principle: multiple models, when combined thoughtfully, can achieve higher accuracy, greater stability, and better generalization than any single model acting alone. Bootstrapping and aggregation in bagging provide a concrete and widely used example of how this principle is implemented in practice.\n\n3.10.1 Training models on sampled data: Bootstrap Aggregating (Bagging)\nMachine learning models especially decision trees can be sensitive to noise, outliers, and small sampling fluctuations. In clinical and biomedical datasets, this instability becomes even more pronounced: measurement error, biological heterogeneity, and uneven sampling across patient subgroups can all lead a single model to overfit.\nIn our running example, where we aim to predict response_percent using clinical covariates and approximately 2,000 gene expression features, a single decision tree may latch onto idiosyncratic patterns that do not generalize beyond the training patients.\nBootstrap aggregating, or bagging, is a technique designed to address this problem. The idea is straightforward: instead of training one model on one dataset, we train many variations of the same model on many slightly different datasets, each created by a random resampling technique named bootstrap, and then combine (aggregate) their outputs. By averaging over many high-variance learners, bagging produces predictions that are more stable, more accurate, and less sensitive to noise.\n\n\n\nSchematic explanation on bagging.\n\n\n\n\n3.10.2 How bagging works\nThe bagging workflow can be summarized in five steps:\n\nChoose how many sub-models to train (for example, 200 trees).\nDraw a bootstrap sample for each sub-model by sampling patients with replacement from the training set until the sample is the same size as the original training data.\n\nSome patients appear multiple times.\nSome are not selected at all.\n\nTrain a sub-model on each bootstrap sample.\n\nIn our setting: a decision tree trained on a resampled set of patients with resampled gene expression profiles.\n\nGenerate predictions for new data using every sub-model.\nAggregate the predictions.\n\nFor regression (our task): take the mean of the predicted values.\nFor classification: take the majority vote.\n\n\nThe critical mechanism is the bootstrap sampling itself. When sampling with replacement, cases near the center of the data distribution tend to be selected more frequently than rare or extreme observations. Some bootstrap datasets will contain more extreme cases than others; some trees will fit these extremes poorly. But when aggregated, these idiosyncrasies tend to cancel out. The ensemble prediction is effectively an average across many plausible models, each capturing different aspects of the training data.\nThe net effect is a substantial reduction in variance the component of prediction error driven by model instability.\n\n\n3.10.3 Why bagging helps in our chemotherapy-trial case study\nIn the chemotherapy response dataset, decision trees face three major challenges:\n\nHigh dimensionality: thousands of gene expressions.\nMeasurement noise: assay variability, heterogeneous tumour biology.\nComplex interactions: clinical and molecular variables interact in ways that are hard to model with a single tree.\n\nA single deep tree may overfit heavily detecting spurious splits driven by noisy gene measurements or by small patient subgroups. Bagging mitigates this risk by averaging many such trees, each trained on a slightly different bootstrap sample. Trees that “overreact” to particular patients or gene-expression artefacts have their influence diminished when averaged with hundreds of others.\nFor this reason, a bagged ensemble of trees often forms a far more robust predictor than any individual tree, especially in biomedical datasets where high-variance learning is a known challenge.\nAs you will see later, the Random Forest algorithm builds directly on this idea: it is essentially bagging with an additional layer of randomness, making it one of the most powerful and widely used tree-based ensemble models in modern machine learning.\n\n\n3.10.4 Learning from Previous Models’ Mistakes: Boosting\nWhere bagging creates many models in parallel and averages their predictions to reduce variance, boosting takes a different approach. Boosting also builds an ensemble of models, but does so sequentially, allowing each new model to focus specifically on the errors left behind by the models that came before it. The core idea is simple: start with a rough model, identify where it performs poorly, and train the next model to correct those mistakes. Repeating this process many times gradually produces a highly accurate and flexible predictor.\nJust as bagging can be applied to a wide range of supervised learning algorithms, boosting is also a general framework. However, it is especially effective when using weak learners-models that are individually simple and only slightly better than random guessing. In practice, this typically means shallow decision trees, often trees with only a few levels of depth or even trees with a single split. These minimal trees are fast to train, easy to update, and-when used in large numbers-combine to form surprisingly powerful models.\nThe motivation for using weak learners is efficiency: boosting does not benefit from repeatedly training deep, complex trees. The strength of the ensemble comes from the sequence of corrections, not from any individual model. In our chemotherapy-response example, using shallow trees allows the boosting algorithm to slowly uncover clinical or molecular patterns-first correcting broad systematic errors, then gradually refining more subtle relationships among dose intensity, tumour characteristics, and geneexpression features.\n\nBoosting methods differ in how they decide which mistakes to correct. Two major families exist:\n\nAdaptive boosting, which increases the influence of cases that have been misclassified (or poorly predicted) so that subsequent models pay more attention to them. Diagram in Figure 3.4 represents adaptive boosting visually.\n\n\n\n\n\n\n\n\n\nFigure 3.4: Schematic of AdaBoost: case weights are updated at each step, and models vote with weights in the final ensemble.\n\n\n\n\n\nGradient boosting, which directly models the residual errors of the current ensemble, effectively learning a sequence of corrections that push predictions closer to the true values. The diagram in Figure 3.5 represents a gradient boosting strategy.\n\n\n\n\n\n\n\n\n\nFigure 3.5: Schematic of boosting: weak learners are added sequentially, each correcting the errors of the current ensemble.\n\n\n\n\n\n\n3.10.5 Stacking\n\n\n\n\n\n\n\n\nFigure 3.6: Schematic of stacking: multiple base learners feed their predictions into a meta-model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#setting-up-test-and-train-datasets",
    "href": "three_methods.html#setting-up-test-and-train-datasets",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.11 Setting up test and train datasets",
    "text": "3.11 Setting up test and train datasets\nWe already learnt the importance of using training and testing datasets in our modelling procedures. In this section we will prepare the such datasets to be used then to fit random forests and XGboost analysis.\n\nset.seed(2025)\n\nlibrary(dplyr)\n\n# 1) Train/test split (70/30) -------------------------\nn &lt;- nrow(trial_ct)\nidx_train &lt;- sample(seq_len(n), size = 0.7 * n)\n\ntrain &lt;- trial_ct[idx_train, ]\ntest  &lt;- trial_ct[-idx_train, ]\n\n# 2) True outcomes ------------------------------------\ny_train &lt;- train$response_percent\ny_test  &lt;- test$response_percent",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#random-forests",
    "href": "three_methods.html#random-forests",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.12 Random Forests",
    "text": "3.12 Random Forests\nRandom Forests generalize bootstrap aggregating by introducing feature-level stochasticity during tree construction. Each tree is trained on a bootstrap sample of the data, and at every split the algorithm selects the best partition only from a randomly drawn subset of predictors (of size mtrym_{}mtry​). This mechanism reduces the correlation between trees, which in turn lowers the variance of the aggregated ensemble estimator. Random Forests are consistent for both regression and classification, provide unbiased estimates of generalization error via out-of-bag predictions, and incorporate variable importance metrics based on impurity reduction or permutation. Their ability to approximate complex interaction structures without explicit feature engineering, combined with robustness to high-dimensional predictors and noisy inputs, makes them a powerful nonparametric baseline for tabular biomedical data. Despite limited interpretability relative to single trees, Random Forests offer strong predictive performance and stability across heterogeneous clinical settings.\nWith the following lines of code we will learn how to run a random forest example in our chemoterapy case. This code fits a Random Forest regression model to predict tumour response percentage from a collection of clinical and molecular features. The model is built using the ranger package, a fast and scalable implementation designed for high-dimensional datasets such as ours, which contains thousands of gene-expression variables. The formula response_percent ~ . specifies that all available predictors should be used, but four variables are explicitly removed from the model: patient_id (an identifier), high_response (a derived binary outcome that would leak information), and the tumour measurements baseline_tumor_mm and post_tumor_mm, which are deterministically related to the response and would therefore artificially inflate model performance.\nThe call to ranger() constructs an ensemble of 500 decision trees. At each split, instead of evaluating all predictors, the algorithm considers only a random subset whose size is defined by mtry. Here, mtry is set to the square root of the effective number of predictors, a widely used heuristic that helps decorrelate the trees and therefore reduce variance in the final ensemble. The model also computes impurity-based variable importance, which allows later examination of which genes or clinical features contributed most strongly to the predictions.\nOnce the forest has been trained, the model is applied to both the training and test sets to obtain predicted tumour-response percentages. These predictions are extracted via the $predictions element of the output returned by predict(). The final section evaluates model performance by computing mean absolute error (MAE) and root-mean-square error (RMSE) using the helper function eval_perf(). Computing performance on the training set allows us to assess whether the forest has fit the data effectively, whereas evaluating on the test set quantifies generalization to unseen patients a crucial step in understanding whether the model can support predictive decision-making in a clinical or therapeutic context.\n\nlibrary(ranger)\n\nrf_fit &lt;- ranger(\n  response_percent ~ . \n    - patient_id \n    - high_response \n    - baseline_tumor_mm \n    - post_tumor_mm,\n  data       = train,\n  num.trees  = 500,\n  mtry       = floor(sqrt(ncol(train) - 5)),  # approx: drop 4 predictors + outcome\n  importance = \"impurity\"\n)\n\nGrowing trees.. Progress: 87%. Estimated remaining time: 4 seconds.\n\n# Predictions ------------------------------------------------------\n\nrf_pred_train &lt;- predict(rf_fit, data = train)$predictions\nrf_pred_test  &lt;- predict(rf_fit, data = test)$predictions\n\n# Performance ------------------------------------------------------\n\nrf_perf_train &lt;- eval_perf(y_train, rf_pred_train)\nrf_perf_test  &lt;- eval_perf(y_test,  rf_pred_test)\n\nrf_perf_train\n\n# A tibble: 1 × 2\n    MAE  RMSE\n  &lt;dbl&gt; &lt;dbl&gt;\n1  2.17  2.68\n\nrf_perf_test\n\n# A tibble: 1 × 2\n    MAE  RMSE\n  &lt;dbl&gt; &lt;dbl&gt;\n1  5.54  6.80",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#xgboost",
    "href": "three_methods.html#xgboost",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.13 XGboost",
    "text": "3.13 XGboost\nGradient boosting is one of the most powerful ideas in modern machine learning. While Random Forests reduce variance by averaging many decorrelated trees, boosting takes the opposite approach: it builds trees sequentially, where each new tree attempts to correct the errors of the ensemble so far. XGBoost (Extreme Gradient Boosting) is a highly optimized implementation of this idea and has become the dominant algorithm for tabular biomedical prediction, especially in high-dimensional settings with complex nonlinear relationships, such as gene-expression data in therapeutic studies.\nIn boosting, the model begins with a simple prediction often the mean value of the outcome and then iteratively adds small regression trees. Each new tree is fitted to the residuals (the mistakes) of the current model. Because each tree is intentionally shallow, it captures only a small part of the remaining structure. However, when hundreds of these trees are combined, the model can approximate highly intricate interactions, nonlinearities, and threshold behaviours. Crucially, XGBoost incorporates additional mechanisms that make it robust and scalable: L1 and L2 regularization, learning-rate shrinkage, column and row subsampling, and efficient handling of sparse matrices. Together, these features make XGBoost far less prone to overfitting than naïve boosting methods, even with thousands of predictors.\nThe code below shows how XGBoost is trained to predict tumour-response percentage using the chemotherapy dataset.\nFirst, we construct the predictor matrices for training and test sets. Because XGBoost expects purely numeric input, we use model.matrix() to convert categorical variables into dummy indicators and to ensure the same set of columns is used across both datasets. All leakage-prone variables (patient_id, high_response, baseline_tumor_mm, post_tumor_mm, and the outcome response_percent) are removed from the predictor matrix. The remaining clinical and molecular features form a high-dimensional design matrix with potentially thousands of columns, which XGBoost handles naturally.\nNext, the data are converted into xgb.DMatrix objects. This format stores the matrix efficiently and allows XGBoost to apply internal optimizations such as sparse-feature handling and fast column access. Labels (the continuous response values) are attached here.\nThe hyperparameters defined in xgb_params specify the behaviour of the boosting process. We use objective = \"reg:squarederror\" because this is a continuous regression task. The parameter eta controls the learning rate: each tree only makes a small correction to the existing model, which stabilizes training. The arguments max_depth, subsample, and colsample_bytree restrict the complexity of individual trees and the diversity of information they see, thereby preventing overfitting and encouraging generalization.\nTraining occurs through xgb.train(), which iteratively builds 300 boosting rounds. Each iteration grows a shallow tree tailored to the current residuals. Because the watchlist contains the training set, XGBoost can report internal diagnostics (here suppressed with verbose = 0).\nFinally, predictions on both training and test data are obtained with predict(). These predicted continuous response values are evaluated using MAE and RMSE, allowing direct comparison with linear models, trees, and Random Forests. Typically, XGBoost provides the strongest performance in this type of biological regression setting, as it captures subtle gene–gene interactions, nonlinear dose effects, and patient heterogeneity more effectively than any single model family.\n\nlibrary(xgboost)\n\n# 1) Build predictor matrices -------------------------------------\n\ncols_to_drop &lt;- c(\n  \"patient_id\",\n  \"high_response\",\n  \"baseline_tumor_mm\",\n  \"post_tumor_mm\",\n  \"response_percent\"   # outcome must also be removed from X\n)\n\nX_train &lt;- model.matrix(\n  ~ .,\n  data = train |&gt; select(-all_of(cols_to_drop))\n)\n\nX_test &lt;- model.matrix(\n  ~ .,\n  data = test  |&gt; select(-all_of(cols_to_drop))\n)\n\n# Optional: check dimensions\ndim(X_train)\n\n[1] 7000 2007\n\ndim(X_test)\n\n[1] 3000 2007\n\n# 2) Convert to DMatrix -------------------------------------------\n\ndtrain &lt;- xgb.DMatrix(data = X_train, label = y_train)\ndtest  &lt;- xgb.DMatrix(data = X_test,  label = y_test)\n\n# 3) Basic XGBoost hyperparameters --------------------------------\n\nxgb_params &lt;- list(\n  objective        = \"reg:squarederror\",\n  eta              = 0.05,   # learning rate\n  max_depth        = 4,\n  subsample        = 0.7,    # row subsampling\n  colsample_bytree = 0.7     # column subsampling\n)\n\n# 4) Train XGBoost model ------------------------------------------\n\nxgb_fit &lt;- xgb.train(\n  params    = xgb_params,\n  data      = dtrain,\n  nrounds   = 300,\n  watchlist = list(train = dtrain),\n  verbose   = 0\n)\n\n# 5) Predictions ---------------------------------------------------\n\nxgb_pred_train &lt;- predict(xgb_fit, dtrain)\nxgb_pred_test  &lt;- predict(xgb_fit, dtest)\n\n# 6) Performance ---------------------------------------------------\n\nxgb_perf_train &lt;- eval_perf(y_train, xgb_pred_train)\nxgb_perf_test  &lt;- eval_perf(y_test,  xgb_pred_test)\n\nxgb_perf_train\n\n# A tibble: 1 × 2\n    MAE  RMSE\n  &lt;dbl&gt; &lt;dbl&gt;\n1 0.958  1.26\n\nxgb_perf_test\n\n# A tibble: 1 × 2\n    MAE  RMSE\n  &lt;dbl&gt; &lt;dbl&gt;\n1  1.34  1.79",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#comparison-random-forests-and-xgboost-for-our-data",
    "href": "three_methods.html#comparison-random-forests-and-xgboost-for-our-data",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.14 Comparison Random Forests and XGboost for our data",
    "text": "3.14 Comparison Random Forests and XGboost for our data\nAfter fitting both a Random Forest and an XGBoost model to the clinical trial dataset, we evaluate their predictive performance on the test set using the same metrics employed throughout the chapter mean absolute error (MAE) and root mean squared error (RMSE). The following code constructs a simple comparison table:\n\nlibrary(dplyr)\n\nrf_xgb_compare &lt;- bind_rows(\n  \"Random Forest\" = rf_perf_test,\n  \"XGBoost\"       = xgb_perf_test,\n  .id = \"Model\"\n)\n\nrf_xgb_compare\n\n# A tibble: 2 × 3\n  Model           MAE  RMSE\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n1 Random Forest  5.54  6.80\n2 XGBoost        1.34  1.79\n\n\nThis comparison highlights a substantial difference in predictive accuracy between the two ensemble methods. The Random Forest, which aggregates many decorrelated decision trees built on bootstrap samples, provides solid performance with MAE = 5.58 and RMSE = 6.84 on the test set values typical of a stable but variance-oriented ensemble. In contrast, the XGBoost model achieves dramatically lower error, with MAE = 1.37 and RMSE = 1.82, reducing both metrics by more than a factor of three.\nThis improvement reflects the fundamental difference in how the two algorithms learn. Random Forests reduce variance by averaging many deep, high-variance trees grown independently; XGBoost, instead, builds trees sequentially, with each tree correcting the residuals of the previous ensemble. The combination of a small learning rate, explicit L1/L2 regularization, and shallow trees allows XGBoost to capture nonlinear and interaction patterns more efficiently and with greater stability.\nIn this dataset characterized by nonlinear dose–response relationships, interactions between tumour grade and gene-expression features, and substantial patient heterogeneity the gradient-boosting strategy produces a far more accurate approximation of the underlying biological response function. This illustrates why boosted tree models often outperform bagging-based methods in biomedical prediction tasks where subtle patterns, thresholds, and gene-level interactions play a central role.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#comparing-random-forests-and-xgboost-with-ols-lasso-ridge-and-elastic-net",
    "href": "three_methods.html#comparing-random-forests-and-xgboost-with-ols-lasso-ridge-and-elastic-net",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.15 Comparing Random Forests and XGboost with OLS, LASSO, Ridge and Elastic NET",
    "text": "3.15 Comparing Random Forests and XGboost with OLS, LASSO, Ridge and Elastic NET\n\n# Columns we do NOT want as predictors\ncols_to_drop &lt;- c(\"patient_id\", \"high_response\", \"baseline_tumor_mm\", \"post_tumor_mm\")\n\n# Create reduced train/test data frames for modeling\ntrain_lm &lt;- train |&gt;\n  dplyr::select(-all_of(cols_to_drop))\n\ntest_lm &lt;- test |&gt;\n  dplyr::select(-all_of(cols_to_drop))\n\n# Ordinary Least Squares (no regularization)\nols_fit &lt;- lm(\n  response_percent ~ .,\n  data = train_lm\n)\n\nols_pred_train &lt;- predict(ols_fit, newdata = train_lm)\nols_pred_test  &lt;- predict(ols_fit, newdata = test_lm)\n\nols_perf_train &lt;- eval_perf(y_train, ols_pred_train)\nols_perf_test  &lt;- eval_perf(y_test,  ols_pred_test)\n\nols_perf_test\n\n# A tibble: 1 × 2\n    MAE  RMSE\n  &lt;dbl&gt; &lt;dbl&gt;\n1  1.83  2.28\n\n\n\nlibrary(glmnet)\n\n# Ridge regression (alpha = 0) ----------------------------\n\nridge_cv &lt;- cv.glmnet(\n  x = X_train,\n  y = y_train,\n  alpha = 0\n)\n\nridge_pred_train &lt;- as.numeric(predict(ridge_cv, newx = X_train, s = \"lambda.min\"))\nridge_pred_test  &lt;- as.numeric(predict(ridge_cv, newx = X_test,  s = \"lambda.min\"))\n\nridge_perf_train &lt;- eval_perf(y_train, ridge_pred_train)\nridge_perf_test  &lt;- eval_perf(y_test,  ridge_pred_test)\n\n# Lasso regression (alpha = 1) ---------------------------\n\nlasso_cv &lt;- cv.glmnet(\n  x = X_train,\n  y = y_train,\n  alpha = 1\n)\n\nlasso_pred_train &lt;- as.numeric(predict(lasso_cv, newx = X_train, s = \"lambda.min\"))\nlasso_pred_test  &lt;- as.numeric(predict(lasso_cv, newx = X_test,  s = \"lambda.min\"))\n\nlasso_perf_train &lt;- eval_perf(y_train, lasso_pred_train)\nlasso_perf_test  &lt;- eval_perf(y_test,  lasso_pred_test)\n\n# Elastic Net (alpha between 0 and 1) --------------------\n\nelastic_cv &lt;- cv.glmnet(\n  x = X_train,\n  y = y_train,\n  alpha = 0.5   # 0.5 = equal mix of L1 and L2; you can tune this\n)\n\nelastic_pred_train &lt;- as.numeric(predict(elastic_cv, newx = X_train, s = \"lambda.min\"))\nelastic_pred_test  &lt;- as.numeric(predict(elastic_cv, newx = X_test,  s = \"lambda.min\"))\n\nelastic_perf_train &lt;- eval_perf(y_train, elastic_pred_train)\nelastic_perf_test  &lt;- eval_perf(y_test,  elastic_pred_test)\n\n\nbenchmark_tbl &lt;- dplyr::bind_rows(\n  \"OLS\"           = ols_perf_test,\n  \"Ridge\"         = ridge_perf_test,\n  \"Lasso\"         = lasso_perf_test,\n  \"Elastic Net\"   = elastic_perf_test,\n  \"Random Forest\" = rf_perf_test,\n  \"XGBoost\"       = xgb_perf_test,\n  .id = \"Model\"\n)\n\nbenchmark_tbl\n\n# A tibble: 6 × 3\n  Model           MAE  RMSE\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt;\n1 OLS            1.83  2.28\n2 Ridge          1.78  2.22\n3 Lasso          1.53  1.92\n4 Elastic Net    1.53  1.92\n5 Random Forest  5.54  6.80\n6 XGBoost        1.34  1.79\n\n\nTo evaluate how different modelling strategies behave in our example, we compared six regression models OLS, Lasso, Ridge, Elastic Net, Random Forest, and XGBoost on the same task: predicting response_percent, a continuous measure of tumour shrinkage in a chemotherapy trial. All models were trained on the same 70% split and evaluated on the same 30% test set using identical predictors: clinical variables and approximately 2,000 gene-expression features (after excluding identifiers and tumour-size measurements).\n\n3.15.1 Linear Models with and without Regularization\nOLS offers a transparent baseline but performs poorly with thousands of correlated gene features, leading to unstable coefficients and weak generalization (MAE ≈ 1.83; RMSE ≈ 2.28)\nRidge stabilizes coefficients through L2 shrinkage, producing modest gains but preserving all predictors, which limits interpretability.\nLasso substantially improves performance (MAE ≈ 1.53; RMSE ≈ 1.92) by selecting a sparse subset of informative genes, making it both predictive and biologically interpretable.\nElastic Net, combining L1 and L2 penalties, achieves the best performance among linear models (MAE ≈ 1.52; RMSE ≈ 1.91), particularly well-suited for groups of correlated genes commonly found in expression data.\n\n\n3.15.2 Tree-Based Ensemble Models\nRandom Forest performs unexpectedly poorly in this ultra-high-dimensional setting (MAE ≈ 5.61; RMSE ≈ 6.87). With so many predictors, random subsets rarely contain strong signals, leading to noisy splits and poor generalization.\nXGBoost delivers the strongest predictive performance overall (MAE ≈ 1.32; RMSE ≈ 1.78). Its sequential boosting mechanism targets residual structure directly, while regularization (L1 + L2), subsampling, and shallow trees help control overfitting. This allows XGBoost to recover nonlinear relationships and interaction effects that Random Forest fails to capture.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "three_methods.html#trees-for-classification-tasks",
    "href": "three_methods.html#trees-for-classification-tasks",
    "title": "3  Supervised Learning: Tree Methods",
    "section": "3.16 Trees for classification tasks",
    "text": "3.16 Trees for classification tasks\nIn the previous section, we used trees to predict a continuous response (response_percent). Decision trees, however, can also be used for classification tasks, where the goal is to predict a categorical outcome such as high_response (1 = high responder, 0 = low responder).\n\nThis type of model belongs to the Classification and Regression Tree (CART) family the same general framework we used for regression trees, but with a different impurity measure and output interpretation.\nIn a classification tree, each node represents a subset of the data that is more or less “pure” with respect to the outcome classes. The tree is built by recursively splitting the data into increasingly homogeneous groups, using thresholds on explanatory variables (e.g., gene expression, dose intensity, tumor grade).\n\n3.16.1 Model setup\nWe will use the same chemotherapy trial dataset as before, but now we define the binary target high_response (1 = tumor reduction ≥ 30%) and use the function rpart() with method = \"class\".\n\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(dplyr)\n\n# 1) Make sure the binary target exists\n\ntrial_ct &lt;- readRDS(\"~/att_ai_ml/data/trial_ct_chemo_cont.rds\")\n\ntrial_ct &lt;- trial_ct %&gt;%\nmutate(high_response = as.integer(response_percent &gt;= 30))\n\n# 2) Drop ID and leakage columns\n\ncols_drop &lt;- c(\"patient_id\", \"response_percent\", \"baseline_tumor_mm\", \"post_tumor_mm\")\ntrial_ct &lt;- trial_ct %&gt;% select(-any_of(cols_drop))\n\n# 3) Train/test split (70/30)\n\nset.seed(123)\nn &lt;- nrow(trial_ct)\nidx_train &lt;- sample(seq_len(n), size = 0.7 * n)\ntrain_cls &lt;- trial_ct[idx_train, ]\ntest_cls  &lt;- trial_ct[-idx_train, ]\n\n# 4) Fit classification tree\n\nctree_cls &lt;- rpart(\nhigh_response ~ .,\ndata = train_cls,\nmethod = \"class\",\ncontrol = rpart.control(cp = 0.001, maxdepth = 8, minsplit = 30)\n)\n\n\n\n3.16.2 Inspecting and pruning the tree\nThe complexity parameter (cp) controls how aggressively the tree grows. As before, we inspect the cost-complexity table to identify the optimal pruning point via the 1-SE rule.\n\n# Examine complexity parameter (cost-complexity table)\n\nprintcp(ctree_cls)\n\n\nClassification tree:\nrpart(formula = high_response ~ ., data = train_cls, method = \"class\", \n    control = rpart.control(cp = 0.001, maxdepth = 8, minsplit = 30))\n\nVariables actually used in tree construction:\n [1] gene_05   gene_08   gene_1074 gene_112  gene_14   gene_1654 gene_1721\n [8] gene_19   gene_1982 gene_1986 gene_447  gene_567  gene_694  gene_726 \n[15] gene_78  \n\nRoot node error: 2637/7000 = 0.37671\n\nn= 7000 \n\n          CP nsplit rel error   xerror      xstd\n1  0.8862344      0  1.000000 1.000000 0.0153741\n2  0.0125142      1  0.113766 0.124384 0.0067051\n3  0.0113766      2  0.101251 0.117937 0.0065373\n4  0.0068259      3  0.089875 0.101631 0.0060881\n5  0.0026545      5  0.076223 0.089875 0.0057383\n6  0.0024649      6  0.073568 0.091392 0.0057848\n7  0.0022753      8  0.068639 0.090633 0.0057616\n8  0.0020857     11  0.061813 0.088737 0.0057031\n9  0.0018961     14  0.054608 0.089496 0.0057266\n10 0.0016433     15  0.052711 0.089116 0.0057149\n11 0.0011377     18  0.047782 0.092909 0.0058309\n12 0.0010000     20  0.045506 0.096322 0.0059331\n\nplotcp(ctree_cls, main = \"Classification tree: CP plot\")\n\n\n\n\n\n\n\n# Apply 1-SE rule for pruning\n\nbest_row  &lt;- which.min(ctree_cls$cptable[, \"xerror\"])\nxerr_min  &lt;- ctree_cls$cptable[best_row, \"xerror\"]\nxstd_min  &lt;- ctree_cls$cptable[best_row, \"xstd\"]\ncp_1se    &lt;- ctree_cls$cptable[ctree_cls$cptable[, \"xerror\"] &lt;= xerr_min + xstd_min, \"CP\"][1]\nctree_pruned &lt;- prune(ctree_cls, cp = cp_1se)\n\n\n\n3.16.3 Visualizing grown and pruned trees\n\nrpart.plot(ctree_cls, type = 2, extra = 104, box.palette = \"Blues\",\nmain = \"Classification Tree (grown)\")\n\n\n\n\n\n\n\nrpart.plot(ctree_pruned, type = 2, extra = 104, box.palette = \"GnBu\",\nmain = \"Classification Tree (pruned, 1-SE rule)\")\n\n\n\n\n\n\n\n\nEach internal node displays the variable and threshold that best separates responders from non-responders. Each leaf (terminal node) reports the predicted class, the probability of high_response, and the number of patients in that subgroup.\nFor example, a path like:If gene_14 ≥ 0.98 and gene_08 ≥ 2.03 → predict High response (p = 0.93)\nshows that patients with high expression of those two genes have about a 93 % probability of meaningful tumor shrinkage.\nIn regression trees, we minimized the within-node variance.\n\nIn classification trees, we minimize the impurity of the node, typically measured by the Gini index:\n\\[\nI_{\\text {Gini }}(S)=1-\\sum_{k=1}^K p_k^2\n\\]\nwhere \\(p_k\\) is the proportion of observations in class \\(k\\) within node \\(S\\).\nA node is pure (impurity \\(=0\\) ) if all observations belong to the same class, and most impure (maximum) when classes are evenly mixed.\nEach split is chosen to maximize impurity reduction:\n\\[\n\\Delta I=I(\\text { parent })-\\frac{n_L}{n_{\\text {parent }}} I(\\text { left })-\\frac{n_R}{n_{\\text {parent }}} I(\\text { right })\n\\]\nrpart() performs this optimization automatically, evaluating all candidate thresholds for numeric variables and the best grouping for categorical ones.\n\n\n3.16.4 Predictions and confusing matrix\nOnce the pruned tree is finalized, we can generate predictions and evaluate its performance on the test set.\n\n# Predictions on test data\n\npred_prob &lt;- predict(ctree_pruned, newdata = test_cls, type = \"prob\")[, 2]\npred_class &lt;- as.integer(pred_prob &gt;= 0.5)\n\n# True labels\n\ny_true &lt;- test_cls$high_response\n\n# Confusion matrix\n\ncm &lt;- table(Predicted = pred_class, True = y_true)\ncm\n\n         True\nPredicted    0    1\n        0 1845   38\n        1   43 1074\n\n# Accuracy, sensitivity, specificity\n\naccuracy  &lt;- sum(diag(cm)) / sum(cm)\nsensitivity &lt;- cm[\"1\", \"1\"] / sum(cm[, \"1\"])  # recall\nspecificity &lt;- cm[\"0\", \"0\"] / sum(cm[, \"0\"])\nc(Accuracy = accuracy, Sensitivity = sensitivity, Specificity = specificity)\n\n   Accuracy Sensitivity Specificity \n  0.9730000   0.9658273   0.9772246 \n\n\n\n\n3.16.5 ROC and AUC for the classification tree\nThe ROC curve visualizes the trade-off between sensitivity and specificity as we vary the decision threshold on the predicted probability.\n\nlibrary(pROC)\nroc_tree &lt;- roc(y_true, pred_prob)\nplot(roc_tree, col = \"#2b8cbe\", lwd = 2,\nmain = sprintf(\"ROC curve (AUC = %.3f)\", auc(roc_tree)))\nabline(a = 0, b = 1, lty = 2, col = \"gray\")\n\n\n\n\n\n\n\n\nAn AUC close to 1 indicates strong discrimination between high and low responders.\n\nIn clinical prediction tasks, AUC values between 0.8 and 0.9 are considered good, and above 0.9 excellent.\n\n\n3.16.6 Interpreting the classification trees\nThe grown tree explores all possible splits that reduce impurity, achieving near-perfect fit on training data but risking overfitting.\n\nThe pruned tree, selected by the 1-SE rule, retains only the most stable, clinically interpretable decision rules.\nEach path from root to leaf forms an explicit “if–then” rule linking biological and clinical features to treatment response.\n\nThis interpretability is valuable in medicine: unlike logistic regression coefficients, tree rules are easily read and discussed with clinicians.\nAlthough a single classification tree is highly interpretable, it has important limitations:\nA first limitation is high variance. Small changes in the data may lead the tree to choose very different splitting variables, thresholds, and rule structures. The model therefore tends to overfit training data and generalize poorly to unseen patients.\nA second limitation is that the tree can capture only one hierarchy of interactions at a time. If multiple patterns or gene combinations can predict response, the tree must choose between them, discarding alternative useful pathways.\nA third limitation is instability in high-dimensional settings. In our dataset of more than 2,000 gene expression variables, many predictors are only weakly informative. The greedy splitting process may select noise variables simply because they provide small, random decreases in impurity. For these reasons, while a pruned tree is clinically interpretable, it rarely matches the predictive performance of modern ensemble methods.\n\n\n3.16.7 Ensemble methods for classification: Random Forest and XGBoost\nIn the previous section, we fitted a single classification tree to predict the probability of high response (high_response). Although interpretable, single trees tend to have high variance they fit training data too well and generalize poorly. Two powerful ensemble methods Random Forest and XGBoost can dramatically improve performance by combining the predictions of many trees.\n\n\n3.16.8 Random Forest classifier\nRandom Forest (RF) builds many trees, each trained on a bootstrap sample of the training data and a random subset of predictors at each split. Each tree votes for the predicted class, and the forest aggregates these votes.\n\nlibrary(ranger)\nlibrary(dplyr)\n\n# Make sure the outcome is a factor with levels 0 and 1\ntrain_cls &lt;- train_cls %&gt;% mutate(high_response = factor(high_response, levels = c(0, 1)))\ntest_cls  &lt;- test_cls  %&gt;% mutate(high_response = factor(high_response, levels = c(0, 1)))\n\nset.seed(123)\n\n# Fit the random forest classifier\nrf_class &lt;- ranger(\n  high_response ~ .,\n  data = train_cls,\n  num.trees = 800,\n  mtry = floor(sqrt(ncol(train_cls) - 1)),\n  min.node.size = 5,\n  importance = \"impurity\",\n  probability = TRUE,      # ensures probability predictions\n  oob.error = TRUE\n)\n\nrf_class\n\nRanger result\n\nCall:\n ranger(high_response ~ ., data = train_cls, num.trees = 800,      mtry = floor(sqrt(ncol(train_cls) - 1)), min.node.size = 5,      importance = \"impurity\", probability = TRUE, oob.error = TRUE) \n\nType:                             Probability estimation \nNumber of trees:                  800 \nSample size:                      7000 \nNumber of independent variables:  2005 \nMtry:                             44 \nTarget node size:                 5 \nVariable importance mode:         impurity \nSplitrule:                        gini \nOOB prediction error (Brier s.):  0.04554791 \n\n# OOB error (classification): use sqrt only for regression. For classification:\nrf_class$prediction.error   # OOB misclassification error\n\n[1] 0.04554791\n\n# Predict on test data\npred_obj &lt;- predict(rf_class, data = test_cls)\n\n# Examine the structure if curious:\n# str(pred_obj$predictions)\n\n# Extract probability for class \"1\"\n# If the column has names \"0\" and \"1\", use them:\npred_rf_prob &lt;- pred_obj$predictions[, \"1\"]\n\n# Convert probabilities into class predictions\npred_rf_class &lt;- ifelse(pred_rf_prob &gt;= 0.5, 1, 0)\n\n# Confusion matrix\ncm_rf &lt;- table(\n  Predicted = pred_rf_class,\n  True = test_cls$high_response\n)\n\ncm_rf\n\n         True\nPredicted    0    1\n        0 1853   56\n        1   35 1056\n\n\nRandom Forest performs well in many clinical applications because it averages hundreds of decorrelated trees, reducing the variance inherent in single-tree models. The bootstrap sampling at the tree level and the random subset of predictors at each split ensure that individual trees explore different parts of the feature space.\nHowever, in ultra–high-dimensional settings such as our chemotherapy trial, several challenges emerge:\nFirst, when the number of predictors is extremely large (more than 2,000 gene features), the chance of selecting truly informative predictors at each split becomes small. The majority of splits may involve irrelevant variables, weakening each individual tree.\nSecond, if the data contain many weak or noisy predictors, Random Forest tends to produce noisy decision boundaries. Averaging many weak learners helps, but does not completely overcome this issue.\nThird, even though Random Forest reduces variance, it does not reduce bias. When the signal requires subtle, multi-variable interactions, shallow random subtrees may not capture those interactions efficiently.\nNevertheless, in our classification task the Random Forest performs remarkably well, achieving an AUC close to 0.996 largely because the signal-to-noise ratio for the binary target is far higher than for the continuous regression outcome studied earlier.\n\n\n3.16.9 XGboost for classification\nRandom forests reduce variance by averaging many decorrelated trees, but they do not directly address model bias. Boosting methods particularly gradient boosting take the opposite strategy: build many trees sequentially, where each new tree tries to correct the mistakes (the residuals) of the ensemble so far.\nXGBoost (Extreme Gradient Boosting) is the most widely used implementation of gradient boosting for tabular data. It fits many shallow trees, each one focusing on the patterns the previous trees failed to capture. The final model is a weighted sum of these trees, typically yielding excellent predictive performance.\nFor binary classification (our high_response task), XGBoost builds trees that predict the log-odds of response and uses a differentiable loss function (usually logistic loss) to guide the sequence of improvements.\nBoosting follows this conceptual sequence:\n\nStart with a simple model, usually predicting the average log-odds of class 1 .\nCompute residuals, which for classification are the gradients of logistic loss.\nGrow a small tree that best predicts these residuals.\nAdd the tree to the model, scaled by a “learning rate”.\nRepeat hundreds of times, gradually refining the model.\n\nThe trees are intentionally shallow ( \\(2-6\\) splits) so that each one captures only simple interactions; the strength comes from the accumulation of many small improvements.\n\n3.16.9.1 Preparing the data to run XGboost\nXGBoost requires: - the outcome encoded as 0/1 numeric - predictors in a numeric matrix (no factors)\nWe therefore recode the data consistently.\n\n# Convert outcome to numeric 0/1\ntrain_xgb &lt;- train_cls %&gt;%\n  mutate(high_response = as.numeric(as.character(high_response)))\n\ntest_xgb &lt;- test_cls %&gt;%\n  mutate(high_response = as.numeric(as.character(high_response)))\n\n# Convert predictors to a numeric matrix\ny_train &lt;- train_xgb$high_response\nX_train &lt;- model.matrix(high_response ~ . - 1, data = train_xgb)\n\ny_test  &lt;- test_xgb$high_response\nX_test  &lt;- model.matrix(high_response ~ . - 1, data = test_xgb)\n\n\n\n\n3.16.10 Fitting XGboost\n\nlibrary(xgboost)\n\nset.seed(123)\n\nxgb_fit &lt;- xgboost(\n  data = X_train,\n  label = y_train,\n  objective = \"binary:logistic\",\n  nrounds = 150,\n  eta = 0.1,             # learning rate\n  max_depth = 4,\n  min_child_weight = 3,\n  subsample = 0.8,\n  colsample_bytree = 0.6,\n  eval_metric = \"logloss\",\n  verbose = 0\n)\n\n\n# Predict probabilities on the test set\npred_xgb_prob &lt;- predict(xgb_fit, newdata = X_test)\n\n# Convert to classes\npred_xgb_class &lt;- ifelse(pred_xgb_prob &gt;= 0.5, 1, 0)\n\n# Confusion matrix\ncm_xgb &lt;- table(\n  Predicted = pred_xgb_class,\n  True = y_test\n)\ncm_xgb\n\n         True\nPredicted    0    1\n        0 1852   27\n        1   36 1085\n\n\n\n\n3.16.11 Prediction and Evaluation\n\n# Predict probabilities on the test set\npred_xgb_prob &lt;- predict(xgb_fit, newdata = X_test)\n\n# Convert to classes\npred_xgb_class &lt;- ifelse(pred_xgb_prob &gt;= 0.5, 1, 0)\n\n# Confusion matrix\ncm_xgb &lt;- table(\n  Predicted = pred_xgb_class,\n  True = y_test\n)\ncm_xgb\n\n         True\nPredicted    0    1\n        0 1852   27\n        1   36 1085\n\n\n\nlibrary(pROC)\nauc_xgb &lt;- roc(y_test, pred_xgb_prob)$auc\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nauc_xgb\n\nArea under the curve: 0.9982\n\n\nXGBoost consistently achieves state-of-the-art performance in tasks involving complex interactions and high dimensionality. Several characteristics make it particularly effective for clinical biomarker modelling:\nThe first characteristic is the use of gradient boosting, where each tree is trained to correct the errors of the previous ensemble. This iterative refinement allows the model to approximate highly nonlinear decision boundaries.\nA second strength is regularization, both L1 (sparsity) and L2 (shrinkage), which prevents overfitting even when thousands of predictors are available.\nA third advantage is column and row subsampling, which improves generalization and stabilizes tree structure in datasets dominated by noisy features.\nIn practice, XGBoost reliably identifies subtle combinations of gene expression patterns that predict tumour response. In our benchmark, it achieves the highest AUC (≈0.998), outperforming both Random Forest and all linear models.\n\n\n3.16.12 Re-fitting the logistic regression model\n\n# Ensure high_response is a factor with correct levels\ntrain_cls &lt;- train_cls %&gt;% \n  mutate(high_response = factor(high_response, levels = c(0,1)))\n\ntest_cls &lt;- test_cls %&gt;% \n  mutate(high_response = factor(high_response, levels = c(0,1)))\n\n# Fit logistic regression using all predictors\nglm_logit &lt;- glm(\n  high_response ~ .,\n  data = train_cls,\n  family = binomial(link = \"logit\")\n)\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(glm_logit)\n\n\nCall:\nglm(formula = high_response ~ ., family = binomial(link = \"logit\"), \n    data = train_cls)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)       -4.621e+02  2.773e+06       0        1\ntreatmentchemo    -8.278e+00  1.878e+05       0        1\ndose_intensity     6.959e+00  1.766e+05       0        1\npatient_age       -2.166e-02  8.222e+02       0        1\ntumor_gradeG2     -7.342e-01  3.083e+04       0        1\ntumor_gradeG3      2.364e+00  3.768e+04       0        1\nperformance_score -1.027e+00  2.072e+04       0        1\ngene_01           -2.212e+00  4.662e+04       0        1\ngene_02           -7.427e-01  4.241e+04       0        1\ngene_03           -3.158e+00  4.725e+04       0        1\ngene_04            9.486e-01  4.378e+04       0        1\ngene_05           -4.389e+00  7.996e+04       0        1\ngene_06           -1.842e+00  4.131e+04       0        1\ngene_07            1.930e+00  3.861e+04       0        1\ngene_08            2.282e+01  6.418e+04       0        1\ngene_09            9.822e-01  4.462e+04       0        1\ngene_10            1.280e+00  4.667e+04       0        1\ngene_11            2.077e+00  3.995e+04       0        1\ngene_12           -2.437e+00  4.802e+04       0        1\ngene_13            2.341e+00  4.225e+04       0        1\ngene_14            2.053e+01  5.508e+04       0        1\ngene_15            9.468e-01  4.515e+04       0        1\ngene_16            6.128e+00  4.592e+04       0        1\ngene_17            7.882e-01  4.680e+04       0        1\ngene_18            4.287e-01  4.268e+04       0        1\ngene_19            1.653e+01  5.858e+04       0        1\ngene_20            2.324e+00  4.520e+04       0        1\ngene_21           -1.283e+00  4.395e+04       0        1\ngene_22           -5.782e-01  4.702e+04       0        1\ngene_23           -1.581e+00  4.540e+04       0        1\ngene_24            1.716e+00  4.456e+04       0        1\ngene_25           -4.102e-01  4.384e+04       0        1\ngene_26           -2.000e+00  4.913e+04       0        1\ngene_27            3.614e+00  4.093e+04       0        1\ngene_28            6.128e-01  4.493e+04       0        1\ngene_29            8.504e-01  4.281e+04       0        1\ngene_30           -2.530e+00  4.641e+04       0        1\ngene_31           -3.446e+00  4.469e+04       0        1\ngene_32           -2.765e-01  4.748e+04       0        1\ngene_33           -3.891e+00  4.253e+04       0        1\ngene_34           -3.956e+00  4.809e+04       0        1\ngene_35           -2.129e+00  5.121e+04       0        1\ngene_36           -9.715e-01  4.366e+04       0        1\ngene_37           -1.589e+00  4.420e+04       0        1\ngene_38            5.142e+00  3.811e+04       0        1\ngene_39            2.661e-01  4.945e+04       0        1\ngene_40           -1.244e+00  4.491e+04       0        1\ngene_41           -2.145e+00  5.060e+04       0        1\ngene_42            4.458e+00  4.953e+04       0        1\ngene_43           -3.837e+00  4.729e+04       0        1\ngene_44            2.744e+00  4.553e+04       0        1\ngene_45           -1.568e+00  4.570e+04       0        1\ngene_46            3.832e+00  4.357e+04       0        1\ngene_47           -2.475e+00  4.548e+04       0        1\ngene_48            2.453e-01  4.128e+04       0        1\ngene_49           -2.177e+00  4.477e+04       0        1\ngene_50           -6.271e+00  4.365e+04       0        1\ngene_51           -1.659e+00  4.482e+04       0        1\ngene_52            3.285e+00  4.935e+04       0        1\ngene_53           -1.122e+00  4.460e+04       0        1\ngene_54           -6.524e-01  4.474e+04       0        1\ngene_55           -2.202e+00  4.442e+04       0        1\ngene_56            3.408e+00  4.526e+04       0        1\ngene_57            2.738e+00  4.440e+04       0        1\ngene_58            2.583e+00  4.773e+04       0        1\ngene_59            6.975e+00  5.021e+04       0        1\ngene_60            1.172e+00  4.405e+04       0        1\ngene_61           -9.131e-02  4.524e+04       0        1\ngene_62            2.406e+00  4.190e+04       0        1\ngene_63            3.997e-01  5.010e+04       0        1\ngene_64           -1.135e+00  4.828e+04       0        1\ngene_65            4.063e+00  4.414e+04       0        1\ngene_66           -1.088e+00  4.680e+04       0        1\ngene_67           -2.286e+00  4.265e+04       0        1\ngene_68            1.654e+00  4.198e+04       0        1\ngene_69            5.896e+00  5.183e+04       0        1\ngene_70            5.891e+00  4.547e+04       0        1\ngene_71            2.611e-01  5.184e+04       0        1\ngene_72            2.922e+00  4.375e+04       0        1\ngene_73           -1.924e+00  4.488e+04       0        1\ngene_74            1.038e+00  4.410e+04       0        1\ngene_75           -2.215e+00  4.109e+04       0        1\ngene_76            4.540e+00  4.780e+04       0        1\ngene_77           -3.158e+00  4.802e+04       0        1\ngene_78           -3.506e+00  4.589e+04       0        1\ngene_79           -2.927e+00  4.191e+04       0        1\ngene_80            2.597e+00  4.655e+04       0        1\ngene_81           -1.492e+00  4.337e+04       0        1\ngene_82           -1.498e+00  4.254e+04       0        1\ngene_83            5.030e+00  4.701e+04       0        1\ngene_84           -4.174e+00  4.740e+04       0        1\ngene_85            1.376e+00  5.021e+04       0        1\ngene_86           -2.173e+00  4.203e+04       0        1\ngene_87            6.469e-01  4.557e+04       0        1\ngene_88            4.718e+00  4.305e+04       0        1\ngene_89           -7.473e-01  4.108e+04       0        1\ngene_90            4.478e+00  4.646e+04       0        1\ngene_91            6.719e-01  4.189e+04       0        1\ngene_92           -1.357e+00  3.860e+04       0        1\ngene_93           -2.028e+00  4.716e+04       0        1\ngene_94            4.859e-01  4.491e+04       0        1\ngene_95           -5.528e+00  4.313e+04       0        1\ngene_96           -1.793e+00  4.081e+04       0        1\ngene_97            3.023e+00  4.402e+04       0        1\ngene_98            1.444e+00  4.280e+04       0        1\ngene_99           -1.882e+00  4.675e+04       0        1\ngene_100           1.193e+00  4.722e+04       0        1\ngene_101          -1.037e+00  4.967e+04       0        1\ngene_102           2.372e-01  5.024e+04       0        1\ngene_103          -1.412e+00  4.609e+04       0        1\ngene_104          -4.912e-01  4.202e+04       0        1\ngene_105           1.137e+00  4.385e+04       0        1\ngene_106           4.062e+00  4.768e+04       0        1\ngene_107          -7.803e-01  4.720e+04       0        1\ngene_108          -9.054e-01  4.635e+04       0        1\ngene_109           3.636e-02  4.771e+04       0        1\ngene_110           4.039e+00  4.948e+04       0        1\ngene_111          -3.650e-01  4.800e+04       0        1\ngene_112          -5.082e-01  4.727e+04       0        1\ngene_113           3.804e+00  4.764e+04       0        1\ngene_114           7.492e-01  4.832e+04       0        1\ngene_115           2.187e+00  4.164e+04       0        1\ngene_116           1.498e+00  4.494e+04       0        1\ngene_117          -3.397e+00  4.496e+04       0        1\ngene_118          -4.769e+00  4.317e+04       0        1\ngene_119          -5.671e+00  4.560e+04       0        1\ngene_120           2.918e+00  4.405e+04       0        1\ngene_121          -7.076e-01  4.937e+04       0        1\ngene_122           1.587e+00  5.137e+04       0        1\ngene_123           1.183e+00  4.403e+04       0        1\ngene_124           4.967e-01  4.565e+04       0        1\ngene_125           4.209e-01  4.616e+04       0        1\ngene_126          -4.480e+00  4.030e+04       0        1\ngene_127           3.821e-01  4.558e+04       0        1\ngene_128           5.950e-01  4.174e+04       0        1\ngene_129           8.982e-01  4.744e+04       0        1\ngene_130           6.619e+00  4.112e+04       0        1\ngene_131           4.464e+00  4.339e+04       0        1\ngene_132          -1.777e+00  4.740e+04       0        1\ngene_133          -1.909e+00  4.621e+04       0        1\ngene_134           1.323e+00  5.198e+04       0        1\ngene_135           4.360e-01  4.512e+04       0        1\ngene_136          -2.448e-01  4.373e+04       0        1\ngene_137           3.273e+00  4.782e+04       0        1\ngene_138          -8.534e-01  4.348e+04       0        1\ngene_139           1.737e+00  4.482e+04       0        1\ngene_140           5.835e-01  4.177e+04       0        1\ngene_141           3.470e-01  4.834e+04       0        1\ngene_142          -1.937e+00  4.312e+04       0        1\ngene_143          -1.651e+00  4.304e+04       0        1\ngene_144           4.570e+00  4.325e+04       0        1\ngene_145           3.153e+00  4.067e+04       0        1\ngene_146           5.275e+00  4.348e+04       0        1\ngene_147          -1.483e+00  4.363e+04       0        1\ngene_148          -2.958e-02  4.225e+04       0        1\ngene_149          -3.688e+00  4.154e+04       0        1\ngene_150           9.275e-01  4.784e+04       0        1\ngene_151          -5.656e-01  4.404e+04       0        1\ngene_152           2.940e+00  4.794e+04       0        1\ngene_153          -3.666e+00  4.248e+04       0        1\ngene_154           1.317e+00  4.763e+04       0        1\ngene_155          -2.161e+00  4.824e+04       0        1\ngene_156           3.332e-01  4.345e+04       0        1\ngene_157           3.172e-01  4.478e+04       0        1\ngene_158           2.478e+00  4.561e+04       0        1\ngene_159           4.667e-01  4.394e+04       0        1\ngene_160          -6.058e-02  4.095e+04       0        1\ngene_161          -2.649e+00  4.911e+04       0        1\ngene_162          -2.890e+00  4.458e+04       0        1\ngene_163          -7.251e+00  4.371e+04       0        1\ngene_164          -2.047e+00  4.171e+04       0        1\ngene_165          -5.061e+00  4.354e+04       0        1\ngene_166           7.553e-01  4.502e+04       0        1\ngene_167          -2.852e-02  4.653e+04       0        1\ngene_168          -2.234e+00  4.414e+04       0        1\ngene_169           2.251e+00  4.216e+04       0        1\ngene_170          -2.140e+00  4.171e+04       0        1\ngene_171          -4.561e+00  4.652e+04       0        1\ngene_172           3.325e-01  4.530e+04       0        1\ngene_173          -2.772e+00  4.871e+04       0        1\ngene_174           5.240e+00  4.474e+04       0        1\ngene_175          -6.821e-01  4.325e+04       0        1\ngene_176          -2.296e+00  4.745e+04       0        1\ngene_177          -1.768e+00  4.990e+04       0        1\ngene_178          -1.185e+00  4.235e+04       0        1\ngene_179          -1.682e-01  4.601e+04       0        1\ngene_180           5.008e-01  4.357e+04       0        1\ngene_181          -2.323e+00  4.555e+04       0        1\ngene_182          -2.141e-01  4.417e+04       0        1\ngene_183          -6.508e+00  4.843e+04       0        1\ngene_184          -3.876e-01  5.237e+04       0        1\ngene_185          -4.831e+00  5.099e+04       0        1\ngene_186           1.607e+00  4.597e+04       0        1\ngene_187           1.196e+00  3.984e+04       0        1\ngene_188          -9.955e-01  4.216e+04       0        1\ngene_189          -1.895e-01  4.066e+04       0        1\ngene_190          -2.020e+00  4.303e+04       0        1\ngene_191          -3.128e-01  4.382e+04       0        1\ngene_192          -1.828e-01  4.421e+04       0        1\ngene_193          -2.463e+00  4.313e+04       0        1\ngene_194          -3.139e+00  4.694e+04       0        1\ngene_195           1.669e+00  4.536e+04       0        1\ngene_196           1.327e+00  4.274e+04       0        1\ngene_197           7.651e-01  4.379e+04       0        1\ngene_198          -9.987e-01  3.909e+04       0        1\ngene_199           1.432e+00  4.699e+04       0        1\ngene_200           1.708e+00  4.330e+04       0        1\ngene_201           1.432e+00  4.088e+04       0        1\ngene_202          -1.752e-01  4.601e+04       0        1\ngene_203           2.493e-01  4.802e+04       0        1\ngene_204           2.382e-01  4.575e+04       0        1\ngene_205           4.263e+00  4.812e+04       0        1\ngene_206           3.454e+00  5.045e+04       0        1\ngene_207           2.120e+00  4.273e+04       0        1\ngene_208          -2.129e+00  4.805e+04       0        1\ngene_209          -1.790e+00  4.308e+04       0        1\ngene_210           8.272e-01  4.554e+04       0        1\ngene_211           6.137e-01  4.337e+04       0        1\ngene_212          -5.409e+00  4.810e+04       0        1\ngene_213          -1.533e-01  4.505e+04       0        1\ngene_214          -1.951e+00  4.556e+04       0        1\ngene_215           2.366e-01  4.497e+04       0        1\ngene_216           2.184e+00  4.512e+04       0        1\ngene_217           1.044e+00  5.006e+04       0        1\ngene_218          -4.528e+00  4.084e+04       0        1\ngene_219           1.097e+00  4.869e+04       0        1\ngene_220          -3.870e+00  5.055e+04       0        1\ngene_221           3.625e+00  4.455e+04       0        1\ngene_222           1.383e+00  4.559e+04       0        1\ngene_223          -1.465e+00  4.229e+04       0        1\ngene_224          -1.179e-01  4.149e+04       0        1\ngene_225           3.002e+00  4.608e+04       0        1\ngene_226           2.169e+00  4.702e+04       0        1\ngene_227          -1.582e+00  4.787e+04       0        1\ngene_228          -5.287e-01  4.381e+04       0        1\ngene_229           1.781e+00  4.702e+04       0        1\ngene_230           2.701e+00  4.665e+04       0        1\ngene_231           3.633e+00  4.633e+04       0        1\ngene_232           1.938e+00  4.614e+04       0        1\ngene_233          -2.560e-01  4.556e+04       0        1\ngene_234           2.724e+00  4.349e+04       0        1\ngene_235          -3.001e+00  4.297e+04       0        1\ngene_236           2.106e+00  4.579e+04       0        1\ngene_237           3.547e+00  4.587e+04       0        1\ngene_238          -1.336e+00  4.541e+04       0        1\ngene_239           1.106e+00  4.266e+04       0        1\ngene_240          -3.480e+00  4.834e+04       0        1\ngene_241          -2.709e+00  4.536e+04       0        1\ngene_242           4.642e+00  4.548e+04       0        1\ngene_243           2.182e+00  4.801e+04       0        1\ngene_244          -1.936e+00  5.015e+04       0        1\ngene_245          -3.615e+00  4.716e+04       0        1\ngene_246           1.442e+00  4.277e+04       0        1\ngene_247          -9.744e-01  4.986e+04       0        1\ngene_248           9.580e-01  4.321e+04       0        1\ngene_249          -2.059e+00  4.610e+04       0        1\ngene_250           1.594e-01  4.910e+04       0        1\ngene_251          -1.352e+00  4.986e+04       0        1\ngene_252           3.080e-01  4.598e+04       0        1\ngene_253          -3.477e+00  4.465e+04       0        1\ngene_254          -1.354e-01  4.761e+04       0        1\ngene_255           5.796e-01  4.022e+04       0        1\ngene_256           2.342e-01  4.579e+04       0        1\ngene_257           9.246e-01  4.313e+04       0        1\ngene_258          -7.595e-01  4.219e+04       0        1\ngene_259           4.144e-01  4.053e+04       0        1\ngene_260           4.332e-01  5.213e+04       0        1\ngene_261           2.518e-01  4.748e+04       0        1\ngene_262           2.832e-01  4.334e+04       0        1\ngene_263          -1.215e+00  4.927e+04       0        1\ngene_264          -6.575e-01  4.671e+04       0        1\ngene_265           3.110e+00  4.458e+04       0        1\ngene_266          -7.142e-02  4.160e+04       0        1\ngene_267          -6.993e+00  3.863e+04       0        1\ngene_268          -2.294e+00  5.181e+04       0        1\ngene_269          -4.704e-01  4.201e+04       0        1\ngene_270           2.413e+00  4.814e+04       0        1\ngene_271          -2.261e+00  4.779e+04       0        1\ngene_272           2.378e+00  4.643e+04       0        1\ngene_273           5.864e+00  4.511e+04       0        1\ngene_274          -2.986e-02  4.328e+04       0        1\ngene_275          -1.085e+00  4.642e+04       0        1\ngene_276           1.287e+00  4.484e+04       0        1\ngene_277          -3.019e+00  4.954e+04       0        1\ngene_278           1.745e+00  4.149e+04       0        1\ngene_279           2.216e+00  4.425e+04       0        1\ngene_280          -4.258e+00  4.570e+04       0        1\ngene_281          -2.161e+00  4.899e+04       0        1\ngene_282           4.481e+00  4.267e+04       0        1\ngene_283           2.118e+00  4.813e+04       0        1\ngene_284           5.219e-01  4.631e+04       0        1\ngene_285          -7.065e-01  4.359e+04       0        1\ngene_286          -3.430e+00  4.428e+04       0        1\ngene_287          -3.563e+00  4.240e+04       0        1\ngene_288          -1.568e+00  4.526e+04       0        1\ngene_289           6.221e-01  4.600e+04       0        1\ngene_290           6.298e-01  4.453e+04       0        1\ngene_291           5.415e+00  4.422e+04       0        1\ngene_292           1.869e+00  4.645e+04       0        1\ngene_293          -3.148e+00  4.636e+04       0        1\ngene_294          -1.668e+00  4.147e+04       0        1\ngene_295           3.672e-01  4.436e+04       0        1\ngene_296          -8.683e-01  4.406e+04       0        1\ngene_297           3.314e-01  4.833e+04       0        1\ngene_298          -1.275e+00  4.728e+04       0        1\ngene_299           6.062e+00  4.491e+04       0        1\ngene_300           2.598e+00  4.520e+04       0        1\ngene_301           1.850e+00  4.521e+04       0        1\ngene_302          -3.570e+00  4.717e+04       0        1\ngene_303          -6.820e+00  4.307e+04       0        1\ngene_304          -2.797e+00  4.643e+04       0        1\ngene_305          -5.872e+00  4.913e+04       0        1\ngene_306          -2.113e+00  4.658e+04       0        1\ngene_307           2.420e+00  4.306e+04       0        1\ngene_308          -2.822e+00  4.468e+04       0        1\ngene_309          -3.103e+00  4.224e+04       0        1\ngene_310           2.030e+00  4.563e+04       0        1\ngene_311           5.971e+00  4.588e+04       0        1\ngene_312          -5.964e-01  4.499e+04       0        1\ngene_313          -4.573e-01  4.548e+04       0        1\ngene_314          -4.406e+00  4.631e+04       0        1\ngene_315           3.110e+00  4.531e+04       0        1\ngene_316           4.651e+00  4.789e+04       0        1\ngene_317          -1.195e+00  4.345e+04       0        1\ngene_318          -2.066e+00  4.442e+04       0        1\ngene_319           3.228e+00  4.853e+04       0        1\ngene_320          -4.109e-01  4.335e+04       0        1\ngene_321          -5.503e-01  3.950e+04       0        1\ngene_322          -1.539e+00  4.845e+04       0        1\ngene_323          -1.307e+00  4.024e+04       0        1\ngene_324           2.196e+00  4.166e+04       0        1\ngene_325           3.813e+00  4.612e+04       0        1\ngene_326          -9.091e-01  4.297e+04       0        1\ngene_327          -1.330e+00  4.539e+04       0        1\ngene_328           3.412e+00  4.788e+04       0        1\ngene_329           1.019e+00  4.519e+04       0        1\ngene_330          -3.148e+00  4.569e+04       0        1\ngene_331          -2.178e+00  4.514e+04       0        1\ngene_332           2.923e-01  4.247e+04       0        1\ngene_333          -5.608e+00  4.480e+04       0        1\ngene_334           2.087e+00  4.479e+04       0        1\ngene_335          -3.092e+00  4.185e+04       0        1\ngene_336           3.317e+00  4.647e+04       0        1\ngene_337           5.799e+00  4.624e+04       0        1\ngene_338           5.077e+00  4.460e+04       0        1\ngene_339          -1.906e+00  4.311e+04       0        1\ngene_340          -8.799e-02  4.096e+04       0        1\ngene_341           7.554e-01  4.398e+04       0        1\ngene_342          -1.246e+00  4.725e+04       0        1\ngene_343          -6.766e-01  4.512e+04       0        1\ngene_344          -4.900e+00  4.225e+04       0        1\ngene_345          -3.186e+00  4.111e+04       0        1\ngene_346          -2.108e+00  4.198e+04       0        1\ngene_347          -6.956e-01  4.782e+04       0        1\ngene_348          -3.225e-01  4.455e+04       0        1\ngene_349          -1.654e+00  4.371e+04       0        1\ngene_350          -8.766e-01  4.164e+04       0        1\ngene_351          -4.415e+00  4.052e+04       0        1\ngene_352           9.497e-01  4.551e+04       0        1\ngene_353          -3.802e+00  4.503e+04       0        1\ngene_354          -3.836e-01  4.143e+04       0        1\ngene_355          -2.089e-01  4.648e+04       0        1\ngene_356          -4.737e+00  4.749e+04       0        1\ngene_357          -3.869e+00  4.330e+04       0        1\ngene_358          -6.505e-01  4.738e+04       0        1\ngene_359          -1.538e+00  4.789e+04       0        1\ngene_360           3.088e+00  4.954e+04       0        1\ngene_361           2.085e+00  4.709e+04       0        1\ngene_362           1.933e-01  4.924e+04       0        1\ngene_363          -2.374e+00  4.234e+04       0        1\ngene_364          -1.360e+00  4.334e+04       0        1\ngene_365           2.175e+00  4.642e+04       0        1\ngene_366           3.988e+00  4.506e+04       0        1\ngene_367          -5.516e-01  4.329e+04       0        1\ngene_368          -2.202e-01  4.763e+04       0        1\ngene_369          -3.600e-01  5.285e+04       0        1\ngene_370           2.094e+00  5.000e+04       0        1\ngene_371           1.891e-01  4.153e+04       0        1\ngene_372           1.564e-01  4.573e+04       0        1\ngene_373          -1.258e+00  4.437e+04       0        1\ngene_374           2.449e+00  4.578e+04       0        1\ngene_375           3.106e+00  4.604e+04       0        1\ngene_376          -6.705e+00  4.637e+04       0        1\ngene_377           1.306e+00  4.189e+04       0        1\ngene_378           2.014e+00  4.590e+04       0        1\ngene_379          -6.571e+00  4.433e+04       0        1\ngene_380          -1.437e+00  4.554e+04       0        1\ngene_381           3.388e+00  4.162e+04       0        1\ngene_382           2.069e+00  5.106e+04       0        1\ngene_383           4.472e+00  4.447e+04       0        1\ngene_384           4.643e+00  4.493e+04       0        1\ngene_385          -5.686e+00  4.352e+04       0        1\ngene_386           1.405e+00  4.289e+04       0        1\ngene_387          -4.461e+00  4.700e+04       0        1\ngene_388          -1.708e+00  4.459e+04       0        1\ngene_389          -5.913e-01  4.419e+04       0        1\ngene_390           1.296e+00  4.860e+04       0        1\ngene_391          -4.617e+00  4.262e+04       0        1\ngene_392           1.383e+00  4.584e+04       0        1\ngene_393          -4.128e+00  4.375e+04       0        1\ngene_394          -1.505e+00  4.897e+04       0        1\ngene_395          -8.764e-01  4.479e+04       0        1\ngene_396          -5.389e+00  4.492e+04       0        1\ngene_397           4.210e+00  4.077e+04       0        1\ngene_398          -2.503e+00  4.586e+04       0        1\ngene_399          -3.358e+00  4.188e+04       0        1\ngene_400          -2.317e+00  4.373e+04       0        1\ngene_401          -2.786e+00  4.492e+04       0        1\ngene_402          -4.930e+00  4.549e+04       0        1\ngene_403           4.375e+00  4.384e+04       0        1\ngene_404           1.170e+00  4.287e+04       0        1\ngene_405           2.586e-01  4.963e+04       0        1\ngene_406          -1.642e+00  4.091e+04       0        1\ngene_407           2.658e-01  4.339e+04       0        1\ngene_408           3.152e-01  4.734e+04       0        1\ngene_409           9.770e-01  4.461e+04       0        1\ngene_410          -3.940e+00  4.737e+04       0        1\ngene_411          -4.174e+00  4.178e+04       0        1\ngene_412          -2.118e+00  4.058e+04       0        1\ngene_413           1.445e-01  4.729e+04       0        1\ngene_414           2.999e-01  4.603e+04       0        1\ngene_415           1.280e+00  4.448e+04       0        1\ngene_416           9.304e-04  4.692e+04       0        1\ngene_417          -8.362e-01  4.790e+04       0        1\ngene_418          -1.977e+00  4.802e+04       0        1\ngene_419          -4.002e-02  4.089e+04       0        1\ngene_420           2.897e+00  4.024e+04       0        1\ngene_421          -2.077e+00  4.120e+04       0        1\ngene_422          -3.559e+00  4.475e+04       0        1\ngene_423          -1.076e-02  4.296e+04       0        1\ngene_424           2.175e+00  4.358e+04       0        1\ngene_425           9.485e-01  4.532e+04       0        1\ngene_426          -4.263e+00  4.457e+04       0        1\ngene_427           3.443e+00  4.480e+04       0        1\ngene_428           1.873e+00  4.443e+04       0        1\ngene_429           3.211e+00  4.117e+04       0        1\ngene_430          -9.417e-01  4.550e+04       0        1\ngene_431          -1.805e+00  4.608e+04       0        1\ngene_432           3.512e+00  4.393e+04       0        1\ngene_433          -1.633e+00  4.340e+04       0        1\ngene_434           4.951e+00  4.258e+04       0        1\ngene_435          -1.160e+00  4.372e+04       0        1\ngene_436          -1.217e+00  4.676e+04       0        1\ngene_437           1.697e-02  4.401e+04       0        1\ngene_438          -7.155e-01  4.216e+04       0        1\ngene_439           2.581e+00  4.200e+04       0        1\ngene_440           4.170e-01  4.369e+04       0        1\ngene_441          -2.590e+00  4.497e+04       0        1\ngene_442           1.282e+00  4.747e+04       0        1\ngene_443           2.953e+00  4.608e+04       0        1\ngene_444           4.070e+00  4.476e+04       0        1\ngene_445           1.946e+00  4.561e+04       0        1\ngene_446          -4.309e+00  4.495e+04       0        1\ngene_447           6.913e+00  4.902e+04       0        1\ngene_448           4.915e+00  4.468e+04       0        1\ngene_449          -2.049e+00  4.626e+04       0        1\ngene_450           8.218e-02  4.183e+04       0        1\ngene_451           4.184e+00  4.245e+04       0        1\ngene_452          -4.858e+00  4.363e+04       0        1\ngene_453          -7.136e-02  4.529e+04       0        1\ngene_454           4.997e-01  4.727e+04       0        1\ngene_455          -2.759e+00  4.339e+04       0        1\ngene_456          -9.128e-02  4.445e+04       0        1\ngene_457          -1.384e-01  4.294e+04       0        1\ngene_458           2.198e+00  4.075e+04       0        1\ngene_459           6.027e+00  4.358e+04       0        1\ngene_460          -4.130e+00  4.455e+04       0        1\ngene_461          -1.087e+00  4.152e+04       0        1\ngene_462           1.639e+00  4.596e+04       0        1\ngene_463           2.876e-01  4.820e+04       0        1\ngene_464          -1.185e+00  4.812e+04       0        1\ngene_465          -1.790e-01  4.846e+04       0        1\ngene_466          -5.517e+00  4.188e+04       0        1\ngene_467           2.883e+00  4.213e+04       0        1\ngene_468          -2.714e+00  4.659e+04       0        1\ngene_469          -1.700e+00  4.917e+04       0        1\ngene_470          -3.545e+00  4.290e+04       0        1\ngene_471           3.398e+00  4.634e+04       0        1\ngene_472          -9.803e-01  5.062e+04       0        1\ngene_473          -1.703e-01  4.382e+04       0        1\ngene_474          -1.267e+00  4.636e+04       0        1\ngene_475           9.380e-01  4.322e+04       0        1\ngene_476           1.397e-01  4.943e+04       0        1\ngene_477          -2.662e+00  4.110e+04       0        1\ngene_478          -1.295e+00  4.864e+04       0        1\ngene_479          -8.941e-02  4.173e+04       0        1\ngene_480           2.345e+00  4.116e+04       0        1\ngene_481          -2.435e+00  4.437e+04       0        1\ngene_482          -1.243e+00  4.679e+04       0        1\ngene_483          -1.924e+00  4.051e+04       0        1\ngene_484           2.086e-01  4.802e+04       0        1\ngene_485           4.874e-01  4.770e+04       0        1\ngene_486          -4.277e+00  4.420e+04       0        1\ngene_487          -2.068e+00  4.389e+04       0        1\ngene_488           1.060e+00  4.015e+04       0        1\ngene_489           2.943e+00  4.192e+04       0        1\ngene_490           1.255e-01  4.503e+04       0        1\ngene_491          -2.419e+00  4.574e+04       0        1\ngene_492          -3.358e+00  4.441e+04       0        1\ngene_493           2.267e+00  4.342e+04       0        1\ngene_494           1.095e+00  4.808e+04       0        1\ngene_495           1.602e+00  4.106e+04       0        1\ngene_496           2.833e+00  4.383e+04       0        1\ngene_497           9.113e-01  4.807e+04       0        1\ngene_498          -8.802e-01  4.556e+04       0        1\ngene_499          -2.074e+00  4.252e+04       0        1\ngene_500          -1.218e+00  4.394e+04       0        1\ngene_501           1.970e-01  4.089e+04       0        1\ngene_502           7.694e-01  4.126e+04       0        1\ngene_503           2.464e+00  4.530e+04       0        1\ngene_504           4.226e+00  4.590e+04       0        1\ngene_505          -2.558e+00  4.467e+04       0        1\ngene_506          -2.442e-01  4.164e+04       0        1\ngene_507          -2.245e+00  4.164e+04       0        1\ngene_508           3.672e+00  4.419e+04       0        1\ngene_509           1.992e+00  4.186e+04       0        1\ngene_510           2.991e+00  4.542e+04       0        1\ngene_511          -4.555e-01  4.234e+04       0        1\ngene_512          -2.137e+00  3.984e+04       0        1\ngene_513           1.714e-02  4.583e+04       0        1\ngene_514          -6.371e-01  4.507e+04       0        1\ngene_515           4.668e+00  4.404e+04       0        1\ngene_516          -3.285e-01  4.357e+04       0        1\ngene_517           1.341e+00  4.476e+04       0        1\ngene_518           2.766e+00  4.178e+04       0        1\ngene_519          -2.116e+00  4.498e+04       0        1\ngene_520          -4.000e+00  4.460e+04       0        1\ngene_521          -5.364e+00  4.321e+04       0        1\ngene_522          -2.682e+00  4.531e+04       0        1\ngene_523           1.289e+00  4.200e+04       0        1\ngene_524           1.318e+00  5.069e+04       0        1\ngene_525           3.477e+00  4.556e+04       0        1\ngene_526           3.411e+00  4.325e+04       0        1\ngene_527          -1.331e+00  4.363e+04       0        1\ngene_528          -8.127e-01  4.420e+04       0        1\ngene_529           2.431e+00  4.734e+04       0        1\ngene_530           3.156e+00  4.627e+04       0        1\ngene_531          -4.530e+00  4.369e+04       0        1\ngene_532           2.734e+00  4.312e+04       0        1\ngene_533           2.862e+00  4.475e+04       0        1\ngene_534          -3.681e+00  4.890e+04       0        1\ngene_535          -4.697e+00  4.917e+04       0        1\ngene_536           1.198e+00  4.264e+04       0        1\ngene_537           1.399e+00  4.307e+04       0        1\ngene_538           2.251e+00  4.826e+04       0        1\ngene_539           2.880e+00  4.726e+04       0        1\ngene_540          -2.163e+00  4.410e+04       0        1\ngene_541          -9.378e-01  4.780e+04       0        1\ngene_542           5.088e-01  4.302e+04       0        1\ngene_543          -9.462e-02  4.277e+04       0        1\ngene_544          -2.899e+00  3.998e+04       0        1\ngene_545           3.009e-01  4.619e+04       0        1\ngene_546           3.133e+00  4.704e+04       0        1\ngene_547           2.286e+00  4.201e+04       0        1\ngene_548          -1.448e-01  4.130e+04       0        1\ngene_549           2.857e+00  4.459e+04       0        1\ngene_550          -2.901e+00  4.765e+04       0        1\ngene_551          -7.065e-01  3.942e+04       0        1\ngene_552           2.579e+00  4.365e+04       0        1\ngene_553           4.465e+00  4.530e+04       0        1\ngene_554           8.412e-02  4.361e+04       0        1\ngene_555          -3.513e-01  4.285e+04       0        1\ngene_556           3.132e+00  4.518e+04       0        1\ngene_557           2.855e+00  4.647e+04       0        1\ngene_558          -2.126e+00  4.341e+04       0        1\ngene_559          -2.158e+00  4.173e+04       0        1\ngene_560          -2.889e-01  4.444e+04       0        1\ngene_561           1.077e+00  4.649e+04       0        1\ngene_562          -3.558e-01  4.556e+04       0        1\ngene_563           2.474e+00  4.377e+04       0        1\ngene_564           9.546e-01  4.180e+04       0        1\ngene_565          -2.042e+00  4.038e+04       0        1\ngene_566          -2.558e-01  4.238e+04       0        1\ngene_567          -1.067e+00  4.078e+04       0        1\ngene_568          -4.891e+00  4.382e+04       0        1\ngene_569           1.140e+00  4.523e+04       0        1\ngene_570          -1.624e+00  4.334e+04       0        1\ngene_571          -1.736e+00  4.446e+04       0        1\ngene_572          -6.087e-01  4.620e+04       0        1\ngene_573          -1.885e+00  4.047e+04       0        1\ngene_574          -2.661e+00  4.702e+04       0        1\ngene_575          -1.908e+00  4.130e+04       0        1\ngene_576          -9.007e-01  4.410e+04       0        1\ngene_577          -1.923e+00  4.431e+04       0        1\ngene_578           1.481e-01  4.609e+04       0        1\ngene_579           9.415e-01  4.335e+04       0        1\ngene_580           7.978e+00  4.615e+04       0        1\ngene_581           3.917e+00  4.480e+04       0        1\ngene_582           5.127e+00  4.618e+04       0        1\ngene_583           1.321e-02  4.325e+04       0        1\ngene_584          -8.675e-01  4.825e+04       0        1\ngene_585           3.679e+00  4.543e+04       0        1\ngene_586          -3.585e+00  4.637e+04       0        1\ngene_587          -5.324e-01  4.350e+04       0        1\ngene_588           3.538e-02  4.596e+04       0        1\ngene_589           1.991e+00  4.664e+04       0        1\ngene_590          -2.826e+00  4.349e+04       0        1\ngene_591           1.218e+00  4.405e+04       0        1\ngene_592           2.416e+00  4.530e+04       0        1\ngene_593          -1.083e+00  4.488e+04       0        1\ngene_594           8.546e-01  4.778e+04       0        1\ngene_595           3.314e+00  4.065e+04       0        1\ngene_596           1.612e+00  4.737e+04       0        1\ngene_597           5.927e-01  4.293e+04       0        1\ngene_598          -1.821e+00  4.783e+04       0        1\ngene_599          -6.325e-01  4.666e+04       0        1\ngene_600          -2.468e+00  4.591e+04       0        1\ngene_601           1.339e+00  4.279e+04       0        1\ngene_602           6.609e-01  4.277e+04       0        1\ngene_603           2.702e-01  4.524e+04       0        1\ngene_604           1.806e-01  5.046e+04       0        1\ngene_605           1.807e-01  4.959e+04       0        1\ngene_606           6.877e-01  4.099e+04       0        1\ngene_607          -6.379e-01  4.291e+04       0        1\ngene_608           2.756e+00  4.766e+04       0        1\ngene_609          -5.140e+00  4.501e+04       0        1\ngene_610          -1.637e+00  4.271e+04       0        1\ngene_611          -2.630e+00  4.364e+04       0        1\ngene_612          -2.532e+00  4.745e+04       0        1\ngene_613           1.481e-01  4.455e+04       0        1\ngene_614          -1.518e+00  4.358e+04       0        1\ngene_615           3.080e+00  4.120e+04       0        1\ngene_616           4.295e+00  4.696e+04       0        1\ngene_617           2.233e+00  4.338e+04       0        1\ngene_618          -3.166e+00  4.601e+04       0        1\ngene_619           7.019e-01  4.581e+04       0        1\ngene_620           5.388e+00  4.473e+04       0        1\ngene_621           5.773e+00  4.665e+04       0        1\ngene_622          -2.674e+00  4.728e+04       0        1\ngene_623           1.926e+00  4.182e+04       0        1\ngene_624           6.735e-02  4.335e+04       0        1\ngene_625          -2.146e+00  4.751e+04       0        1\ngene_626          -1.630e+00  4.679e+04       0        1\ngene_627           1.747e-02  4.856e+04       0        1\ngene_628           3.792e+00  4.516e+04       0        1\ngene_629          -1.482e+00  4.827e+04       0        1\ngene_630           3.162e+00  4.407e+04       0        1\ngene_631          -6.158e-01  4.661e+04       0        1\ngene_632           2.757e+00  4.434e+04       0        1\ngene_633           2.784e+00  4.800e+04       0        1\ngene_634          -1.475e+00  4.299e+04       0        1\ngene_635           1.734e+00  4.781e+04       0        1\ngene_636          -6.420e-01  4.823e+04       0        1\ngene_637          -2.210e+00  4.544e+04       0        1\ngene_638           3.507e+00  5.009e+04       0        1\ngene_639           3.896e+00  4.889e+04       0        1\ngene_640          -6.137e-01  4.970e+04       0        1\ngene_641          -3.775e+00  4.902e+04       0        1\ngene_642           1.346e+00  4.790e+04       0        1\ngene_643          -7.777e-01  3.889e+04       0        1\ngene_644          -3.138e+00  4.353e+04       0        1\ngene_645           3.284e+00  4.087e+04       0        1\ngene_646          -6.107e+00  4.500e+04       0        1\ngene_647           6.319e-02  4.826e+04       0        1\ngene_648          -1.115e+00  4.622e+04       0        1\ngene_649          -6.775e-01  4.060e+04       0        1\ngene_650           7.928e-03  4.289e+04       0        1\ngene_651           1.379e+00  4.255e+04       0        1\ngene_652          -2.327e+00  4.558e+04       0        1\ngene_653           2.849e+00  4.484e+04       0        1\ngene_654           4.665e+00  4.647e+04       0        1\ngene_655          -6.885e-01  4.349e+04       0        1\ngene_656           4.813e+00  4.388e+04       0        1\ngene_657          -2.084e+00  4.688e+04       0        1\ngene_658          -1.377e+00  4.279e+04       0        1\ngene_659          -4.729e+00  4.676e+04       0        1\ngene_660           1.790e+00  4.871e+04       0        1\ngene_661          -9.000e-01  4.557e+04       0        1\ngene_662           1.711e+00  4.594e+04       0        1\ngene_663           1.681e+00  4.442e+04       0        1\ngene_664          -5.551e+00  4.228e+04       0        1\ngene_665           4.114e+00  4.680e+04       0        1\ngene_666           1.167e+00  4.469e+04       0        1\ngene_667           1.351e+00  4.161e+04       0        1\ngene_668          -5.125e-01  4.395e+04       0        1\ngene_669          -8.853e-01  4.519e+04       0        1\ngene_670           3.057e+00  4.702e+04       0        1\ngene_671          -1.887e+00  4.650e+04       0        1\ngene_672          -2.572e+00  4.343e+04       0        1\ngene_673           2.743e+00  4.611e+04       0        1\ngene_674          -1.259e+00  4.887e+04       0        1\ngene_675          -1.955e-01  4.509e+04       0        1\ngene_676           1.982e+00  4.535e+04       0        1\ngene_677           1.859e+00  4.147e+04       0        1\ngene_678           1.414e+00  4.319e+04       0        1\ngene_679           1.611e+00  4.542e+04       0        1\ngene_680           6.742e-01  4.874e+04       0        1\ngene_681          -8.128e-01  4.593e+04       0        1\ngene_682          -1.662e+00  4.662e+04       0        1\ngene_683           5.410e-01  4.688e+04       0        1\ngene_684          -5.013e+00  4.113e+04       0        1\ngene_685          -8.981e-01  4.747e+04       0        1\ngene_686           2.928e+00  4.669e+04       0        1\ngene_687           1.210e+00  4.085e+04       0        1\ngene_688           4.249e+00  4.411e+04       0        1\ngene_689          -1.074e+00  4.200e+04       0        1\ngene_690          -1.610e+00  4.577e+04       0        1\ngene_691           1.016e+00  4.607e+04       0        1\ngene_692           7.392e-01  4.213e+04       0        1\ngene_693          -2.116e+00  4.226e+04       0        1\ngene_694          -1.689e+00  4.404e+04       0        1\ngene_695          -1.865e+00  4.412e+04       0        1\ngene_696          -1.066e+00  4.828e+04       0        1\ngene_697           2.718e-01  4.556e+04       0        1\ngene_698           1.421e+00  4.464e+04       0        1\ngene_699          -2.734e+00  4.311e+04       0        1\ngene_700          -2.167e+00  4.685e+04       0        1\ngene_701          -4.436e-02  4.631e+04       0        1\ngene_702           9.633e-01  4.382e+04       0        1\ngene_703           6.131e-01  4.720e+04       0        1\ngene_704           1.276e-01  4.622e+04       0        1\ngene_705          -3.528e+00  4.396e+04       0        1\ngene_706          -1.235e+00  4.501e+04       0        1\ngene_707          -6.893e-01  3.876e+04       0        1\ngene_708          -8.714e-01  4.670e+04       0        1\ngene_709           1.446e+00  5.238e+04       0        1\ngene_710           3.143e+00  4.723e+04       0        1\ngene_711           1.231e+00  4.347e+04       0        1\ngene_712          -1.700e-01  4.884e+04       0        1\ngene_713           3.156e+00  4.547e+04       0        1\ngene_714          -2.413e+00  4.720e+04       0        1\ngene_715          -2.059e+00  4.259e+04       0        1\ngene_716           3.154e+00  4.860e+04       0        1\ngene_717           2.189e+00  4.490e+04       0        1\ngene_718          -1.157e+00  4.471e+04       0        1\ngene_719           1.294e+00  4.886e+04       0        1\ngene_720          -9.630e-01  4.205e+04       0        1\ngene_721           5.420e+00  4.525e+04       0        1\ngene_722           3.168e+00  4.708e+04       0        1\ngene_723          -1.271e+00  4.575e+04       0        1\ngene_724          -3.907e+00  4.654e+04       0        1\ngene_725          -1.685e+00  4.560e+04       0        1\ngene_726           1.168e+00  4.545e+04       0        1\ngene_727          -4.606e+00  4.622e+04       0        1\ngene_728          -2.458e+00  4.414e+04       0        1\ngene_729          -8.098e-01  4.918e+04       0        1\ngene_730           1.254e+00  4.245e+04       0        1\ngene_731          -1.524e+00  4.201e+04       0        1\ngene_732           3.909e+00  3.997e+04       0        1\ngene_733          -6.874e-02  4.530e+04       0        1\ngene_734           1.622e+00  4.351e+04       0        1\ngene_735           1.689e+00  4.575e+04       0        1\ngene_736          -1.296e+00  5.090e+04       0        1\ngene_737          -8.091e-01  4.233e+04       0        1\ngene_738          -3.905e-01  4.745e+04       0        1\ngene_739           7.930e-01  4.608e+04       0        1\ngene_740           8.763e-01  4.393e+04       0        1\ngene_741          -7.933e+00  4.882e+04       0        1\ngene_742           1.038e+00  4.257e+04       0        1\ngene_743           1.342e+00  4.484e+04       0        1\ngene_744           3.030e-01  4.969e+04       0        1\ngene_745           5.218e-01  4.658e+04       0        1\ngene_746           9.767e-01  4.532e+04       0        1\ngene_747           1.509e+00  4.333e+04       0        1\ngene_748          -8.020e-01  4.929e+04       0        1\ngene_749          -8.922e-01  4.335e+04       0        1\ngene_750          -1.428e+00  4.332e+04       0        1\ngene_751           1.836e+00  4.454e+04       0        1\ngene_752           1.005e+00  4.526e+04       0        1\ngene_753           2.835e+00  4.520e+04       0        1\ngene_754          -3.176e+00  4.529e+04       0        1\ngene_755           1.005e+00  4.539e+04       0        1\ngene_756          -2.033e+00  4.675e+04       0        1\ngene_757          -1.344e+00  4.608e+04       0        1\ngene_758           1.429e+00  4.517e+04       0        1\ngene_759           1.310e+00  4.582e+04       0        1\ngene_760           4.113e+00  4.871e+04       0        1\ngene_761           6.236e-01  4.611e+04       0        1\ngene_762          -1.231e+00  4.009e+04       0        1\ngene_763           2.514e+00  4.530e+04       0        1\ngene_764           5.875e-01  4.484e+04       0        1\ngene_765          -1.570e+00  4.262e+04       0        1\ngene_766           3.793e-01  4.420e+04       0        1\ngene_767          -4.305e-01  4.549e+04       0        1\ngene_768          -3.419e+00  4.340e+04       0        1\ngene_769          -1.855e+00  4.824e+04       0        1\ngene_770          -5.353e-01  4.534e+04       0        1\ngene_771           2.638e+00  4.600e+04       0        1\ngene_772          -4.158e+00  4.663e+04       0        1\ngene_773          -1.709e+00  4.685e+04       0        1\ngene_774          -3.186e+00  4.663e+04       0        1\ngene_775           3.965e-01  4.655e+04       0        1\ngene_776          -2.519e+00  4.296e+04       0        1\ngene_777           2.340e+00  4.139e+04       0        1\ngene_778          -1.039e-01  5.186e+04       0        1\ngene_779           8.801e-01  4.696e+04       0        1\ngene_780           1.346e+00  4.714e+04       0        1\ngene_781           3.323e+00  4.648e+04       0        1\ngene_782          -2.793e+00  4.629e+04       0        1\ngene_783           1.354e+00  4.553e+04       0        1\ngene_784           1.570e+00  4.567e+04       0        1\ngene_785           5.125e+00  4.268e+04       0        1\ngene_786           1.501e+00  4.827e+04       0        1\ngene_787           3.835e+00  4.877e+04       0        1\ngene_788           3.139e+00  4.663e+04       0        1\ngene_789           3.328e+00  4.290e+04       0        1\ngene_790          -1.168e+00  4.640e+04       0        1\ngene_791           1.502e+00  4.311e+04       0        1\ngene_792          -6.208e+00  4.026e+04       0        1\ngene_793           5.806e+00  4.449e+04       0        1\ngene_794          -5.033e+00  4.543e+04       0        1\ngene_795           1.845e+00  4.708e+04       0        1\ngene_796           5.526e-01  4.548e+04       0        1\ngene_797           1.395e+00  4.316e+04       0        1\ngene_798           1.790e-01  4.067e+04       0        1\ngene_799          -5.005e+00  4.315e+04       0        1\ngene_800          -8.736e-01  4.652e+04       0        1\ngene_801          -2.979e+00  4.611e+04       0        1\ngene_802           1.280e+00  4.534e+04       0        1\ngene_803           5.364e-01  4.754e+04       0        1\ngene_804           8.406e-01  3.916e+04       0        1\ngene_805           2.559e-01  4.443e+04       0        1\ngene_806          -2.301e+00  4.710e+04       0        1\ngene_807           6.020e-01  4.875e+04       0        1\ngene_808          -1.555e+00  4.521e+04       0        1\ngene_809          -1.538e+00  4.055e+04       0        1\ngene_810          -4.207e-01  4.664e+04       0        1\ngene_811           2.163e+00  4.800e+04       0        1\ngene_812          -6.904e-01  4.678e+04       0        1\ngene_813          -7.437e-01  4.440e+04       0        1\ngene_814          -1.942e+00  4.629e+04       0        1\ngene_815           3.878e+00  4.620e+04       0        1\ngene_816           1.957e+00  4.391e+04       0        1\ngene_817          -2.241e+00  4.469e+04       0        1\ngene_818          -4.586e+00  4.389e+04       0        1\ngene_819          -2.876e+00  4.735e+04       0        1\ngene_820          -3.498e+00  4.498e+04       0        1\ngene_821           1.930e-01  5.485e+04       0        1\ngene_822          -1.055e+00  4.180e+04       0        1\ngene_823           4.467e+00  4.448e+04       0        1\ngene_824           5.184e+00  4.434e+04       0        1\ngene_825           3.502e+00  4.444e+04       0        1\ngene_826           2.935e+00  4.837e+04       0        1\ngene_827           2.549e+00  4.424e+04       0        1\ngene_828          -1.001e-01  4.452e+04       0        1\ngene_829          -6.847e-01  4.308e+04       0        1\ngene_830           5.466e+00  4.608e+04       0        1\ngene_831          -2.046e+00  4.809e+04       0        1\ngene_832          -2.143e+00  4.248e+04       0        1\ngene_833          -8.540e-01  4.963e+04       0        1\ngene_834          -1.778e+00  4.255e+04       0        1\ngene_835           2.184e-01  4.736e+04       0        1\ngene_836           1.819e+00  4.550e+04       0        1\ngene_837          -3.661e+00  4.490e+04       0        1\ngene_838          -2.826e+00  4.584e+04       0        1\ngene_839           8.377e-01  4.570e+04       0        1\ngene_840           3.790e+00  4.332e+04       0        1\ngene_841           2.024e+00  4.784e+04       0        1\ngene_842           9.931e-01  4.432e+04       0        1\ngene_843          -1.433e+00  4.824e+04       0        1\ngene_844          -4.872e+00  4.217e+04       0        1\ngene_845          -2.521e+00  4.537e+04       0        1\ngene_846          -9.664e-01  4.405e+04       0        1\ngene_847           2.460e+00  4.324e+04       0        1\ngene_848          -4.631e+00  4.554e+04       0        1\ngene_849          -1.976e+00  4.455e+04       0        1\ngene_850          -8.899e-01  4.067e+04       0        1\ngene_851           7.086e-01  4.441e+04       0        1\ngene_852          -1.425e+00  4.599e+04       0        1\ngene_853           7.351e-02  4.510e+04       0        1\ngene_854          -1.100e+00  4.442e+04       0        1\ngene_855          -3.259e+00  4.250e+04       0        1\ngene_856          -2.169e+00  4.751e+04       0        1\ngene_857          -1.188e+00  4.099e+04       0        1\ngene_858          -8.495e-01  4.167e+04       0        1\ngene_859           2.114e+00  4.918e+04       0        1\ngene_860           3.614e+00  4.403e+04       0        1\ngene_861           4.660e+00  4.453e+04       0        1\ngene_862          -3.940e+00  5.030e+04       0        1\ngene_863           2.131e+00  5.129e+04       0        1\ngene_864          -7.683e-01  4.321e+04       0        1\ngene_865          -1.266e+00  4.642e+04       0        1\ngene_866           4.440e-01  4.270e+04       0        1\ngene_867           1.620e+00  4.486e+04       0        1\ngene_868          -1.432e+00  4.264e+04       0        1\ngene_869           2.699e+00  4.481e+04       0        1\ngene_870          -3.012e+00  4.466e+04       0        1\ngene_871           6.437e-01  4.795e+04       0        1\ngene_872           1.692e+00  4.467e+04       0        1\ngene_873          -3.300e+00  3.673e+04       0        1\ngene_874          -1.538e+00  4.884e+04       0        1\ngene_875          -3.173e+00  4.500e+04       0        1\ngene_876          -4.478e-01  4.558e+04       0        1\ngene_877          -1.201e+00  4.524e+04       0        1\ngene_878          -2.492e+00  4.641e+04       0        1\ngene_879          -5.053e+00  4.198e+04       0        1\ngene_880          -9.210e-01  4.673e+04       0        1\ngene_881           1.750e+00  4.553e+04       0        1\ngene_882          -5.076e+00  4.205e+04       0        1\ngene_883          -2.995e+00  5.088e+04       0        1\ngene_884          -4.250e-01  3.922e+04       0        1\ngene_885           6.669e-01  4.453e+04       0        1\ngene_886           2.409e+00  4.467e+04       0        1\ngene_887          -1.736e+00  4.238e+04       0        1\ngene_888          -1.246e+00  4.555e+04       0        1\ngene_889           1.178e+00  4.590e+04       0        1\ngene_890           4.644e+00  4.617e+04       0        1\ngene_891          -1.991e+00  4.750e+04       0        1\ngene_892          -1.732e+00  4.247e+04       0        1\ngene_893           1.943e+00  4.612e+04       0        1\ngene_894          -2.648e-01  4.124e+04       0        1\ngene_895           4.294e+00  4.367e+04       0        1\ngene_896          -1.410e+00  4.348e+04       0        1\ngene_897          -6.037e-01  4.733e+04       0        1\ngene_898          -9.878e-01  4.328e+04       0        1\ngene_899          -4.229e-02  4.434e+04       0        1\ngene_900           2.039e+00  4.443e+04       0        1\ngene_901           3.091e+00  4.222e+04       0        1\ngene_902           8.632e-01  4.648e+04       0        1\ngene_903           1.282e+00  4.271e+04       0        1\ngene_904           1.915e+00  5.077e+04       0        1\ngene_905           1.114e+00  4.793e+04       0        1\ngene_906          -1.187e+00  4.446e+04       0        1\ngene_907          -3.123e+00  4.585e+04       0        1\ngene_908           4.164e+00  4.239e+04       0        1\ngene_909          -4.876e+00  4.688e+04       0        1\ngene_910          -2.238e+00  4.607e+04       0        1\ngene_911          -4.799e+00  4.448e+04       0        1\ngene_912           2.180e+00  4.725e+04       0        1\ngene_913          -2.983e+00  4.750e+04       0        1\ngene_914           3.400e+00  4.595e+04       0        1\ngene_915           1.826e+00  4.560e+04       0        1\ngene_916          -3.154e+00  4.465e+04       0        1\ngene_917           1.340e+00  4.792e+04       0        1\ngene_918           1.972e+00  4.600e+04       0        1\ngene_919           1.865e+00  4.282e+04       0        1\ngene_920          -1.228e+00  5.063e+04       0        1\ngene_921          -1.954e-01  4.801e+04       0        1\ngene_922           1.690e+00  4.640e+04       0        1\ngene_923           2.282e-01  4.074e+04       0        1\ngene_924           1.636e+00  4.331e+04       0        1\ngene_925           2.422e+00  4.496e+04       0        1\ngene_926           1.214e+00  4.694e+04       0        1\ngene_927           4.149e+00  4.465e+04       0        1\ngene_928           1.135e+00  4.207e+04       0        1\ngene_929           5.723e-01  4.329e+04       0        1\ngene_930          -2.377e+00  4.423e+04       0        1\ngene_931          -4.726e+00  4.830e+04       0        1\ngene_932           1.014e+00  4.436e+04       0        1\ngene_933          -1.322e+00  5.150e+04       0        1\ngene_934          -3.191e+00  4.537e+04       0        1\ngene_935           3.577e+00  4.410e+04       0        1\ngene_936           2.534e+00  4.408e+04       0        1\ngene_937           1.849e+00  4.050e+04       0        1\ngene_938           7.854e-01  4.786e+04       0        1\ngene_939          -9.236e-02  4.298e+04       0        1\ngene_940           3.176e-01  4.538e+04       0        1\ngene_941          -1.262e+00  4.112e+04       0        1\ngene_942           2.550e+00  4.468e+04       0        1\ngene_943           5.054e-01  4.581e+04       0        1\ngene_944          -4.079e+00  4.309e+04       0        1\ngene_945          -2.815e+00  4.688e+04       0        1\ngene_946           8.553e-01  4.514e+04       0        1\ngene_947          -7.293e+00  4.208e+04       0        1\ngene_948           1.993e+00  4.319e+04       0        1\ngene_949          -8.090e-01  4.573e+04       0        1\ngene_950           8.820e-01  4.168e+04       0        1\ngene_951           7.165e-01  4.611e+04       0        1\ngene_952          -3.548e+00  4.286e+04       0        1\ngene_953          -1.700e+00  4.078e+04       0        1\ngene_954           7.803e-02  4.641e+04       0        1\ngene_955           3.434e-01  4.467e+04       0        1\ngene_956           1.194e+00  4.442e+04       0        1\ngene_957           2.396e+00  4.483e+04       0        1\ngene_958          -3.507e+00  4.720e+04       0        1\ngene_959          -1.454e+00  4.262e+04       0        1\ngene_960           2.701e+00  4.318e+04       0        1\ngene_961           2.113e+00  4.887e+04       0        1\ngene_962          -3.750e+00  4.419e+04       0        1\ngene_963          -1.169e+00  4.055e+04       0        1\ngene_964           1.824e+00  4.573e+04       0        1\ngene_965           4.620e-02  4.747e+04       0        1\ngene_966          -3.456e+00  4.883e+04       0        1\ngene_967          -1.113e+00  4.320e+04       0        1\ngene_968           2.089e-01  4.837e+04       0        1\ngene_969          -1.209e+00  4.587e+04       0        1\ngene_970          -4.935e+00  4.713e+04       0        1\ngene_971           6.075e-01  4.532e+04       0        1\ngene_972          -3.233e+00  4.956e+04       0        1\ngene_973           1.264e-01  4.650e+04       0        1\ngene_974           1.695e+00  4.968e+04       0        1\ngene_975           3.694e-01  4.513e+04       0        1\ngene_976          -1.343e+00  4.565e+04       0        1\ngene_977          -2.361e+00  4.592e+04       0        1\ngene_978           1.206e+00  4.301e+04       0        1\ngene_979          -2.761e-01  4.129e+04       0        1\ngene_980          -2.709e+00  5.111e+04       0        1\ngene_981           3.487e+00  4.241e+04       0        1\ngene_982           2.991e+00  4.535e+04       0        1\ngene_983           2.702e+00  4.926e+04       0        1\ngene_984          -2.480e+00  4.310e+04       0        1\ngene_985           7.496e-01  4.730e+04       0        1\ngene_986          -1.228e+00  4.397e+04       0        1\ngene_987           2.161e+00  4.447e+04       0        1\ngene_988           3.477e+00  4.325e+04       0        1\ngene_989           8.830e-01  4.534e+04       0        1\ngene_990          -2.003e+00  4.835e+04       0        1\ngene_991           8.186e-01  5.077e+04       0        1\ngene_992           2.170e+00  4.301e+04       0        1\ngene_993          -1.638e+00  4.084e+04       0        1\ngene_994           3.863e+00  4.189e+04       0        1\ngene_995           1.350e-01  4.176e+04       0        1\ngene_996           3.502e-01  4.061e+04       0        1\ngene_997          -8.334e-01  4.415e+04       0        1\ngene_998           2.309e+00  4.436e+04       0        1\ngene_999           1.790e+00  4.894e+04       0        1\ngene_1000         -1.042e+00  4.477e+04       0        1\ngene_1001         -2.129e+00  4.994e+04       0        1\ngene_1002         -1.948e+00  4.342e+04       0        1\ngene_1003          2.414e-01  4.797e+04       0        1\ngene_1004          1.606e-01  4.460e+04       0        1\ngene_1005         -1.672e+00  4.531e+04       0        1\ngene_1006         -1.086e+00  4.248e+04       0        1\ngene_1007          8.641e-01  4.465e+04       0        1\ngene_1008         -1.028e+00  4.518e+04       0        1\ngene_1009          1.827e-01  4.830e+04       0        1\ngene_1010         -1.355e+00  4.699e+04       0        1\ngene_1011          2.431e+00  4.615e+04       0        1\ngene_1012          3.974e+00  4.591e+04       0        1\ngene_1013         -3.100e+00  4.370e+04       0        1\ngene_1014          1.152e+00  4.050e+04       0        1\ngene_1015         -2.145e+00  4.784e+04       0        1\ngene_1016         -8.182e-01  4.067e+04       0        1\ngene_1017         -8.775e-01  4.684e+04       0        1\ngene_1018          5.715e-02  4.649e+04       0        1\ngene_1019         -3.465e+00  4.299e+04       0        1\ngene_1020          1.723e+00  4.323e+04       0        1\ngene_1021          9.837e-01  4.419e+04       0        1\ngene_1022         -3.035e+00  4.956e+04       0        1\ngene_1023         -1.946e+00  4.542e+04       0        1\ngene_1024         -2.095e-01  5.199e+04       0        1\ngene_1025          3.862e+00  4.412e+04       0        1\ngene_1026         -5.518e-01  4.994e+04       0        1\ngene_1027          6.009e-01  4.647e+04       0        1\ngene_1028         -8.577e-02  4.817e+04       0        1\ngene_1029          1.328e+00  4.205e+04       0        1\ngene_1030          7.588e-01  4.578e+04       0        1\ngene_1031          3.705e-01  4.365e+04       0        1\ngene_1032          1.830e+00  4.710e+04       0        1\ngene_1033         -2.457e+00  4.440e+04       0        1\ngene_1034          2.183e+00  4.325e+04       0        1\ngene_1035         -4.250e-01  4.284e+04       0        1\ngene_1036          1.059e+00  4.448e+04       0        1\ngene_1037         -3.442e+00  4.302e+04       0        1\ngene_1038         -1.089e+00  4.144e+04       0        1\ngene_1039          4.663e-01  4.344e+04       0        1\ngene_1040          2.102e+00  4.716e+04       0        1\ngene_1041          1.917e+00  4.689e+04       0        1\ngene_1042         -4.450e-01  4.430e+04       0        1\ngene_1043         -8.219e-01  4.780e+04       0        1\ngene_1044         -3.349e+00  4.145e+04       0        1\ngene_1045         -1.710e+00  4.786e+04       0        1\ngene_1046          4.749e+00  4.494e+04       0        1\ngene_1047         -1.854e+00  3.961e+04       0        1\ngene_1048         -5.982e-01  4.470e+04       0        1\ngene_1049         -5.689e+00  4.298e+04       0        1\ngene_1050          3.477e+00  4.264e+04       0        1\ngene_1051          5.995e-01  4.471e+04       0        1\ngene_1052         -3.585e+00  4.501e+04       0        1\ngene_1053         -2.602e-01  4.681e+04       0        1\ngene_1054         -1.937e+00  4.892e+04       0        1\ngene_1055         -4.330e+00  4.471e+04       0        1\ngene_1056          3.986e+00  4.265e+04       0        1\ngene_1057         -1.368e+00  4.603e+04       0        1\ngene_1058          1.626e+00  4.487e+04       0        1\ngene_1059         -4.095e+00  4.437e+04       0        1\ngene_1060         -2.036e+00  4.399e+04       0        1\ngene_1061         -1.745e+00  4.072e+04       0        1\ngene_1062          7.200e-02  4.246e+04       0        1\ngene_1063         -1.671e+00  3.842e+04       0        1\ngene_1064          3.338e+00  4.397e+04       0        1\ngene_1065          2.286e+00  4.266e+04       0        1\ngene_1066         -6.691e-01  5.045e+04       0        1\ngene_1067         -9.421e-01  4.120e+04       0        1\ngene_1068         -3.903e+00  4.418e+04       0        1\ngene_1069         -1.743e+00  4.128e+04       0        1\ngene_1070          3.176e+00  4.207e+04       0        1\ngene_1071         -4.552e+00  4.608e+04       0        1\ngene_1072         -1.501e+00  4.658e+04       0        1\ngene_1073         -4.294e-03  4.478e+04       0        1\ngene_1074         -1.752e-01  4.337e+04       0        1\ngene_1075         -1.079e+00  4.854e+04       0        1\ngene_1076          3.033e+00  3.915e+04       0        1\ngene_1077         -2.082e+00  4.961e+04       0        1\ngene_1078          3.081e+00  4.249e+04       0        1\ngene_1079          4.878e-01  4.919e+04       0        1\ngene_1080          2.349e+00  4.819e+04       0        1\ngene_1081         -2.652e+00  4.435e+04       0        1\ngene_1082         -8.060e-01  4.598e+04       0        1\ngene_1083         -2.295e-02  4.267e+04       0        1\ngene_1084          5.345e-01  4.513e+04       0        1\ngene_1085          1.399e+00  4.460e+04       0        1\ngene_1086         -3.366e+00  4.316e+04       0        1\ngene_1087          4.186e+00  4.446e+04       0        1\ngene_1088          2.012e+00  4.579e+04       0        1\ngene_1089          4.849e+00  4.377e+04       0        1\ngene_1090          1.922e+00  4.479e+04       0        1\ngene_1091          1.334e+00  4.395e+04       0        1\ngene_1092          5.613e-02  4.515e+04       0        1\ngene_1093          1.700e-01  4.015e+04       0        1\ngene_1094          1.686e+00  4.500e+04       0        1\ngene_1095          4.657e+00  4.678e+04       0        1\ngene_1096         -1.781e-02  4.892e+04       0        1\ngene_1097          4.413e+00  4.586e+04       0        1\ngene_1098         -1.923e+00  4.517e+04       0        1\ngene_1099         -2.292e+00  4.578e+04       0        1\ngene_1100         -4.251e+00  4.102e+04       0        1\ngene_1101          3.203e+00  4.111e+04       0        1\ngene_1102         -2.579e+00  4.466e+04       0        1\ngene_1103         -9.769e-01  5.046e+04       0        1\ngene_1104         -2.868e+00  4.492e+04       0        1\ngene_1105          3.545e+00  4.030e+04       0        1\ngene_1106         -1.271e+00  4.298e+04       0        1\ngene_1107          1.351e+00  4.449e+04       0        1\ngene_1108          2.464e+00  4.063e+04       0        1\ngene_1109         -2.515e+00  4.272e+04       0        1\ngene_1110          3.369e+00  4.355e+04       0        1\ngene_1111         -9.170e-01  4.519e+04       0        1\ngene_1112         -1.063e-01  4.693e+04       0        1\ngene_1113         -2.123e-01  4.776e+04       0        1\ngene_1114          4.068e-02  4.918e+04       0        1\ngene_1115          2.033e+00  4.709e+04       0        1\ngene_1116         -9.148e-01  4.545e+04       0        1\ngene_1117         -3.669e+00  4.497e+04       0        1\ngene_1118          5.810e+00  3.963e+04       0        1\ngene_1119         -4.507e-01  4.376e+04       0        1\ngene_1120         -3.585e+00  4.852e+04       0        1\ngene_1121         -1.658e+00  4.539e+04       0        1\ngene_1122         -9.343e-01  4.461e+04       0        1\ngene_1123         -2.194e+00  4.327e+04       0        1\ngene_1124          9.192e-01  4.829e+04       0        1\ngene_1125         -3.591e+00  4.416e+04       0        1\ngene_1126         -9.191e-01  4.651e+04       0        1\ngene_1127          1.754e+00  4.711e+04       0        1\ngene_1128          1.072e-02  4.181e+04       0        1\ngene_1129          3.705e+00  4.337e+04       0        1\ngene_1130          1.406e+00  4.468e+04       0        1\ngene_1131          3.424e+00  4.607e+04       0        1\ngene_1132          2.307e+00  4.824e+04       0        1\ngene_1133          3.142e+00  4.918e+04       0        1\ngene_1134         -1.835e+00  4.621e+04       0        1\ngene_1135         -9.615e-01  4.098e+04       0        1\ngene_1136         -3.006e-01  4.709e+04       0        1\ngene_1137          4.830e+00  4.284e+04       0        1\ngene_1138          2.401e+00  4.266e+04       0        1\ngene_1139          1.105e+00  4.560e+04       0        1\ngene_1140          1.436e+00  4.421e+04       0        1\ngene_1141         -1.342e+00  4.343e+04       0        1\ngene_1142         -7.544e-01  4.361e+04       0        1\ngene_1143          1.423e-01  4.724e+04       0        1\ngene_1144         -1.234e+00  4.702e+04       0        1\ngene_1145          3.038e+00  4.685e+04       0        1\ngene_1146         -1.922e+00  4.582e+04       0        1\ngene_1147         -1.035e+00  4.552e+04       0        1\ngene_1148          4.400e+00  4.435e+04       0        1\ngene_1149         -2.200e+00  4.499e+04       0        1\ngene_1150          4.080e+00  4.285e+04       0        1\ngene_1151         -4.714e+00  4.347e+04       0        1\ngene_1152          2.043e+00  4.643e+04       0        1\ngene_1153          3.495e+00  4.542e+04       0        1\ngene_1154         -1.475e+00  4.303e+04       0        1\ngene_1155         -4.290e-01  5.249e+04       0        1\ngene_1156         -1.532e+00  4.326e+04       0        1\ngene_1157         -5.786e-01  4.832e+04       0        1\ngene_1158          2.140e+00  4.565e+04       0        1\ngene_1159          2.767e+00  4.381e+04       0        1\ngene_1160         -3.908e+00  4.391e+04       0        1\ngene_1161          6.464e+00  4.290e+04       0        1\ngene_1162         -9.201e-01  4.742e+04       0        1\ngene_1163          1.038e+00  4.662e+04       0        1\ngene_1164          1.173e+00  4.565e+04       0        1\ngene_1165          3.138e+00  4.222e+04       0        1\ngene_1166          1.440e+00  4.751e+04       0        1\ngene_1167          7.610e-01  4.148e+04       0        1\ngene_1168          1.956e+00  4.283e+04       0        1\ngene_1169         -4.948e+00  4.284e+04       0        1\ngene_1170          8.334e-01  4.543e+04       0        1\ngene_1171          4.752e+00  4.501e+04       0        1\ngene_1172          2.384e-01  4.051e+04       0        1\ngene_1173         -7.514e-01  4.087e+04       0        1\ngene_1174          2.471e+00  4.022e+04       0        1\ngene_1175          1.602e+00  4.402e+04       0        1\ngene_1176          2.991e+00  4.881e+04       0        1\ngene_1177         -4.921e+00  3.922e+04       0        1\ngene_1178          2.085e+00  4.017e+04       0        1\ngene_1179         -1.811e+00  4.468e+04       0        1\ngene_1180          2.259e+00  4.411e+04       0        1\ngene_1181         -1.914e+00  4.526e+04       0        1\ngene_1182         -4.199e+00  4.582e+04       0        1\ngene_1183          2.146e+00  4.183e+04       0        1\ngene_1184         -2.058e+00  4.870e+04       0        1\ngene_1185         -3.055e+00  4.592e+04       0        1\ngene_1186          4.077e-02  4.317e+04       0        1\ngene_1187          3.510e+00  4.580e+04       0        1\ngene_1188         -1.814e-01  4.400e+04       0        1\ngene_1189          7.554e-01  4.633e+04       0        1\ngene_1190         -1.688e+00  4.724e+04       0        1\ngene_1191          9.593e-01  4.422e+04       0        1\ngene_1192          4.095e-01  4.208e+04       0        1\ngene_1193          1.872e-01  4.422e+04       0        1\ngene_1194         -3.137e+00  4.499e+04       0        1\ngene_1195          3.465e+00  5.147e+04       0        1\ngene_1196          6.887e-01  4.843e+04       0        1\ngene_1197         -4.966e+00  4.322e+04       0        1\ngene_1198          5.376e+00  5.001e+04       0        1\ngene_1199          2.073e+00  4.338e+04       0        1\ngene_1200         -7.942e-01  4.487e+04       0        1\ngene_1201          4.956e+00  4.325e+04       0        1\ngene_1202         -8.074e-01  4.282e+04       0        1\ngene_1203          3.758e+00  4.480e+04       0        1\ngene_1204          1.495e+00  4.569e+04       0        1\ngene_1205         -6.231e+00  4.480e+04       0        1\ngene_1206         -1.253e-01  4.481e+04       0        1\ngene_1207         -6.408e+00  4.538e+04       0        1\ngene_1208          2.728e+00  4.683e+04       0        1\ngene_1209         -1.633e+00  4.626e+04       0        1\ngene_1210         -1.716e-01  4.709e+04       0        1\ngene_1211         -3.446e+00  4.226e+04       0        1\ngene_1212          1.575e+00  4.303e+04       0        1\ngene_1213         -8.974e-01  4.412e+04       0        1\ngene_1214         -6.632e+00  4.577e+04       0        1\ngene_1215          1.584e+00  4.896e+04       0        1\ngene_1216         -5.466e-01  4.694e+04       0        1\ngene_1217          2.576e-01  4.648e+04       0        1\ngene_1218         -8.841e-01  4.688e+04       0        1\ngene_1219          2.781e+00  4.680e+04       0        1\ngene_1220         -1.458e+00  4.274e+04       0        1\ngene_1221          1.380e+00  4.498e+04       0        1\ngene_1222          4.707e+00  4.935e+04       0        1\ngene_1223          2.362e-01  4.511e+04       0        1\ngene_1224         -9.443e-01  5.271e+04       0        1\ngene_1225          1.329e+00  4.964e+04       0        1\ngene_1226         -6.422e-01  4.914e+04       0        1\ngene_1227          4.490e+00  5.037e+04       0        1\ngene_1228          9.185e-01  4.745e+04       0        1\ngene_1229         -3.051e+00  4.292e+04       0        1\ngene_1230         -5.312e-01  4.705e+04       0        1\ngene_1231         -1.931e-01  4.285e+04       0        1\ngene_1232          2.293e+00  5.274e+04       0        1\ngene_1233         -4.267e-01  4.271e+04       0        1\ngene_1234          2.064e+00  4.876e+04       0        1\ngene_1235         -5.604e-01  4.348e+04       0        1\ngene_1236         -4.790e+00  4.840e+04       0        1\ngene_1237          1.970e+00  4.270e+04       0        1\ngene_1238          1.058e-01  4.130e+04       0        1\ngene_1239          1.272e+00  4.606e+04       0        1\ngene_1240         -2.495e+00  4.172e+04       0        1\ngene_1241          3.571e+00  4.288e+04       0        1\ngene_1242          4.338e+00  4.701e+04       0        1\ngene_1243          2.691e+00  4.677e+04       0        1\ngene_1244          1.896e-01  4.539e+04       0        1\ngene_1245          8.149e-01  4.943e+04       0        1\ngene_1246          5.468e-01  4.428e+04       0        1\ngene_1247         -4.066e+00  4.933e+04       0        1\ngene_1248          2.247e-02  4.851e+04       0        1\ngene_1249         -4.116e+00  4.306e+04       0        1\ngene_1250         -1.085e+00  4.479e+04       0        1\ngene_1251         -3.368e-01  4.711e+04       0        1\ngene_1252          1.590e+00  4.628e+04       0        1\ngene_1253         -1.585e+00  4.397e+04       0        1\ngene_1254         -3.613e+00  4.883e+04       0        1\ngene_1255          3.175e+00  4.592e+04       0        1\ngene_1256          6.001e+00  5.205e+04       0        1\ngene_1257          2.828e+00  4.368e+04       0        1\ngene_1258          6.820e-02  4.650e+04       0        1\ngene_1259          4.128e+00  4.405e+04       0        1\ngene_1260         -3.961e+00  4.037e+04       0        1\ngene_1261         -3.490e-01  4.210e+04       0        1\ngene_1262         -2.329e+00  4.317e+04       0        1\ngene_1263         -4.320e-01  4.564e+04       0        1\ngene_1264         -1.860e+00  4.790e+04       0        1\ngene_1265         -3.580e+00  4.602e+04       0        1\ngene_1266         -1.551e+00  4.930e+04       0        1\ngene_1267         -1.152e+00  4.605e+04       0        1\ngene_1268         -6.082e+00  4.664e+04       0        1\ngene_1269         -2.913e+00  4.839e+04       0        1\ngene_1270         -1.904e+00  4.467e+04       0        1\ngene_1271         -3.303e+00  4.108e+04       0        1\ngene_1272          3.744e+00  4.553e+04       0        1\ngene_1273          3.404e+00  4.373e+04       0        1\ngene_1274          1.022e-01  4.604e+04       0        1\ngene_1275          5.636e-01  4.680e+04       0        1\ngene_1276         -5.799e+00  4.379e+04       0        1\ngene_1277          3.808e-01  4.648e+04       0        1\ngene_1278          2.249e+00  4.570e+04       0        1\ngene_1279         -1.821e+00  4.183e+04       0        1\ngene_1280         -4.884e+00  4.445e+04       0        1\ngene_1281         -2.281e-01  5.052e+04       0        1\ngene_1282          1.419e-01  4.565e+04       0        1\ngene_1283         -1.688e+00  4.619e+04       0        1\ngene_1284          1.380e+00  4.604e+04       0        1\ngene_1285         -2.042e+00  4.485e+04       0        1\ngene_1286         -4.857e+00  4.600e+04       0        1\ngene_1287         -2.401e+00  4.568e+04       0        1\ngene_1288         -2.460e+00  4.564e+04       0        1\ngene_1289          6.224e+00  4.332e+04       0        1\ngene_1290          1.359e+00  4.827e+04       0        1\ngene_1291          2.706e+00  4.610e+04       0        1\ngene_1292          7.370e-01  4.563e+04       0        1\ngene_1293         -9.135e-02  4.495e+04       0        1\ngene_1294         -1.129e+00  4.352e+04       0        1\ngene_1295         -4.447e+00  4.578e+04       0        1\ngene_1296         -1.482e+00  4.513e+04       0        1\ngene_1297          1.821e+00  4.494e+04       0        1\ngene_1298         -5.118e-01  4.327e+04       0        1\ngene_1299          1.220e+00  4.462e+04       0        1\ngene_1300         -1.150e+00  5.144e+04       0        1\ngene_1301         -8.632e-02  4.824e+04       0        1\ngene_1302          7.638e-01  4.509e+04       0        1\ngene_1303         -2.062e-01  4.474e+04       0        1\ngene_1304         -4.036e+00  4.480e+04       0        1\ngene_1305          1.817e+00  4.825e+04       0        1\ngene_1306          4.666e+00  4.319e+04       0        1\ngene_1307         -1.783e+00  4.232e+04       0        1\ngene_1308         -4.148e+00  4.688e+04       0        1\ngene_1309         -4.615e-01  5.086e+04       0        1\ngene_1310         -7.959e-01  4.111e+04       0        1\ngene_1311          3.070e+00  3.900e+04       0        1\ngene_1312         -9.908e-01  4.569e+04       0        1\ngene_1313          2.757e-01  4.322e+04       0        1\ngene_1314          1.376e-02  4.742e+04       0        1\ngene_1315          3.068e+00  4.669e+04       0        1\ngene_1316         -2.206e+00  4.489e+04       0        1\ngene_1317         -9.317e-01  4.975e+04       0        1\ngene_1318         -2.446e+00  4.344e+04       0        1\ngene_1319          2.078e+00  4.627e+04       0        1\ngene_1320         -5.517e-01  4.175e+04       0        1\ngene_1321         -4.960e+00  5.056e+04       0        1\ngene_1322          1.179e+00  4.168e+04       0        1\ngene_1323         -2.164e-01  4.867e+04       0        1\ngene_1324          1.198e+00  4.024e+04       0        1\ngene_1325         -1.420e+00  4.217e+04       0        1\ngene_1326          1.200e+00  4.451e+04       0        1\ngene_1327         -1.217e+00  4.552e+04       0        1\ngene_1328          5.363e+00  4.604e+04       0        1\ngene_1329          2.956e-03  4.140e+04       0        1\ngene_1330          1.369e-01  4.402e+04       0        1\ngene_1331         -5.950e-01  4.551e+04       0        1\ngene_1332         -2.149e+00  4.447e+04       0        1\ngene_1333          5.401e+00  4.536e+04       0        1\ngene_1334          2.076e+00  4.204e+04       0        1\ngene_1335          6.672e-02  4.803e+04       0        1\ngene_1336          1.641e+00  3.754e+04       0        1\ngene_1337         -4.073e+00  4.378e+04       0        1\ngene_1338         -6.217e+00  4.624e+04       0        1\ngene_1339          4.675e+00  4.542e+04       0        1\ngene_1340         -3.036e+00  4.568e+04       0        1\ngene_1341         -2.035e+00  4.640e+04       0        1\ngene_1342         -7.674e-02  4.457e+04       0        1\ngene_1343          5.412e+00  4.844e+04       0        1\ngene_1344         -2.262e-01  4.251e+04       0        1\ngene_1345         -4.385e-01  4.093e+04       0        1\ngene_1346          7.339e-02  4.228e+04       0        1\ngene_1347         -1.728e+00  4.421e+04       0        1\ngene_1348         -6.545e-03  4.232e+04       0        1\ngene_1349         -9.868e-01  4.374e+04       0        1\ngene_1350         -7.339e-01  4.429e+04       0        1\ngene_1351          7.208e-01  4.541e+04       0        1\ngene_1352          1.794e+00  4.792e+04       0        1\ngene_1353          7.918e-01  4.176e+04       0        1\ngene_1354         -8.510e-01  4.233e+04       0        1\ngene_1355          1.038e+00  4.557e+04       0        1\ngene_1356         -5.587e-01  4.360e+04       0        1\ngene_1357         -4.478e+00  4.650e+04       0        1\ngene_1358          1.891e+00  4.540e+04       0        1\ngene_1359         -4.152e+00  4.506e+04       0        1\ngene_1360         -9.909e-01  4.565e+04       0        1\ngene_1361          3.778e+00  4.406e+04       0        1\ngene_1362          7.588e-01  4.841e+04       0        1\ngene_1363          4.959e+00  4.364e+04       0        1\ngene_1364          1.482e+00  4.382e+04       0        1\ngene_1365          4.843e+00  4.406e+04       0        1\ngene_1366          1.219e+00  4.684e+04       0        1\ngene_1367         -1.347e+00  4.554e+04       0        1\ngene_1368          4.104e-03  4.868e+04       0        1\ngene_1369          4.203e-01  4.138e+04       0        1\ngene_1370          1.223e+00  4.174e+04       0        1\ngene_1371          2.179e+00  5.265e+04       0        1\ngene_1372         -4.853e+00  4.159e+04       0        1\ngene_1373         -2.021e+00  4.877e+04       0        1\ngene_1374         -1.684e+00  4.811e+04       0        1\ngene_1375          9.751e-03  4.511e+04       0        1\ngene_1376         -2.245e+00  4.363e+04       0        1\ngene_1377         -6.714e+00  4.485e+04       0        1\ngene_1378         -1.161e+00  4.742e+04       0        1\ngene_1379         -7.239e-01  4.883e+04       0        1\ngene_1380         -2.596e+00  4.370e+04       0        1\ngene_1381         -1.688e+00  4.495e+04       0        1\ngene_1382         -1.319e+00  4.836e+04       0        1\ngene_1383          1.108e+00  4.462e+04       0        1\ngene_1384         -2.770e+00  4.496e+04       0        1\ngene_1385          2.690e+00  4.313e+04       0        1\ngene_1386         -1.084e-01  4.271e+04       0        1\ngene_1387         -1.555e+00  4.925e+04       0        1\ngene_1388         -6.706e+00  4.512e+04       0        1\ngene_1389          4.131e+00  4.567e+04       0        1\ngene_1390         -2.297e+00  4.576e+04       0        1\ngene_1391          1.227e+00  4.330e+04       0        1\ngene_1392         -2.252e+00  4.585e+04       0        1\ngene_1393         -3.665e+00  4.363e+04       0        1\ngene_1394          2.236e+00  4.013e+04       0        1\ngene_1395         -1.983e+00  4.885e+04       0        1\ngene_1396          5.950e-01  4.508e+04       0        1\ngene_1397         -8.185e-01  4.499e+04       0        1\ngene_1398         -2.970e+00  4.443e+04       0        1\ngene_1399         -5.699e+00  4.478e+04       0        1\ngene_1400         -1.771e+00  4.168e+04       0        1\ngene_1401         -4.225e+00  4.833e+04       0        1\ngene_1402          8.945e-01  4.661e+04       0        1\ngene_1403         -3.065e+00  4.031e+04       0        1\ngene_1404         -4.077e-01  4.649e+04       0        1\ngene_1405         -1.101e+00  4.555e+04       0        1\ngene_1406          2.636e+00  4.643e+04       0        1\ngene_1407         -6.238e-01  4.355e+04       0        1\ngene_1408          2.968e+00  4.504e+04       0        1\ngene_1409         -1.085e-01  4.103e+04       0        1\ngene_1410         -1.723e+00  4.605e+04       0        1\ngene_1411         -1.542e+00  4.276e+04       0        1\ngene_1412          3.211e+00  4.683e+04       0        1\ngene_1413         -1.789e+00  4.104e+04       0        1\ngene_1414          1.949e+00  4.428e+04       0        1\ngene_1415         -2.861e-01  4.104e+04       0        1\ngene_1416         -2.025e+00  4.786e+04       0        1\ngene_1417         -8.071e-01  4.519e+04       0        1\ngene_1418         -2.631e+00  4.864e+04       0        1\ngene_1419         -8.050e-02  4.455e+04       0        1\ngene_1420          1.541e+00  4.658e+04       0        1\ngene_1421          1.244e+00  4.659e+04       0        1\ngene_1422         -3.084e-01  4.062e+04       0        1\ngene_1423          9.861e-01  5.029e+04       0        1\ngene_1424         -1.754e+00  4.401e+04       0        1\ngene_1425          2.064e+00  3.983e+04       0        1\ngene_1426         -5.027e+00  4.382e+04       0        1\ngene_1427          3.935e-01  4.710e+04       0        1\ngene_1428         -2.822e+00  4.561e+04       0        1\ngene_1429         -2.352e+00  4.561e+04       0        1\ngene_1430          1.662e+00  4.686e+04       0        1\ngene_1431          2.440e+00  4.451e+04       0        1\ngene_1432         -5.947e-01  4.831e+04       0        1\ngene_1433          1.412e+00  4.767e+04       0        1\ngene_1434          4.495e+00  4.344e+04       0        1\ngene_1435         -9.674e-01  4.698e+04       0        1\ngene_1436          6.433e+00  4.593e+04       0        1\ngene_1437         -4.447e+00  4.827e+04       0        1\ngene_1438          1.732e+00  4.592e+04       0        1\ngene_1439          5.857e-01  4.689e+04       0        1\ngene_1440          3.694e+00  4.517e+04       0        1\ngene_1441         -3.954e+00  4.290e+04       0        1\ngene_1442          8.360e-01  4.545e+04       0        1\ngene_1443          1.685e+00  4.473e+04       0        1\ngene_1444          3.019e+00  4.124e+04       0        1\ngene_1445          3.285e+00  4.459e+04       0        1\ngene_1446          9.405e-01  4.569e+04       0        1\ngene_1447         -9.025e-01  4.719e+04       0        1\ngene_1448          3.100e-01  4.547e+04       0        1\ngene_1449          5.656e-01  4.834e+04       0        1\ngene_1450         -1.084e+00  4.181e+04       0        1\ngene_1451          1.154e+00  5.133e+04       0        1\ngene_1452          1.830e+00  4.366e+04       0        1\ngene_1453         -1.078e+00  4.811e+04       0        1\ngene_1454         -2.626e+00  4.822e+04       0        1\ngene_1455          4.646e-02  4.632e+04       0        1\ngene_1456          9.236e-01  4.517e+04       0        1\ngene_1457         -4.333e-01  4.333e+04       0        1\ngene_1458         -1.304e+00  4.292e+04       0        1\ngene_1459         -2.210e+00  4.437e+04       0        1\ngene_1460          3.108e+00  4.957e+04       0        1\ngene_1461         -2.137e+00  4.487e+04       0        1\ngene_1462         -5.413e-01  4.497e+04       0        1\ngene_1463         -2.054e+00  4.091e+04       0        1\ngene_1464         -2.834e-01  4.455e+04       0        1\ngene_1465          1.696e+00  4.255e+04       0        1\ngene_1466          1.991e-01  4.240e+04       0        1\ngene_1467          2.840e+00  4.588e+04       0        1\ngene_1468         -7.285e+00  4.539e+04       0        1\ngene_1469         -1.470e+00  4.060e+04       0        1\ngene_1470         -2.317e+00  4.791e+04       0        1\ngene_1471          6.604e-01  4.153e+04       0        1\ngene_1472          4.739e+00  5.203e+04       0        1\ngene_1473         -1.865e-01  4.440e+04       0        1\ngene_1474         -4.549e-01  4.439e+04       0        1\ngene_1475          6.999e-02  4.427e+04       0        1\ngene_1476          1.873e+00  4.371e+04       0        1\ngene_1477         -2.658e+00  4.475e+04       0        1\ngene_1478          1.252e-01  4.931e+04       0        1\ngene_1479          2.240e+00  4.646e+04       0        1\ngene_1480          4.774e+00  4.434e+04       0        1\ngene_1481          2.607e+00  4.501e+04       0        1\ngene_1482         -1.690e+00  4.607e+04       0        1\ngene_1483          2.682e+00  4.703e+04       0        1\ngene_1484         -5.665e-01  4.619e+04       0        1\ngene_1485          2.373e+00  4.263e+04       0        1\ngene_1486          4.809e+00  4.548e+04       0        1\ngene_1487         -1.690e+00  4.456e+04       0        1\ngene_1488         -2.823e-01  4.475e+04       0        1\ngene_1489          3.598e+00  4.652e+04       0        1\ngene_1490          7.180e+00  4.473e+04       0        1\ngene_1491         -9.748e-01  3.989e+04       0        1\ngene_1492         -1.643e+00  4.613e+04       0        1\ngene_1493         -3.474e-01  4.767e+04       0        1\ngene_1494         -1.266e+00  4.325e+04       0        1\ngene_1495         -1.154e+00  4.364e+04       0        1\ngene_1496          1.067e+00  4.841e+04       0        1\ngene_1497         -7.616e-01  4.354e+04       0        1\ngene_1498         -3.715e+00  4.397e+04       0        1\ngene_1499          1.910e+00  4.474e+04       0        1\ngene_1500         -1.482e+00  4.638e+04       0        1\ngene_1501          1.759e-01  4.483e+04       0        1\ngene_1502         -1.909e-02  4.292e+04       0        1\ngene_1503         -2.536e+00  4.940e+04       0        1\ngene_1504         -2.207e+00  4.666e+04       0        1\ngene_1505          9.596e-01  4.931e+04       0        1\ngene_1506          1.821e+00  4.528e+04       0        1\ngene_1507          6.495e-01  4.578e+04       0        1\ngene_1508          2.457e+00  4.540e+04       0        1\ngene_1509         -3.736e+00  4.585e+04       0        1\ngene_1510          3.455e+00  4.027e+04       0        1\ngene_1511         -1.407e+00  4.475e+04       0        1\ngene_1512          2.169e+00  4.386e+04       0        1\ngene_1513         -9.476e-01  4.419e+04       0        1\ngene_1514          1.968e+00  4.713e+04       0        1\ngene_1515         -8.794e-01  4.774e+04       0        1\ngene_1516         -8.761e-01  4.786e+04       0        1\ngene_1517         -3.024e+00  4.146e+04       0        1\ngene_1518         -3.108e+00  4.586e+04       0        1\ngene_1519          4.281e+00  4.264e+04       0        1\ngene_1520         -1.772e+00  5.104e+04       0        1\ngene_1521          1.369e+00  4.052e+04       0        1\ngene_1522         -2.080e+00  4.455e+04       0        1\ngene_1523          1.531e+00  4.229e+04       0        1\ngene_1524          1.309e+00  3.935e+04       0        1\ngene_1525         -4.421e+00  4.445e+04       0        1\ngene_1526         -2.468e+00  4.431e+04       0        1\ngene_1527         -2.973e+00  4.783e+04       0        1\ngene_1528          5.051e+00  4.350e+04       0        1\ngene_1529         -8.222e-01  4.058e+04       0        1\ngene_1530          3.545e+00  4.426e+04       0        1\ngene_1531         -4.494e+00  4.787e+04       0        1\ngene_1532         -2.099e+00  4.475e+04       0        1\ngene_1533         -4.359e-01  4.645e+04       0        1\ngene_1534          1.718e+00  4.692e+04       0        1\ngene_1535         -1.359e+00  4.625e+04       0        1\ngene_1536         -1.125e+00  4.660e+04       0        1\ngene_1537         -1.154e-01  4.837e+04       0        1\ngene_1538          3.658e-01  4.581e+04       0        1\ngene_1539          2.165e+00  4.696e+04       0        1\ngene_1540          1.872e+00  4.121e+04       0        1\ngene_1541         -1.484e+00  4.667e+04       0        1\ngene_1542          1.216e+00  4.402e+04       0        1\ngene_1543          4.111e+00  4.225e+04       0        1\ngene_1544         -7.536e-01  4.665e+04       0        1\ngene_1545         -4.572e+00  4.653e+04       0        1\ngene_1546         -8.387e-01  4.386e+04       0        1\ngene_1547          5.215e+00  4.979e+04       0        1\ngene_1548         -1.447e+00  4.880e+04       0        1\ngene_1549         -3.758e+00  4.499e+04       0        1\ngene_1550         -2.768e+00  4.021e+04       0        1\ngene_1551          1.822e+00  4.638e+04       0        1\ngene_1552         -1.257e+00  3.920e+04       0        1\ngene_1553          1.585e+00  5.138e+04       0        1\ngene_1554         -2.851e+00  4.585e+04       0        1\ngene_1555          3.708e-01  4.176e+04       0        1\ngene_1556          2.445e+00  4.579e+04       0        1\ngene_1557         -1.391e+00  4.617e+04       0        1\ngene_1558          1.649e+00  4.786e+04       0        1\ngene_1559          2.404e+00  4.619e+04       0        1\ngene_1560          1.014e+00  4.576e+04       0        1\ngene_1561          1.957e+00  4.452e+04       0        1\ngene_1562          1.785e+00  4.376e+04       0        1\ngene_1563          1.031e+00  5.017e+04       0        1\ngene_1564         -4.808e+00  4.652e+04       0        1\ngene_1565         -1.517e+00  4.605e+04       0        1\ngene_1566         -1.435e+00  4.199e+04       0        1\ngene_1567         -5.240e+00  4.506e+04       0        1\ngene_1568         -3.883e+00  4.571e+04       0        1\ngene_1569         -2.167e+00  4.249e+04       0        1\ngene_1570          3.667e-01  4.608e+04       0        1\ngene_1571          9.684e-01  4.467e+04       0        1\ngene_1572          4.212e+00  4.437e+04       0        1\ngene_1573         -2.118e+00  4.335e+04       0        1\ngene_1574          2.694e+00  4.072e+04       0        1\ngene_1575         -1.998e+00  4.243e+04       0        1\ngene_1576         -3.456e+00  4.243e+04       0        1\ngene_1577          3.774e-01  4.100e+04       0        1\ngene_1578          1.948e+00  4.317e+04       0        1\ngene_1579          2.027e+00  4.715e+04       0        1\ngene_1580          3.638e+00  4.389e+04       0        1\ngene_1581         -3.364e+00  4.489e+04       0        1\ngene_1582         -9.686e-01  4.418e+04       0        1\ngene_1583          1.308e+00  4.624e+04       0        1\ngene_1584          5.594e-01  4.511e+04       0        1\ngene_1585         -2.039e+00  4.796e+04       0        1\ngene_1586         -2.243e-01  4.438e+04       0        1\ngene_1587         -1.326e+00  4.369e+04       0        1\ngene_1588          4.238e-01  4.846e+04       0        1\ngene_1589         -4.372e+00  4.457e+04       0        1\ngene_1590          3.662e+00  4.366e+04       0        1\ngene_1591         -1.591e+00  4.483e+04       0        1\ngene_1592          4.985e+00  4.383e+04       0        1\ngene_1593          9.002e-01  4.547e+04       0        1\ngene_1594         -3.753e+00  4.733e+04       0        1\ngene_1595         -2.466e+00  4.389e+04       0        1\ngene_1596         -2.223e+00  4.469e+04       0        1\ngene_1597          7.536e+00  4.491e+04       0        1\ngene_1598         -9.320e-01  4.373e+04       0        1\ngene_1599         -1.520e+00  4.824e+04       0        1\ngene_1600          8.426e-01  4.406e+04       0        1\ngene_1601          9.058e-01  4.629e+04       0        1\ngene_1602         -3.036e+00  4.965e+04       0        1\ngene_1603          5.809e-01  4.796e+04       0        1\ngene_1604         -4.762e+00  4.501e+04       0        1\ngene_1605         -1.786e+00  4.513e+04       0        1\ngene_1606         -5.960e-01  4.902e+04       0        1\ngene_1607          4.894e-01  4.588e+04       0        1\ngene_1608          2.651e+00  4.363e+04       0        1\ngene_1609          5.570e+00  4.134e+04       0        1\ngene_1610         -2.070e+00  4.720e+04       0        1\ngene_1611          2.327e+00  5.003e+04       0        1\ngene_1612         -1.987e+00  4.685e+04       0        1\ngene_1613          2.873e-01  4.652e+04       0        1\ngene_1614         -2.134e-01  4.579e+04       0        1\ngene_1615         -6.541e-01  4.427e+04       0        1\ngene_1616          3.615e+00  4.179e+04       0        1\ngene_1617          3.411e+00  4.510e+04       0        1\ngene_1618         -2.782e+00  4.108e+04       0        1\ngene_1619          5.818e+00  4.151e+04       0        1\ngene_1620         -9.357e-01  4.537e+04       0        1\ngene_1621         -3.170e+00  4.077e+04       0        1\ngene_1622          7.685e-01  4.336e+04       0        1\ngene_1623         -6.303e-01  4.732e+04       0        1\ngene_1624         -4.966e+00  4.482e+04       0        1\ngene_1625         -3.222e+00  4.488e+04       0        1\ngene_1626         -2.990e+00  4.552e+04       0        1\ngene_1627         -2.804e+00  4.952e+04       0        1\ngene_1628         -2.118e+00  4.448e+04       0        1\ngene_1629          4.430e+00  4.049e+04       0        1\ngene_1630          2.539e+00  4.647e+04       0        1\ngene_1631         -4.313e+00  4.936e+04       0        1\ngene_1632         -1.747e+00  4.488e+04       0        1\ngene_1633          7.503e-01  4.466e+04       0        1\ngene_1634          3.554e+00  4.516e+04       0        1\ngene_1635          1.936e+00  4.506e+04       0        1\ngene_1636         -1.269e+00  4.846e+04       0        1\ngene_1637         -9.387e-01  4.447e+04       0        1\ngene_1638         -5.299e+00  4.489e+04       0        1\ngene_1639         -5.135e-01  4.455e+04       0        1\ngene_1640         -4.525e+00  4.782e+04       0        1\ngene_1641          7.199e-01  4.355e+04       0        1\ngene_1642         -3.667e+00  4.527e+04       0        1\ngene_1643          9.477e-01  4.476e+04       0        1\ngene_1644         -1.458e+00  4.559e+04       0        1\ngene_1645          6.978e-02  4.328e+04       0        1\ngene_1646          6.620e-01  4.653e+04       0        1\ngene_1647         -4.168e+00  4.523e+04       0        1\ngene_1648          4.175e+00  4.460e+04       0        1\ngene_1649         -2.185e+00  4.034e+04       0        1\ngene_1650          9.186e-01  4.263e+04       0        1\ngene_1651          1.861e+00  4.443e+04       0        1\ngene_1652          2.712e+00  4.387e+04       0        1\ngene_1653          2.092e+00  4.403e+04       0        1\ngene_1654          2.464e+00  4.915e+04       0        1\ngene_1655         -3.039e-01  5.021e+04       0        1\ngene_1656          2.596e-01  4.152e+04       0        1\ngene_1657         -5.286e-01  4.142e+04       0        1\ngene_1658          2.222e+00  4.077e+04       0        1\ngene_1659          1.700e+00  4.503e+04       0        1\ngene_1660         -3.774e+00  3.790e+04       0        1\ngene_1661         -5.386e-01  4.326e+04       0        1\ngene_1662         -3.262e+00  4.543e+04       0        1\ngene_1663         -1.795e+00  4.301e+04       0        1\ngene_1664          2.794e-01  4.371e+04       0        1\ngene_1665          2.886e+00  4.604e+04       0        1\ngene_1666         -1.772e+00  4.498e+04       0        1\ngene_1667         -1.657e+00  4.637e+04       0        1\ngene_1668          1.624e-01  4.494e+04       0        1\ngene_1669          6.403e-01  4.789e+04       0        1\ngene_1670          2.546e+00  4.600e+04       0        1\ngene_1671          3.066e+00  4.431e+04       0        1\ngene_1672          1.268e+00  4.195e+04       0        1\ngene_1673          1.628e+00  4.584e+04       0        1\ngene_1674          3.234e-01  4.492e+04       0        1\ngene_1675         -9.319e-02  4.717e+04       0        1\ngene_1676          5.838e+00  4.445e+04       0        1\ngene_1677          4.015e+00  4.600e+04       0        1\ngene_1678         -3.168e-02  4.430e+04       0        1\ngene_1679          9.034e-01  4.166e+04       0        1\ngene_1680         -8.474e+00  5.008e+04       0        1\ngene_1681          1.193e+00  4.584e+04       0        1\ngene_1682          2.930e+00  4.186e+04       0        1\ngene_1683          4.123e+00  4.250e+04       0        1\ngene_1684         -1.937e-01  4.526e+04       0        1\ngene_1685         -5.475e+00  4.457e+04       0        1\ngene_1686         -3.304e+00  4.316e+04       0        1\ngene_1687          5.365e+00  4.596e+04       0        1\ngene_1688         -8.293e-01  4.135e+04       0        1\ngene_1689         -2.235e+00  4.528e+04       0        1\ngene_1690          2.524e+00  4.054e+04       0        1\ngene_1691          1.266e+00  4.271e+04       0        1\ngene_1692          7.854e-01  4.955e+04       0        1\ngene_1693          2.252e-01  4.789e+04       0        1\ngene_1694          9.940e-02  4.284e+04       0        1\ngene_1695         -2.257e+00  4.105e+04       0        1\ngene_1696         -2.315e+00  4.298e+04       0        1\ngene_1697          2.266e+00  4.288e+04       0        1\ngene_1698         -2.985e+00  4.417e+04       0        1\ngene_1699          4.288e-01  4.623e+04       0        1\ngene_1700          1.539e+00  4.367e+04       0        1\ngene_1701         -4.187e-01  4.920e+04       0        1\ngene_1702          5.530e-03  4.617e+04       0        1\ngene_1703         -1.143e-01  4.330e+04       0        1\ngene_1704          3.258e+00  4.524e+04       0        1\ngene_1705         -3.206e+00  4.931e+04       0        1\ngene_1706         -3.000e+00  4.567e+04       0        1\ngene_1707          2.530e+00  4.325e+04       0        1\ngene_1708         -2.444e-01  4.209e+04       0        1\ngene_1709         -6.140e-01  4.562e+04       0        1\ngene_1710          1.397e+00  4.267e+04       0        1\ngene_1711         -4.760e+00  4.649e+04       0        1\ngene_1712          7.637e-01  4.329e+04       0        1\ngene_1713          2.813e-01  4.299e+04       0        1\ngene_1714          3.442e+00  4.405e+04       0        1\ngene_1715         -1.294e+00  3.947e+04       0        1\ngene_1716         -1.951e+00  4.091e+04       0        1\ngene_1717         -5.730e-01  4.657e+04       0        1\ngene_1718          4.898e+00  4.661e+04       0        1\ngene_1719         -1.228e+00  4.326e+04       0        1\ngene_1720          1.048e+00  3.683e+04       0        1\ngene_1721          1.056e+00  4.671e+04       0        1\ngene_1722         -1.998e+00  4.808e+04       0        1\ngene_1723          7.141e-01  4.372e+04       0        1\ngene_1724         -9.667e-01  4.702e+04       0        1\ngene_1725          9.627e-01  4.882e+04       0        1\ngene_1726          4.329e+00  4.482e+04       0        1\ngene_1727         -5.438e+00  4.645e+04       0        1\ngene_1728          6.427e-01  4.623e+04       0        1\ngene_1729          1.653e+00  4.532e+04       0        1\ngene_1730         -3.123e-01  4.811e+04       0        1\ngene_1731         -3.066e+00  4.138e+04       0        1\ngene_1732         -1.299e+00  4.466e+04       0        1\ngene_1733         -6.992e-01  4.640e+04       0        1\ngene_1734          5.114e+00  4.394e+04       0        1\ngene_1735          9.848e-01  4.206e+04       0        1\ngene_1736          1.455e-01  4.171e+04       0        1\ngene_1737         -7.871e-01  4.534e+04       0        1\ngene_1738          3.073e+00  4.662e+04       0        1\ngene_1739         -2.731e-01  4.850e+04       0        1\ngene_1740          2.206e+00  4.713e+04       0        1\ngene_1741         -4.350e+00  4.404e+04       0        1\ngene_1742          1.219e+00  4.374e+04       0        1\ngene_1743          1.904e+00  4.496e+04       0        1\ngene_1744         -2.047e+00  4.623e+04       0        1\ngene_1745         -2.323e+00  4.198e+04       0        1\ngene_1746          3.564e+00  4.681e+04       0        1\ngene_1747          1.131e+00  4.505e+04       0        1\ngene_1748         -3.820e-01  4.358e+04       0        1\ngene_1749          2.005e+00  4.390e+04       0        1\ngene_1750          1.673e-01  4.262e+04       0        1\ngene_1751         -4.282e-01  4.189e+04       0        1\ngene_1752          1.196e+00  4.611e+04       0        1\ngene_1753          2.635e+00  4.379e+04       0        1\ngene_1754         -2.561e+00  4.366e+04       0        1\ngene_1755          7.525e-01  5.186e+04       0        1\ngene_1756          2.146e+00  4.866e+04       0        1\ngene_1757          8.176e-01  4.667e+04       0        1\ngene_1758         -3.054e+00  4.585e+04       0        1\ngene_1759         -2.221e+00  4.362e+04       0        1\ngene_1760         -1.552e+00  4.338e+04       0        1\ngene_1761         -1.090e+00  4.598e+04       0        1\ngene_1762         -4.435e+00  4.616e+04       0        1\ngene_1763          1.641e+00  4.272e+04       0        1\ngene_1764         -1.468e+00  4.219e+04       0        1\ngene_1765          3.268e-01  4.510e+04       0        1\ngene_1766          1.498e+00  4.466e+04       0        1\ngene_1767         -1.593e+00  4.412e+04       0        1\ngene_1768          9.333e+00  4.727e+04       0        1\ngene_1769         -2.547e+00  4.443e+04       0        1\ngene_1770         -4.199e-01  5.109e+04       0        1\ngene_1771          6.607e+00  4.287e+04       0        1\ngene_1772         -1.396e+00  4.192e+04       0        1\ngene_1773          1.693e+00  4.183e+04       0        1\ngene_1774         -8.657e-01  4.652e+04       0        1\ngene_1775         -3.702e+00  4.242e+04       0        1\ngene_1776         -1.018e-01  4.528e+04       0        1\ngene_1777         -8.130e-02  4.833e+04       0        1\ngene_1778         -2.987e+00  4.428e+04       0        1\ngene_1779          5.787e+00  4.625e+04       0        1\ngene_1780         -2.520e+00  4.247e+04       0        1\ngene_1781          9.092e-01  4.584e+04       0        1\ngene_1782         -1.091e+00  4.630e+04       0        1\ngene_1783          3.479e+00  4.867e+04       0        1\ngene_1784          1.304e+00  4.078e+04       0        1\ngene_1785          3.128e+00  4.227e+04       0        1\ngene_1786          1.749e+00  4.413e+04       0        1\ngene_1787          3.248e+00  4.539e+04       0        1\ngene_1788          1.660e-01  4.430e+04       0        1\ngene_1789         -5.521e-01  4.298e+04       0        1\ngene_1790          2.181e+00  4.411e+04       0        1\ngene_1791          5.359e+00  4.872e+04       0        1\ngene_1792          2.699e+00  4.250e+04       0        1\ngene_1793          1.735e+00  4.981e+04       0        1\ngene_1794          3.053e+00  4.433e+04       0        1\ngene_1795          4.244e+00  4.468e+04       0        1\ngene_1796         -1.489e-01  4.244e+04       0        1\ngene_1797         -3.534e+00  4.320e+04       0        1\ngene_1798          1.438e+00  4.448e+04       0        1\ngene_1799         -5.403e-01  4.995e+04       0        1\ngene_1800          3.799e+00  4.244e+04       0        1\ngene_1801         -4.469e+00  4.388e+04       0        1\ngene_1802          1.226e-01  4.715e+04       0        1\ngene_1803          1.578e-01  4.846e+04       0        1\ngene_1804          3.198e+00  4.148e+04       0        1\ngene_1805          8.954e-01  4.256e+04       0        1\ngene_1806          2.105e+00  4.230e+04       0        1\ngene_1807         -1.800e+00  4.601e+04       0        1\ngene_1808         -1.679e+00  4.505e+04       0        1\ngene_1809         -5.093e+00  4.509e+04       0        1\ngene_1810         -5.995e+00  4.307e+04       0        1\ngene_1811          3.194e+00  4.734e+04       0        1\ngene_1812          4.577e+00  4.439e+04       0        1\ngene_1813          3.769e+00  4.221e+04       0        1\ngene_1814         -1.481e+00  4.319e+04       0        1\ngene_1815          1.072e+00  4.364e+04       0        1\ngene_1816          2.201e+00  4.478e+04       0        1\ngene_1817          3.042e+00  4.584e+04       0        1\ngene_1818         -3.227e+00  4.983e+04       0        1\ngene_1819          6.854e-01  4.416e+04       0        1\ngene_1820         -1.181e+00  4.760e+04       0        1\ngene_1821         -1.091e+00  5.063e+04       0        1\ngene_1822          5.724e+00  4.312e+04       0        1\ngene_1823         -3.354e+00  4.737e+04       0        1\ngene_1824         -3.030e+00  4.750e+04       0        1\ngene_1825          2.508e+00  4.527e+04       0        1\ngene_1826          4.931e+00  4.325e+04       0        1\ngene_1827          1.599e+00  4.566e+04       0        1\ngene_1828          1.709e+00  4.577e+04       0        1\ngene_1829          5.294e-01  4.722e+04       0        1\ngene_1830          4.151e-01  4.296e+04       0        1\ngene_1831          2.305e+00  4.403e+04       0        1\ngene_1832         -1.565e+00  4.440e+04       0        1\ngene_1833         -1.810e+00  4.332e+04       0        1\ngene_1834          2.738e+00  4.844e+04       0        1\ngene_1835          5.403e+00  4.463e+04       0        1\ngene_1836         -3.602e+00  4.377e+04       0        1\ngene_1837          4.964e+00  4.328e+04       0        1\ngene_1838         -6.321e-01  4.259e+04       0        1\ngene_1839         -6.946e-01  4.223e+04       0        1\ngene_1840          5.724e+00  4.335e+04       0        1\ngene_1841          1.672e+00  4.392e+04       0        1\ngene_1842         -2.837e+00  4.303e+04       0        1\ngene_1843         -4.647e+00  4.725e+04       0        1\ngene_1844          2.637e-01  4.856e+04       0        1\ngene_1845          4.285e+00  4.850e+04       0        1\ngene_1846          3.081e+00  4.719e+04       0        1\ngene_1847         -7.393e-01  4.236e+04       0        1\ngene_1848          9.182e-01  4.562e+04       0        1\ngene_1849         -4.452e+00  5.138e+04       0        1\ngene_1850          1.236e-01  4.727e+04       0        1\ngene_1851         -4.020e+00  4.868e+04       0        1\ngene_1852          4.382e+00  4.546e+04       0        1\ngene_1853         -4.731e-01  4.353e+04       0        1\ngene_1854         -1.040e+00  4.292e+04       0        1\ngene_1855          1.862e+00  4.313e+04       0        1\ngene_1856         -2.007e+00  4.048e+04       0        1\ngene_1857         -3.148e+00  4.697e+04       0        1\ngene_1858          1.598e+00  5.104e+04       0        1\ngene_1859         -8.518e-01  4.781e+04       0        1\ngene_1860          3.207e+00  4.984e+04       0        1\ngene_1861          3.551e-01  4.540e+04       0        1\ngene_1862          2.209e+00  4.853e+04       0        1\ngene_1863          3.307e+00  4.260e+04       0        1\ngene_1864          1.577e+00  4.201e+04       0        1\ngene_1865         -2.195e+00  4.628e+04       0        1\ngene_1866          1.308e+00  4.838e+04       0        1\ngene_1867          5.235e-01  4.401e+04       0        1\ngene_1868          2.226e+00  4.748e+04       0        1\ngene_1869          2.399e+00  4.646e+04       0        1\ngene_1870          2.088e+00  4.734e+04       0        1\ngene_1871         -5.780e+00  4.617e+04       0        1\ngene_1872         -9.138e-02  4.782e+04       0        1\ngene_1873          2.750e+00  4.466e+04       0        1\ngene_1874         -2.277e+00  4.716e+04       0        1\ngene_1875         -2.248e+00  4.122e+04       0        1\ngene_1876         -2.518e+00  4.751e+04       0        1\ngene_1877         -9.207e-01  4.619e+04       0        1\ngene_1878          4.518e+00  4.925e+04       0        1\ngene_1879          4.868e-01  4.352e+04       0        1\ngene_1880         -3.013e+00  4.470e+04       0        1\ngene_1881         -7.474e-01  4.477e+04       0        1\ngene_1882         -1.817e+00  4.574e+04       0        1\ngene_1883          5.072e-04  4.547e+04       0        1\ngene_1884          3.804e+00  4.508e+04       0        1\ngene_1885         -6.117e-01  4.067e+04       0        1\ngene_1886         -3.685e-01  3.758e+04       0        1\ngene_1887         -3.663e+00  4.447e+04       0        1\ngene_1888         -5.949e-01  4.140e+04       0        1\ngene_1889         -1.894e+00  4.560e+04       0        1\ngene_1890         -1.000e+00  4.224e+04       0        1\ngene_1891          1.093e+00  4.388e+04       0        1\ngene_1892          1.128e+00  4.802e+04       0        1\ngene_1893         -1.274e+00  4.619e+04       0        1\ngene_1894         -1.141e-01  4.724e+04       0        1\ngene_1895         -7.431e-01  4.646e+04       0        1\ngene_1896          2.452e+00  4.629e+04       0        1\ngene_1897         -5.916e+00  4.332e+04       0        1\ngene_1898          2.974e+00  4.503e+04       0        1\ngene_1899          1.403e+00  4.248e+04       0        1\ngene_1900         -9.403e-01  4.530e+04       0        1\ngene_1901         -1.364e+00  5.014e+04       0        1\ngene_1902          6.917e-01  4.739e+04       0        1\ngene_1903          5.604e+00  4.891e+04       0        1\ngene_1904         -2.718e-01  4.448e+04       0        1\ngene_1905         -6.167e+00  4.785e+04       0        1\ngene_1906          3.260e+00  4.284e+04       0        1\ngene_1907         -2.029e+00  4.652e+04       0        1\ngene_1908         -9.541e-01  4.301e+04       0        1\ngene_1909          2.588e+00  4.420e+04       0        1\ngene_1910         -3.082e+00  4.041e+04       0        1\ngene_1911         -6.889e-01  3.985e+04       0        1\ngene_1912         -2.588e+00  4.420e+04       0        1\ngene_1913         -1.332e+00  4.172e+04       0        1\ngene_1914         -1.864e+00  4.328e+04       0        1\ngene_1915          1.082e+00  4.750e+04       0        1\ngene_1916         -2.359e+00  4.139e+04       0        1\ngene_1917          9.670e-01  5.042e+04       0        1\ngene_1918          2.797e+00  4.980e+04       0        1\ngene_1919         -5.734e-01  4.701e+04       0        1\ngene_1920          1.762e+00  4.741e+04       0        1\ngene_1921          3.352e+00  4.217e+04       0        1\ngene_1922         -3.825e+00  4.135e+04       0        1\ngene_1923          4.200e+00  4.114e+04       0        1\ngene_1924         -3.798e+00  5.010e+04       0        1\ngene_1925          8.670e-01  4.303e+04       0        1\ngene_1926         -1.187e+00  4.600e+04       0        1\ngene_1927         -1.399e+00  4.472e+04       0        1\ngene_1928         -2.551e+00  4.574e+04       0        1\ngene_1929         -1.320e+00  4.552e+04       0        1\ngene_1930         -7.666e-01  4.726e+04       0        1\ngene_1931          5.893e-01  4.429e+04       0        1\ngene_1932         -4.100e+00  4.662e+04       0        1\ngene_1933         -1.557e+00  4.564e+04       0        1\ngene_1934         -2.254e+00  5.375e+04       0        1\ngene_1935          1.432e-01  4.368e+04       0        1\ngene_1936         -2.600e+00  4.574e+04       0        1\ngene_1937          3.983e+00  4.846e+04       0        1\ngene_1938          1.907e+00  4.659e+04       0        1\ngene_1939         -1.327e+00  4.601e+04       0        1\ngene_1940         -1.102e-01  4.659e+04       0        1\ngene_1941          2.959e+00  4.772e+04       0        1\ngene_1942          3.799e-01  4.154e+04       0        1\ngene_1943         -1.225e+00  4.524e+04       0        1\ngene_1944          2.061e+00  4.500e+04       0        1\ngene_1945          2.572e-02  4.393e+04       0        1\ngene_1946          2.810e-01  4.577e+04       0        1\ngene_1947          3.117e+00  4.127e+04       0        1\ngene_1948          2.474e+00  4.942e+04       0        1\ngene_1949         -1.696e+00  4.316e+04       0        1\ngene_1950          4.786e+00  4.547e+04       0        1\ngene_1951          3.334e-01  4.307e+04       0        1\ngene_1952          5.287e+00  4.212e+04       0        1\ngene_1953         -1.336e+00  4.370e+04       0        1\ngene_1954          3.956e+00  4.528e+04       0        1\ngene_1955          4.266e+00  4.303e+04       0        1\ngene_1956          6.683e+00  4.736e+04       0        1\ngene_1957         -1.155e+00  4.139e+04       0        1\ngene_1958         -2.613e+00  4.488e+04       0        1\ngene_1959         -2.227e+00  4.932e+04       0        1\ngene_1960         -2.028e+00  4.680e+04       0        1\ngene_1961          1.014e+00  4.796e+04       0        1\ngene_1962          2.957e+00  4.321e+04       0        1\ngene_1963          1.115e+00  4.643e+04       0        1\ngene_1964          1.298e-01  5.170e+04       0        1\ngene_1965         -2.162e+00  4.364e+04       0        1\ngene_1966          2.879e+00  4.540e+04       0        1\ngene_1967         -3.023e+00  4.331e+04       0        1\ngene_1968         -2.556e+00  4.457e+04       0        1\ngene_1969          3.769e+00  4.578e+04       0        1\ngene_1970         -3.316e+00  4.140e+04       0        1\ngene_1971          3.201e+00  4.358e+04       0        1\ngene_1972          4.366e-01  4.180e+04       0        1\ngene_1973         -5.561e-01  4.601e+04       0        1\ngene_1974          2.545e+00  4.356e+04       0        1\ngene_1975         -2.534e-01  4.949e+04       0        1\ngene_1976          3.089e+00  4.706e+04       0        1\ngene_1977          5.419e-01  4.423e+04       0        1\ngene_1978         -2.079e+00  3.955e+04       0        1\ngene_1979          1.290e+00  4.777e+04       0        1\ngene_1980          4.105e+00  4.199e+04       0        1\ngene_1981          3.979e+00  4.539e+04       0        1\ngene_1982          3.983e+00  4.899e+04       0        1\ngene_1983         -4.914e-01  4.483e+04       0        1\ngene_1984          2.728e+00  4.577e+04       0        1\ngene_1985         -1.871e+00  4.485e+04       0        1\ngene_1986         -3.255e+00  4.380e+04       0        1\ngene_1987          2.699e+00  4.764e+04       0        1\ngene_1988         -1.213e+00  4.706e+04       0        1\ngene_1989         -3.045e+00  4.811e+04       0        1\ngene_1990          3.386e-01  4.240e+04       0        1\ngene_1991          4.574e+00  4.269e+04       0        1\ngene_1992          1.527e+00  4.531e+04       0        1\ngene_1993          3.999e+00  4.749e+04       0        1\ngene_1994         -1.513e+00  4.258e+04       0        1\ngene_1995          2.346e+00  4.511e+04       0        1\ngene_1996         -2.262e+00  4.305e+04       0        1\ngene_1997          1.744e+00  5.087e+04       0        1\ngene_1998          1.934e+00  4.444e+04       0        1\ngene_1999          8.743e-01  4.277e+04       0        1\ngene_2000          1.115e+00  4.687e+04       0        1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 9.2741e+03  on 6999  degrees of freedom\nResidual deviance: 2.0032e-07  on 4993  degrees of freedom\nAIC: 4014\n\nNumber of Fisher Scoring iterations: 25\n\n\n\n# Predicted probabilities\npred_logit_prob &lt;- predict(glm_logit, newdata = test_cls, type = \"response\")\n\n# Predicted classes\npred_logit_class &lt;- ifelse(pred_logit_prob &gt;= 0.5, 1, 0)\n\n# Confusion matrix\ncm_logit &lt;- table(\n  Predicted = pred_logit_class,\n  True = test_cls$high_response\n)\n\ncm_logit\n\n         True\nPredicted    0    1\n        0 1706  171\n        1  182  941\n\n\n\nlibrary(pROC)\nauc_logit &lt;- roc(test_cls$high_response, pred_logit_prob)$auc\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\nauc_logit\n\nArea under the curve: 0.939\n\n\n\n\n3.16.13 Unified comparison\n\n# =======================================================\n# Libraries\n# =======================================================\nlibrary(pROC)\nlibrary(tibble)\nlibrary(dplyr)\nlibrary(rpart)\nlibrary(rpart.plot)\n\n# =======================================================\n# 0) Ensure outcome is coded correctly\n# =======================================================\n\ntrain_cls &lt;- train_cls %&gt;% \n  mutate(high_response = factor(high_response, levels = c(0,1)))\n\ntest_cls &lt;- test_cls %&gt;% \n  mutate(high_response = factor(high_response, levels = c(0,1)))\n\n# =======================================================\n# 1) LOGISTIC REGRESSION\n# =======================================================\n\npred_logit_prob  &lt;- predict(glm_logit, newdata = test_cls, type = \"response\")\npred_logit_class &lt;- ifelse(pred_logit_prob &gt;= 0.5, 1, 0)\n\nauc_logit &lt;- roc(test_cls$high_response, pred_logit_prob)$auc\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\ncm_logit  &lt;- table(Predicted = pred_logit_class, True = test_cls$high_response)\n\n# =======================================================\n# 2) CART CLASSIFICATION TREE (PRUNED)\n# =======================================================\n\n# Build a proper classification tree\nct_class &lt;- rpart(\n  high_response ~ .,\n  data = train_cls,\n  method = \"class\",\n  control = rpart.control(cp = 0.001, maxdepth = 8, minsplit = 30)\n)\n\n# Prune using 1-SE rule\nprintcp(ct_class)\n\n\nClassification tree:\nrpart(formula = high_response ~ ., data = train_cls, method = \"class\", \n    control = rpart.control(cp = 0.001, maxdepth = 8, minsplit = 30))\n\nVariables actually used in tree construction:\n [1] gene_05   gene_08   gene_1074 gene_112  gene_14   gene_1654 gene_1721\n [8] gene_19   gene_1982 gene_1986 gene_447  gene_567  gene_694  gene_726 \n[15] gene_78  \n\nRoot node error: 2637/7000 = 0.37671\n\nn= 7000 \n\n          CP nsplit rel error   xerror      xstd\n1  0.8862344      0  1.000000 1.000000 0.0153741\n2  0.0125142      1  0.113766 0.124384 0.0067051\n3  0.0113766      2  0.101251 0.110732 0.0063435\n4  0.0068259      3  0.089875 0.102768 0.0061207\n5  0.0026545      5  0.076223 0.094046 0.0058652\n6  0.0024649      6  0.073568 0.094805 0.0058879\n7  0.0022753      8  0.068639 0.096322 0.0059331\n8  0.0020857     11  0.061813 0.095563 0.0059106\n9  0.0018961     14  0.054608 0.097459 0.0059667\n10 0.0016433     15  0.052711 0.096701 0.0059443\n11 0.0011377     18  0.047782 0.096322 0.0059331\n12 0.0010000     20  0.045506 0.096701 0.0059443\n\nbest_row &lt;- which.min(ct_class$cptable[,\"xerror\"])\nxerr_min &lt;- ct_class$cptable[best_row, \"xerror\"]\nxstd_min &lt;- ct_class$cptable[best_row, \"xstd\"]\ncp_1se   &lt;- ct_class$cptable[ct_class$cptable[,\"xerror\"] &lt;= xerr_min + xstd_min, \"CP\"][1]\n\nct_pruned_cls &lt;- prune(ct_class, cp = cp_1se)\n\n# Predict probabilities and classes\npred_tree_prob  &lt;- predict(ct_pruned_cls, newdata = test_cls, type = \"prob\")[, \"1\"]\npred_tree_class &lt;- ifelse(pred_tree_prob &gt;= 0.5, 1, 0)\n\nauc_tree &lt;- roc(test_cls$high_response, pred_tree_prob)$auc\n\nSetting levels: control = 0, case = 1\nSetting direction: controls &lt; cases\n\ncm_tree  &lt;- table(Predicted = pred_tree_class, True = test_cls$high_response)\n\n# =======================================================\n# 3) RANDOM FOREST (ranger)\n# =======================================================\n\npred_rf_prob  &lt;- predict(rf_class, data = test_cls)$predictions[, \"1\"]\npred_rf_class &lt;- ifelse(pred_rf_prob &gt;= 0.5, 1, 0)\n\nauc_rf &lt;- roc(test_cls$high_response, pred_rf_prob)$auc\n\nSetting levels: control = 0, case = 1\nSetting direction: controls &lt; cases\n\ncm_rf  &lt;- table(Predicted = pred_rf_class, True = test_cls$high_response)\n\n# =======================================================\n# 4) XGBoost \n# =======================================================\n\n# Here: y_test is numeric 0/1, X_test is a numeric matrix\npred_xgb_prob  &lt;- predict(xgb_fit, newdata = X_test)\npred_xgb_class &lt;- ifelse(pred_xgb_prob &gt;= 0.5, 1, 0)\n\nauc_xgb &lt;- roc(y_test, pred_xgb_prob)$auc\n\nSetting levels: control = 0, case = 1\nSetting direction: controls &lt; cases\n\ncm_xgb  &lt;- table(Predicted = pred_xgb_class, True = y_test)\n\n# =======================================================\n# 5) PERFORMANCE SUMMARY TABLE\n# =======================================================\n\nperf_summary &lt;- tibble(\n  Model = c(\"Logistic Regression\", \"Pruned CART Tree\", \"Random Forest\", \"XGBoost\"),\n  AUC   = c(auc_logit, auc_tree, auc_rf, auc_xgb),\n  Notes = c(\n    \"Linear baseline; interpretable\",\n    \"Simple non-linear rules; high variance\",\n    \"Strong low-variance ensemble\",\n    \"Best accuracy; low bias + low variance\"\n  )\n)\n\nperf_summary\n\n# A tibble: 4 × 3\n  Model                 AUC Notes                                 \n  &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;                                 \n1 Logistic Regression 0.939 Linear baseline; interpretable        \n2 Pruned CART Tree    0.984 Simple non-linear rules; high variance\n3 Random Forest       0.996 Strong low-variance ensemble          \n4 XGBoost             0.998 Best accuracy; low bias + low variance\n\n# =======================================================\n# 6) CONFUSION MATRICES (OPTIONAL PRINT)\n# =======================================================\n\nlist(\n  Logistic_Regression = cm_logit,\n  CART_Pruned = cm_tree,\n  Random_Forest = cm_rf,\n  XGBoost = cm_xgb\n)\n\n$Logistic_Regression\n         True\nPredicted    0    1\n        0 1706  171\n        1  182  941\n\n$CART_Pruned\n         True\nPredicted    0    1\n        0 1845   38\n        1   43 1074\n\n$Random_Forest\n         True\nPredicted    0    1\n        0 1853   56\n        1   35 1056\n\n$XGBoost\n         True\nPredicted    0    1\n        0 1852   27\n        1   36 1085\n\n\n\n\n3.16.14 Final comparison for all models regarding classification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\n\nAccuracy\n\nSensitivity (Recall)\n\nSpecificity\n\nAUC (ROC)\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression (GLM)\n\n0.907\n\n0.897\n\n0.913\n\n0.939\n\nLinear baseline; no regularization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLASSO Logistic Regression\n\n0.911\n\n0.909\n\n0.913\n\n0.948\n\nSparse, interpretable; variable selection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRidge Logistic Regression\n\n0.910\n\n0.922\n\n0.903\n\n0.951\n\nHandles correlated genes; no sparsity\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElastic Net Logistic Regression\n\n0.910\n\n0.909\n\n0.911\n\n0.949\n\nBest linear compromise; stabilizes groups of genes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPruned CART Tree\n\n0.973\n\n0.966\n\n0.977\n\n0.984\n\nSimple nonlinear rules; interpretable; high variance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\n0.972\n\n0.950\n\n0.982\n\n0.996\n\nLow variance ensemble; handles interactions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nXGBoost\n\n0.987\n\n0.976\n\n0.981\n\n0.998\n\nBest accuracy; low bias; robust in high dimension\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe full benchmark highlights a clear hierarchy in classification performance. Penalized logistic regression models (LASSO, Ridge, Elastic Net) outperform standard logistic regression by stabilizing coefficients and reducing overfitting, but remain limited by their linear functional form. Pruned CART models provide interpretable if–then clinical rules and perform much better than linear methods, but still exhibit high variance in high-dimensional genomic settings. Random Forests further improve performance through bagging and feature subsampling, achieving an AUC above 0.99. XGBoost achieves the strongest overall performance (AUC ≈ 0.998), leveraging gradient boosting, regularization, and subsampling to capture subtle nonlinear gene–gene interactions. This pattern is consistent with modern biomedical machine learning: as complexity and dimensionality rise, ensemble tree-based models tend to dominate in predictive accuracy.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supervised Learning: Tree Methods</span>"
    ]
  },
  {
    "objectID": "neural_networks.html",
    "href": "neural_networks.html",
    "title": "4  Neural networks and deep learning",
    "section": "",
    "text": "4.1 Setting up R\nrequired &lt;- c(\"reticulate\",\"ggplot2\",\"dplyr\",\"magick\")\ninstalled &lt;- rownames(installed.packages())\nto_install &lt;- setdiff(required, installed)\nif (length(to_install) &gt; 0) install.packages(to_install)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#neuromorphism",
    "href": "neural_networks.html#neuromorphism",
    "title": "4  Neural networks and deep learning",
    "section": "4.2 Neuromorphism",
    "text": "4.2 Neuromorphism\nThe inspiration for artificial neural networks comes from attempts to capture, in a very simplified mathematical form, how biological neurons compute.\nThis idea is known as neuromorphism (neuromorphic computing):\n\nrepresenting computation using abstractions loosely inspired by the brain, where “neurons” receive signals, combine them, and produce outputs,\nand “synapses” determine how strongly inputs influence the neuron.\n\nThe goal was never to perfectly copy biology, but to learn from its ability to detect patterns, integrate evidence, and generalize. The reasons are that in fact biological systems are extremely complex to be perfectly copied.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#what-is-inside-our-brain",
    "href": "neural_networks.html#what-is-inside-our-brain",
    "title": "4  Neural networks and deep learning",
    "section": "4.3 What is inside our brain!",
    "text": "4.3 What is inside our brain!\nNeurons are specialized cells designed to receive, integrate, and transmit information. Neurons were first described by Santiago Jamon y Cajal, the father of neurology.\nAlthough there are many neuronal subtypes, a “canonical” neuron has the manjor structural components show in Figure 4.1.\n\n\n\n\n\n\nFigure 4.1\n\n\n\n1. Dendrites (the input tree).\nDendrites are branched projections responsible for receiving incoming signals. Neurons typically receive thousands of excitatory and inhibitory synaptic inputs across their dendrites and soma.\nThese inputs arrive as postsynaptic potentials:\n\nEPSPs (excitatory postsynaptic potentials), which depolarize the membrane\nIPSPs (inhibitory postsynaptic potentials), which hyperpolarize the membrane\n\nThe spatial and temporal arrangement of these inputs contributes to how the neuron integrates information.\n2. Synapses (weighted connections)\nEach point of contact between neurons is a synapse, usually located on the dendritic tree. Synapses differ in their strength, some exert strong influence, others weak, and these strengths are not fixed.\nThis plasticity is the biological basis for learning and memory.\n\nSoma (Cell Body), the integrator\n\nThe soma receives and integrates all incoming EPSPs and IPSPs. This process is essentially a biophysical weighted sum of excitatory and inhibitory inputs, influenced by electrical properties of the membrane and dendritic morphology.\nIf the integrated membrane potential reaches a critical threshold, the neuron fires.\n4. Axon Initial Segment, the decision point\nAt the base of the axon lies the Axon Initial Segment (AIS), where voltage-gated sodium channels are densely clustered. This is the site where the neuron decides whether to generate an action potential. If the integrated signal surpasses the threshold (typically around -55 mV ), a rapid depolarization occurs the spike.\n5. Axon, the output cable\nOnce initiated, the action potential propagates along the axon, often insulated by myelin, which speeds conduction. When it reaches the axon terminals, it triggers release of neurotransmitters into the synaptic cleft, thereby influencing other neurons.\nIn summary, a biological neuron performs three essential computational steps:\n1. Receives signals from thousands of synapses (inputs)\n2. Weights and integrates them electrically in the soma\n3. Triggers an action potential if the integrated input crosses a threshold\n4. Propagates that output to other neurons\nThis biological computation is analog, non-linear, and highly dynamic due to plasticity.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#the-artificial-neuron-perceptron",
    "href": "neural_networks.html#the-artificial-neuron-perceptron",
    "title": "4  Neural networks and deep learning",
    "section": "4.4 The artificial neuron (Perceptron)",
    "text": "4.4 The artificial neuron (Perceptron)\nThe artificial neuron, often called a perceptron, is a mathematical abstraction inspired by the computational behavior of biological neurons. While biological neurons operate through complex electrophysiological processes, the perceptron captures their essential functional properties using a simplified, interpretable formalism. This abstraction forms the foundational building block of modern neural networks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#inputs-and-weights",
    "href": "neural_networks.html#inputs-and-weights",
    "title": "4  Neural networks and deep learning",
    "section": "4.5 Inputs and Weights",
    "text": "4.5 Inputs and Weights\nA perceptron receives a set of numerical inputs\n\\[\nx_1, x_2, \\ldots, x_p,\n\\]\nanalogous to the synaptic signals arriving at a biological neuron’s dendrites.\nEach input is associated with a weight:\n\\[\nw_1, w_2, \\ldots, w_p .\n\\]\nThese weights represent the relative importance or influence of each input on the neuron’s final response. Conceptually, they play the role of synaptic strengtris-stronger weights exert greater impact, while",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#weighted-sum-aggregation",
    "href": "neural_networks.html#weighted-sum-aggregation",
    "title": "4  Neural networks and deep learning",
    "section": "4.6 Weighted Sum (Aggregation)",
    "text": "4.6 Weighted Sum (Aggregation)\nThe perceptron aggregates the incoming signals by computing a weighted linear combination:\n\\[\nz=w_1 x_1+w_2 x_2+\\cdots+w_p x_p+b=\\mathbf{w}^{\\top} \\mathbf{x}+b .\n\\]\nThe term \\(b\\) is the bias, which shifts the decision boundary and functions similarly to the membrane resting potential in biological neurons. This operation mirrors the integration of excitatory and inhibitory postsynaptic potentials that occurs within the soma of a biological neuron.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#nonlinear-activation-function",
    "href": "neural_networks.html#nonlinear-activation-function",
    "title": "4  Neural networks and deep learning",
    "section": "4.7 Nonlinear Activation Function",
    "text": "4.7 Nonlinear Activation Function\nOnce the weighted sum is computed, the perceptron applies an activation function, denoted \\(\\sigma(z)\\), to determine its output.\nThis step introduces nonlinearity, allowing neural networks to model any continuous function on compact domains when sufficiently deep, as guaranteed by the Universal Approximation Theorem.\nCommon activation functions include:\n\nStep function (original perceptron):\n\n\\[\n\\sigma(z)= \\begin{cases}1, & z \\geq 0 \\\\ 0, & z&lt;0\\end{cases}\n\\]\n\nLogistic (sigmoid) function:\n\n\\[\n\\sigma(z)=\\frac{1}{1+e^{-z}}\n\\]\n\nReLU (Rectified Linear Unit):\n\n\\[\n\\sigma(z)=\\max (0, z)\n\\]\nIn biological terms, this activation function plays a role analogous to the axon initial segment, which determines whether the integrated membrane potential is sufficient to trigger an action potential.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#output",
    "href": "neural_networks.html#output",
    "title": "4  Neural networks and deep learning",
    "section": "4.8 Output",
    "text": "4.8 Output\nThe final output of the perceptron is:\n\\[\n\\hat{y}=\\sigma\\left(\\mathbf{w}^{\\top} \\mathbf{x}+b\\right),\n\\]\na scalar value that reflects the neuron’s “decision” based on the inputs. With a sigmoid output, for example, \\(\\hat{y}\\) represents the probability of a particular class, making the perceptron suitable for binary classification problems.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#learning-the-parameters",
    "href": "neural_networks.html#learning-the-parameters",
    "title": "4  Neural networks and deep learning",
    "section": "4.9 Learning the Parameters",
    "text": "4.9 Learning the Parameters\nThe perceptron is not static: it learns its weights and bias from data. Learning occurs by adjusting \\(\\mathbf{w}\\) and \\(b\\) to reduce prediction error, typically through optimization algorithms such as gradient descent.\nThis process is the computational analogue of synaptic plasticity in biological neurons. For a training example with true label \\(y\\) and predicted output \\(\\hat{y}\\), the weights are updated to reduce the difference between \\(y\\) and \\(\\hat{y}\\) :\n\\[\nw_j \\leftarrow w_j-\\eta \\frac{\\partial \\mathcal{L}}{\\partial w_j},\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#geometric-interpretation",
    "href": "neural_networks.html#geometric-interpretation",
    "title": "4  Neural networks and deep learning",
    "section": "4.10 Geometric Interpretation",
    "text": "4.10 Geometric Interpretation\nGeometrically, the perceptron learns a linear decision boundary. For a binary classification task with output \\(\\hat{y} \\in\\{0,1\\}\\), the equation\n\\[\n\\mathbf{w}^{\\top} \\mathbf{x}+b=0\n\\]\nrepresents a hyperplane that separates the two classes in input space. The perceptron adjusts its parameters during training until this hyperplane best divides the data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#visualisation-of-a-perceptron",
    "href": "neural_networks.html#visualisation-of-a-perceptron",
    "title": "4  Neural networks and deep learning",
    "section": "4.11 Visualisation of a perceptron",
    "text": "4.11 Visualisation of a perceptron\nThe following diagram shows an illustration of a perceptron.\n\n\n\n\n\nflowchart LR\n\n    x1[\"x₁\"] --&gt; SUM\n    x2[\"x₂\"] --&gt; SUM\n    x3[\"x₃\"] --&gt; SUM\n\n    SUM[\"z = w₁x₁ + w₂x₂ + w₃x₃ + b\"] --&gt; ACT[\"σ(z)\"]\n    ACT --&gt; y[\"ŷ\"]\n\n\n\n\n\n\n\nWhich approximates a human neuron like shown in Figure 4.2.\n\n\n\n\n\n\nFigure 4.2",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#the-perceptron-as-the-building-block-of-neural-networks",
    "href": "neural_networks.html#the-perceptron-as-the-building-block-of-neural-networks",
    "title": "4  Neural networks and deep learning",
    "section": "4.12 The Perceptron as the Building Block of Neural Networks",
    "text": "4.12 The Perceptron as the Building Block of Neural Networks\nAlthough a single perceptron can only model linear relationships, multiple perceptrons combined in layers can approximate highly complex functions.\nThus, the perceptron serves as the elemental computational unit for:\n\nMultilayer Perceptrons (MLPs) such as the ones used for handwritten digit classification\nConvolutional Neural Networks (CNNs), used in Chest X-ray classification\nRecurrent Neural Networks (RNNs), used in Speech recognition\nTransformers and deep architectures as the ones used in GPT models.\n\nEvery deep learning model ultimately relies on combinations of these simple units performing weighted",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#from-a-single-perceptron-to-a-full-neural-network-the-layered-architecture",
    "href": "neural_networks.html#from-a-single-perceptron-to-a-full-neural-network-the-layered-architecture",
    "title": "4  Neural networks and deep learning",
    "section": "4.13 From a Single Perceptron to a Full Neural Network (The Layered Architecture)",
    "text": "4.13 From a Single Perceptron to a Full Neural Network (The Layered Architecture)\nThe perceptron represents the simplest computational unit of artificial neural networks: it receives multiple inputs, applies a weighted sum, adds a bias, and transforms the result through a nonlinear activation function. Although this single unit is mathematically elegant, its expressive power is limited. A lone perceptron can only model linear decision boundaries; no matter how cleverly it is trained, it cannot capture patterns that require curved surfaces, interacting features, hierarchical structure, or multistage reasoning. To overcome these limitations, neural networks combine many perceptrons into a layered architecture.\nA neural network is constructed by arranging perceptrons into layers, where the output of one layer becomes the input to the next. The first layer operates directly on the raw features and is conventionally called the input layer, although it performs no computation itself. The next layer, composed of multiple perceptrons, is known as a hidden layer. Each perceptron in this layer computes its own nonlinear transformation of the input, producing intermediate representations of the data. The network may contain several such hidden layers, each learning increasingly abstract and complex features. The final layer produces the network’s output, such as a class probability or a continuous prediction. See the next Figure.\n\n\n\n\n\nflowchart LR\n\n    %% ==== INPUT LAYER ====\n    subgraph L1[\"Layer L₁ (Input Layer)\"]\n        x1((\"x₁\"))\n        x2((\"x₂\"))\n        x3((\"x₃\"))\n        b1((\"+1\"))\n    end\n\n    %% ==== HIDDEN LAYER 1 ====\n    subgraph L2[\"Layer L₂\"]\n        h11((\"h₁¹\"))\n        h12((\"h₂¹\"))\n        h13((\"h₃¹\"))\n        b2((\"+1\"))\n    end\n\n    %% ==== HIDDEN LAYER 2 ====\n    subgraph L3[\"Layer L₃\"]\n        h21((\"h₁²\"))\n        h22((\"h₂²\"))\n        b3((\"+1\"))\n    end\n\n    %% ==== OUTPUT LAYER ====\n    subgraph L4[\"Layer L₄ (Output Layer)\"]\n        y1((\"ŷ₁\"))\n        y2((\"ŷ₂\"))\n    end\n\n    %% Connections: L1 → L2\n    x1 --&gt; h11\n    x1 --&gt; h12\n    x1 --&gt; h13\n    x2 --&gt; h11\n    x2 --&gt; h12\n    x2 --&gt; h13\n    x3 --&gt; h11\n    x3 --&gt; h12\n    x3 --&gt; h13\n    b1 --&gt; h11\n    b1 --&gt; h12\n    b1 --&gt; h13\n\n    %% Connections: L2 → L3\n    h11 --&gt; h21\n    h11 --&gt; h22\n    h12 --&gt; h21\n    h12 --&gt; h22\n    h13 --&gt; h21\n    h13 --&gt; h22\n    b2 --&gt; h21\n    b2 --&gt; h22\n\n    %% Connections: L3 → L4\n    h21 --&gt; y1\n    h21 --&gt; y2\n    h22 --&gt; y1\n    h22 --&gt; y2\n    b3 --&gt; y1\n    b3 --&gt; y2\n\n    %% ==== Remove background from subgraphs ====\n    style L1 fill:#ffffff00,stroke:#00000000\n    style L2 fill:#ffffff00,stroke:#00000000\n    style L3 fill:#ffffff00,stroke:#00000000\n    style L4 fill:#ffffff00,stroke:#00000000\n\n    %% ==== Style nodes (white background, circular outlines) ====\n    classDef neuron fill:#ffffff,stroke:#333,stroke-width:1px,color:#000;\n\n    class x1,x2,x3,b1,h11,h12,h13,b2,h21,h22,b3,y1,y2 neuron;\n\n\n\n\n\n\nThis layered composition is essential for the power of neural networks. A single nonlinear transformation is insufficient to approximate rich patterns, but the composition of many nonlinear transformations can represent functions of extraordinary complexity. In the early layers, the network tends to learn simple or local dependencies among the inputs. As information flows forward through successive layers, these low-level computations are combined into higher-level abstractions. This hierarchical arrangement mirrors the structure of many natural phenomena and is one reason why deep learning performs so effectively in fields such as imaging, signal processing, and biomedical prediction.\nMathematically, each layer applies an affine transformation followed by a nonlinear activation. If the input vector is denoted by xxx, the first hidden layer computes an intermediate vector\n\\[h(1)=σ(W(1)x+b(1))h^{(1)} = \\sigma(W^{(1)}x + b^{(1)})h(1)=σ(W(1)x+b(1))\\]. A second layer takes these intermediate values and produces\n\\[h(2)=σ(W(2)h(1)+b(2))h^{(2)} = \\sigma(W^{(2)} h^{(1)} + b^{(2)})h(2)=σ(W(2)h(1)+b(2))\\]\nThis process continues until the output layer generates the final prediction. Without the nonlinear activation \\(σ(⋅)\\sigma(\\cdot)σ(⋅)\\), the entire network would collapse into a single linear transformation, no matter how many layers were added. Nonlinearity therefore plays a structural role in allowing the network to represent functions that curve, twist, or vary in ways that linear models never could.\nThe significance of layered architectures is further supported by the Universal Approximation Theorem, which shows that even a network with only one hidden layer can approximate any continuous function on a compact domain, provided it has a sufficient number of hidden units and an appropriate nonlinear activation. In practice, however, deep networks with multiple layers tend to learn more efficiently, generalize better, and capture hierarchical relationships in a way that shallow networks cannot. Additional depth allows the network to distribute complexity across many layers, enabling compact representations and facilitating learning from high-dimensional data.\nThe transition from a single perceptron to a full neural network is therefore a transition from a simple linear classifier to a flexible, hierarchical system capable of modeling intricate structures in data. This layered design underlies all modern deep learning methods, including multilayer perceptrons for tabular radiomics data, convolutional networks for medical imaging, recurrent architectures for temporal sequences, and transformer-based models for multimodal biomedical analysis. Regardless of the sophistication of the final architecture, the fundamental building block remains the perceptron, replicated and arranged into layers that collectively enable powerful function approximation.\n\n\n\n\n\nflowchart LR\n\nA[\"Raw Pixels&lt;br&gt;(Chest X-ray)\"] --&gt; B[\"Feature Extraction&lt;br&gt;(edges, textures)\"]\n\nB --&gt; C[\"High-level Patterns&lt;br&gt;(lobar opacity, asymmetry)\"]\nC --&gt; D[\"Diagnosis Probability&lt;br&gt;(e.g., Pneumonia)\"]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#deep-learning",
    "href": "neural_networks.html#deep-learning",
    "title": "4  Neural networks and deep learning",
    "section": "4.14 Deep learning",
    "text": "4.14 Deep learning\nDeep networks extend the idea of a simple artificial neuron by stacking many such units across multiple layers, allowing the model to construct representations that become increasingly abstract as information flows forward. In shallow architectures, the network can only capture limited interactions, but depth introduces the ability to disentangle structure, recognize hierarchical relationships, and model patterns that vary at different spatial or temporal scales. This is why modern deep learning is often associated with “representation learning”: instead of hand-crafting features, the network discovers them through layer-by-layer transformations.\nWithin this broad family, several architectural classes have emerged, each tailored to a particular kind of structure in the data. Multilayer Perceptrons are the simplest expression of depth and are effective for tabular biomedical data, where each patient is represented by independent covariates. Convolutional Neural Networks introduce spatial invariance and local receptive fields, making them naturally suited for imaging, where patterns such as edges, textures, or radiological opacities repeat across space. Recurrent architectures, and later Transformers, were originally designed to handle temporal dynamics and long-range dependencies, but they have since expanded far beyond sequence data and now play central roles in multimodal clinical modeling.\nWhat unifies all these architectures is the idea that depth allows the model to progressively refine its internal view of the data,from raw measurements to meaningful abstractions,and ultimately to a prediction. In the context of medical imaging, this capacity is especially important. Radiological patterns rarely exist in isolation; instead, they emerge from subtle combinations of shape, density, context, and anatomical variation. A deep network can first detect local primitives, such as edges or small opacity clusters, then aggregate them into regional descriptors like lobar consolidation or architectural distortion, and finally integrate these into global assessments that approximate expert diagnostic reasoning. This capacity to learn multi-scale, hierarchical structure is precisely what distinguishes deep learning from traditional machine-learning approaches that rely on predefined features.\nAs a result, deep architectures have become central to a wide range of imaging applications: classification of chest X-rays and CT scans, segmentation of tumors or anatomical structures, detection of subtle pathologies that may escape hand-engineered filters, and even synthesis or enhancement of images under limited acquisition conditions. Their strength lies not only in predictive performance, but also in their flexibility: the same underlying principles can be adapted to different modalities, resolutions, and clinical objectives. That is why deep learning has become the dominant paradigm in modern medical imaging,because it aligns naturally with how visual information is structured and how diagnostic interpretation unfolds.\nThis broader perspective sets the stage for the toy dataset we analyze next. By working with a simplified, computationally manageable subset of chest radiographs, we can see how even a small CNN begins to enact this hierarchical processing, transforming raw pixel values into the kinds of radiological abstractions that drive clinical decisions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#from-tables-to-tensors",
    "href": "neural_networks.html#from-tables-to-tensors",
    "title": "4  Neural networks and deep learning",
    "section": "4.15 From Tables to Tensors",
    "text": "4.15 From Tables to Tensors\nBefore diving into images, it is worth pausing to understand what a tensor actually is, especially from the perspective of someone used to working with data frames and matrices in R. Deep learning frameworks operate almost exclusively on tensors, but this terminology can feel foreign when you first encounter it. The underlying idea, however, is not new: tensors simply extend the familiar notion of tables of numbers into additional dimensions. The jump from a data frame to a tensor is more of a gentle generalization than a conceptual leap.\nMost students are already comfortable thinking about vectors and matrices. A vector is just an ordered list of numbers—R calls it a numeric vector—and a matrix is a rectangular table with rows and columns. Both of these objects are examples of tensors: a vector is a tensor of rank 1 and a matrix is a tensor of rank 2. Once this connection is made, the rest follows naturally. A tensor is simply an array that can have any number of dimensions. If you add one more dimension to a matrix, you get a rank-3 tensor; one more after that yields rank-4, and so on. Every time you add another axis, you gain the ability to represent structure that cannot be captured in a flat table—for example, stacks of images, sequences of individual patient records, or volumetric scans such as MRI or CT.\nYou can explore this directly in R. A vector behaves exactly like a one-dimensional tensor:\n\n# Vectors: rank 1\nv &lt;- c(1, 2, 3)\nv\n\n[1] 1 2 3\n\nlength(v)\n\n[1] 3\n\n\nLikewise, a matrix is nothing more than a two-dimensional tensor:\n\n# Matrices: rank 2\nM &lt;- matrix(1:9, nrow = 3, ncol = 3)\nM\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\ndim(M)\n\n[1] 3 3\n\n\nAnd by simply adding a third dimension, R constructs a true tensor:\n\n# 3D arrays: rank 3 tensors\nA &lt;- array(1:24, dim = c(3, 4, 2))\nA\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]   13   16   19   22\n[2,]   14   17   20   23\n[3,]   15   18   21   24\n\ndim(A)\n\n[1] 3 4 2\n\n\nThis small example is enough to demystify the term. Tensors are not strange mathematical objects reserved for physicists or machine-learning specialists; they are the everyday data structures we already manipulate, extended into additional dimensions so that neural networks can operate on them efficiently.\nA helpful way to build intuition for tensors is to start not from patient tables, but from the digital images themselves. An image, no matter the modality—X-ray, dermoscopy, retinal fundus, MRI slice—is fundamentally a collection of numbers arranged in a grid. In R, the simplest representation of a grayscale image is just a matrix, where each entry corresponds to a pixel intensity. When we load a picture into R and convert it to grayscale, we obtain a two-dimensional numerical array: height × width. This makes a grayscale image a rank-2 tensor, directly analogous to a matrix you are already familiar with.\nFor example, let’s simulate a tiny 5×4 “toy image” so the structure is easy to see:\n\n# A tiny grayscale “image” (5 × 4 matrix)\nimg &lt;- matrix(c(\n  10, 20, 30, 40,\n  15, 25, 35, 45,\n  18, 28, 38, 48,\n  22, 32, 42, 52,\n  25, 35, 45, 55\n), nrow = 5, ncol = 4, byrow = TRUE)\n\nimg\n\n     [,1] [,2] [,3] [,4]\n[1,]   10   20   30   40\n[2,]   15   25   35   45\n[3,]   18   28   38   48\n[4,]   22   32   42   52\n[5,]   25   35   45   55\n\ndim(img)   # height × width (rank 2)\n\n[1] 5 4\n\n\nThis is exactly the same as the numerical grid underlying a real chest X-ray—just vastly smaller. If we were working with a standard medical image of size 512×512, the structure would be identical, simply larger:\n(512, 512)  → rank-2 tensor\nWhen an image has color channels, such as RGB dermoscopy or retinal photos, we simply add one more dimension. R represents these as an array where the third axis stores the channels, producing a rank-3 tensor:\n\n# Simulate a tiny RGB image (5 × 4 × 3)\nrgb_img &lt;- array(runif(5*4*3), dim = c(5, 4, 3))\ndim(rgb_img)   # height × width × channels (rank 3)\n\n[1] 5 4 3\n\n\nOnce you have this perspective, the next step for deep learning becomes natural. A neural network never trains on a single image at a time. Instead, images are fed in batches, which introduces yet another dimension. Stacking several images together produces a rank-4 tensor:\n\n# Create a batch of 3 small grayscale images (5 × 4 × 1)\nbatch &lt;- array(rep(img, 3), dim = c(5, 4, 1, 3))\ndim(batch)   # height × width × channels × batch_size\n\n[1] 5 4 1 3\n\n\nThis four-dimensional structure (batch, height, width, channels) is the exact format expected by TensorFlow and Keras when training a CNN.\nThe same logic extends effortlessly to 3D medical data. A CT or MRI scan is simply a stack of slices, so instead of a height × width matrix, you have depth × height × width. Add channels (such as multiple MRI sequences), and then add batching, and a full preprocessing pipeline produces tensors with shapes such as:\n(depth, height, width, channels) (batch, depth, height, width, channels)\nThis is why the term “tensor” appears so often in deep learning. It provides a flexible way to represent data of any dimensionality—2D images, 3D volumes, time sequences, or entire batches of radiological studies. Neural networks take these tensors as input and transform them step by step into new tensors, each layer manipulating the shape to distill increasingly abstract information: edges become patterns, patterns become structures, structures become predictions.\nOnce you see tensors as nothing more than structured extensions of matrices, the workflow in image analysis becomes much easier to understand. A grayscale image is a matrix; a colored image is a matrix with channels; a CT scan is a stack of matrices; a batch of CT scans is a stack of stacks. Deep-learning frameworks operate on these tensors seamlessly, and with a bit of practice, the tensor shapes start feeling just as natural as ordinary tables in R.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#images-as-numerical-data-across-medical-modalities",
    "href": "neural_networks.html#images-as-numerical-data-across-medical-modalities",
    "title": "4  Neural networks and deep learning",
    "section": "4.16 Images as Numerical Data Across Medical Modalities",
    "text": "4.16 Images as Numerical Data Across Medical Modalities\nAlthough medical images appear to us as structured visual objects,chest radiographs, dermoscopy photos, histopathology slides, CT volumes, or MRI scans,deep learning systems treat all of them as nothing more than numerical arrays. Regardless of the imaging modality, every visual element ultimately resolves into grids of values representing either pixel intensities in two dimensions or voxel intensities in three dimensions. Neural networks do not “see” lungs, tumors, organs or textures; they process long, multidimensional tensors.\nThis unifying perspective is key because modern medical imaging spans a wide range of technologies. At the simplest end, we find classical two-dimensional modalities such as chest X-rays, ultrasound, dermoscopy, and retinal fundus photographs. These images are already familiar from everyday clinical practice: X-rays and ultrasound are usually stored as grayscale images with a single intensity channel, while dermoscopic and retinal images are acquired in colour using three channels. In all of these cases, once loaded into a deep learning library, an image becomes a rank-three tensor of shape (height, width, channels). A 512×512 chest radiograph is therefore nothing more than a 512×512×1 cube of numbers whose values range from dark to bright depending on the local tissue density.\nHistopathology slides and fluorescence microscopy introduce additional complexity because they frequently contain multiple biochemical stains or multi-channel fluorescence signals. Even though the images can be extremely large,gigapixel whole-slide images are common,they still resolve into two-dimensional grids, often split into smaller patches for analysis. Each patch is represented numerically by arrays such as (224, 224, 3) for standard H&E staining or (224, 224, 5) for multi-spectral microscopy. Deep learning models can thus treat microscopy and pathology in the same numerical framework as chest X-rays, differing only in the number of channels.\nMore advanced imaging modalities bring depth into the picture. CT, MRI, and PET scanners generate full three-dimensional volumes composed of stacked slices. A CT scan might contain sixty or more axial slices, each 512×512 pixels, producing a volumetric array with shape (depth, height, width). Multi-contrast MRI extends this further by including several sequences,T1, T2, FLAIR, DWI,each acting as a distinct channel. These become rank-four tensors, such as (depth, height, width, channels), which are analysed using 3D convolutional networks or hybrid architectures.\nCertain modalities even add a temporal dimension, such as ultrasound cine loops or cardiac MRI sequences. These datasets are represented as tensors like (time, height, width, channels), effectively creating four- or five-dimensional numerical structures. Deep learning architectures built for video analysis,ConvLSTMs, temporal CNNs, or vision transformers,operate naturally on these higher-rank tensors.\nThe unifying thread across all modalities is that pixel or voxel values serve as the raw features. A neural network does not need explicit annotations describing edges, shapes, textures, or pathological patterns because convolutional filters extract these representations automatically. The data themselves, expressed as tensors, contain everything the model needs to discover hierarchical structure: from local gradients and edges, to regional consolidation patterns, to whole-organ abnormalities.\nThis is one reason why the MedMNIST project Yang et al. (2023) is so pedagogically useful. MedMNIST provides a collection of small, preprocessed medical imaging datasets that represent many of these modalities,X-rays, retinal images, dermoscopy photographs, histopathology tiles, and even 3D volumetric scans,while standardizing them to uniform tensor shapes. ChestMNIST, for instance, offers downsampled chest radiographs as grayscale arrays. PathMNIST provides histopathology tiles as color images. OrganMNIST3D and NoduleMNIST3D present small volumetric CT-like cubes as rank-three tensors. Despite covering different anatomical regions, acquisition methods, and clinical purposes, the datasets are interchangeable at the tensor level, which makes them ideal for teaching.\nUnderstanding that all medical images ultimately become tensors prepares us to move into model construction. When we load a chest X-ray for the pneumonia example in this chapter, we are simply converting pixel intensities into a tensor of shape (128, 128, 1). When we feed a batch of images to a convolutional neural network, we are stacking these tensors into a rank-four structure. As the image flows through the layers of the network, it is repeatedly transformed,convolved, pooled, flattened,into new tensors whose shapes reflect the abstraction level of the learned representations. This tensor-centric perspective allows us to navigate seamlessly from radiographs to MRI volumes and from single images to complex medical datasets.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#using-pretrained-architectures-for-medical-imaging",
    "href": "neural_networks.html#using-pretrained-architectures-for-medical-imaging",
    "title": "4  Neural networks and deep learning",
    "section": "4.17 Using Pretrained Architectures for Medical Imaging",
    "text": "4.17 Using Pretrained Architectures for Medical Imaging\nIn many practical situations, we do not need to design a convolutional neural network from scratch. The computer-vision community has spent more than a decade refining deep architectures capable of extracting hierarchical features from images—from edges and textures in early layers to objects and high-level semantics in deeper layers. These models, many of them trained on ImageNet, learn general-purpose visual representations that transfer surprisingly well to medical imaging tasks, even when the target domain differs substantially from the original training data.\nWhen working with chest radiographs, histopathology tiles, dermoscopy photos or retinal fundus images, pretrained networks often dramatically improve performance relative to small, hand-crafted architectures—especially in small datasets where training a large network from scratch would lead to overfitting. A pretrained CNN already “knows” how to detect edges, textures, gradients, and shape patterns. Fine-tuning such a model therefore requires fewer labelled medical images, converges faster, and typically yields higher accuracy.\nIn this chapter, after introducing the basic building blocks of CNNs, we compare four models of increasing complexity:\nTinyCNN – our handcrafted, minimal architecture operating directly on grey-scale tensors.\nResNet-18 – a classical backbone using residual skip connections He et al. (2016).\nResNet-50 – a deeper and more expressive residual network, also defined in He et al. (2016).\nEfficientNet-B0 – a more modern architecture that systematically scales depth, width, and resolution Tan and Le (2019).\nAlthough TinyCNN is intentionally simple, the remaining models reflect decades of architectural development and are widely used across radiology, dermatology, ophthalmology, and digital pathology. Their success in medical tasks demonstrates how transferable large-scale visual representations can be.\nTo illustrate this, we evaluate each architecture on PneumoniaMNIST, a lightweight version of the pediatric chest X-ray dataset originally curated by Paul Mooney Mooney (2018) and standardized in the MedMNIST benchmark suite Yang et al. (2021); Yang et al. (2023). For ResNet and EfficientNet, we convert the grayscale images to RGB to match the pretrained input format, replace the final classification layer with a single output neuron, and compute accuracy and confusion matrices.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#toy-data-for-the-chapter",
    "href": "neural_networks.html#toy-data-for-the-chapter",
    "title": "4  Neural networks and deep learning",
    "section": "4.18 Toy Data for the chapter",
    "text": "4.18 Toy Data for the chapter\nThe first datasert we will use this chapter we will is a curated subset of the Chest X-Ray Pneumonia dataset, originally released on Kaggle by Mooney (2018). The full dataset contains more than 5,000 radiographs with resolutions up to 1024×1024 pixels, distributed across normal and pneumonia classes. While this dataset is ideal for clinical–motivated deep learning, its size makes it challenging for users running models on standard laptops.\nTo address this, we prepared a toy version of the dataset, containing a balanced and substantially smaller sample of images (250 per class). The toy dataset keeps two copies of each selected image:\n\nOriginal image: stored exactly as released (high resolution, variable size).\nCompressed image: resized to a fixed resolution (e.g., 512×512), suitable for CNN training on typical laptops.\n\n\n4.18.1 Image compression\nRaw medical images particularly chest X-rays, CT slices, and MRI scans are often large (1–5 MB per image). Processing them directly can make training:\n\n\nslow, due to large tensor operations,\nmemory-intensive, particularly on laptops without GPUs,\ncomputationally expensive, since convolution scales with image size.\n\nA 1024×1024 radiograph contains four times more pixels than a 512×512 version and sixteen times more than 256×256. Convolutional filters must process each pixel, so reducing resolution leads to substantial gains in training speed with minimal loss of diagnostic signal for tasks like pneumonia detection. For these reasons, the toy dataset includes paired images: the untouched originals and the pre-processed, standardized 512×512 versions. Students can explore the visual differences and appreciate that, although details are reduced, the core diagnostic structures (e.g., opacities, texture patterns) remain visible.\nThe following code allow us to read the data and plot some images to have a glimpse of what the data look like\n\nlibrary(magick)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(grid)\nlibrary(tools)\n\n# ---------------------------------------------------------\n# Paths\n# ---------------------------------------------------------\ntoy_base    &lt;- \"data/chest_toy_250\"\norig_dir    &lt;- file.path(toy_base, \"original\")\nresized_dir &lt;- file.path(toy_base, \"resized\")\n\n# ---------------------------------------------------------\n# Helper: extract class from filename\n# ORIGINAL files look like:  NORMAL_xxx.jpeg  or  PNEUMONIA_xxx.jpeg\n# ---------------------------------------------------------\nget_class &lt;- function(x) {\n  toupper(strsplit(basename(x), \"_\")[[1]][1])\n}\n\n# ---------------------------------------------------------\n# List and classify ORIGINAL images\n# ---------------------------------------------------------\norig_files &lt;- list.files(\n  orig_dir, full.names = TRUE,\n  pattern = \"\\\\.(jpg|jpeg|png)$\"\n)\n\norig_classes &lt;- sapply(orig_files, get_class)\n\n# ---------------------------------------------------------\n# Sample exactly 2 NORMAL and 2 PNEUMONIA\n# ---------------------------------------------------------\norig_norm &lt;- orig_files[orig_classes == \"NORMAL\"]\norig_pneu &lt;- orig_files[orig_classes == \"PNEUMONIA\"]\n\nset.seed(123)\norig_selected &lt;- c(sample(orig_norm, 2), sample(orig_pneu, 2))\n\n# ---------------------------------------------------------\n# Match each selected ORIGINAL to its resized version\n# Resized names look like: NORMAL_xxx_resized.png\n# ---------------------------------------------------------\nres_selected &lt;- sapply(orig_selected, function(f) {\n  stem &lt;- file_path_sans_ext(basename(f))\n  file.path(resized_dir, paste0(stem, \"_resized.png\"))\n})\n\n# ---------------------------------------------------------\n# Load images\n# ---------------------------------------------------------\norig_imgs &lt;- lapply(orig_selected, image_read)\nres_imgs  &lt;- lapply(res_selected, image_read)\n\n# Build titles based on class\ntitles_orig &lt;- sapply(orig_selected, function(f) paste0(get_class(f), \" (Original)\"))\ntitles_res  &lt;- sapply(orig_selected, function(f) paste0(get_class(f), \" (Resized 512×512)\"))\n\n# ---------------------------------------------------------\n# Convert to ggplot grobs\n# ---------------------------------------------------------\nmake_plot &lt;- function(img, title) {\n  ggplot() +\n    annotation_custom(rasterGrob(img)) +\n    ggtitle(title) +\n    theme_void()\n}\n\nplot_list &lt;- list()\n\n# First row: originals\nfor (i in seq_along(orig_imgs)) {\n  plot_list[[length(plot_list)+1]] &lt;- make_plot(orig_imgs[[i]], titles_orig[i])\n}\n\n# Second row: resized\nfor (i in seq_along(res_imgs)) {\n  plot_list[[length(plot_list)+1]] &lt;- make_plot(res_imgs[[i]], titles_res[i])\n}\n\n# ---------------------------------------------------------\n# Plot: 2 rows × 4 columns\n# ---------------------------------------------------------\np=grid.arrange(grobs = plot_list, ncol = 4)\nggsave(\"figures/toy_chest_comparison.png\",\n       plot = p,\n       width = 12, height = 7,\n       dpi = 300,\n       bg = \"white\")\n\n\n\nThe code shown above selects a balanced subset of chest radiographs and prepares two parallel versions of each image. The first retains the original resolution of the dataset, while the second produces a standardized 512×512 pixel version. This allows us to examine the effect of image compression while keeping the resized images computationally manageable for downstream modeling. The script identifies matching pairs of images for both NORMAL and PNEUMONIA categories loads them with the magick library, and arranges them in a grid where the original images appear in the first row and their compressed counterparts appear directly beneath them.\nBecause the resized images preserve lung structure and overall radiological patterns, we can directly assess the trade-off between image fidelity and computational efficiency. Even after downsampling, the essential visual features remain recognizable, which is a central motivation for preprocessing high-resolution medical images before training predictive models. We will no understand this with Figure 4.3.\n\n\n\n\n\n\nFigure 4.3\n\n\n\n\n\n4.18.2 What the figures show\nWhen examining Figure 4.3, we can see that normal pediatric chest radiographs have relatively uniform, darker lung fields, reflecting the low density of air-filled alveoli. The pulmonary vessels appear as fine branching structures, the diaphragmatic borders remain sharp, and the cardiac silhouette stands out clearly against the surrounding aerated lung. There is a sense of symmetry and well-defined anatomical boundaries.\nIn contrast, the pneumonia images display areas of increased opacity regions that appear whiter or denser corresponding to alveoli filled with inflammatory material rather than air. We can see patchy or segmental consolidations that disrupt the uniform darkness of the lungs, obscure normal vascular markings, or blur the heart borders. These opacities may also distort the expected symmetry between the left and right lung fields.\nWhen comparing the original and resized images, we can see that the downscaled versions still retain the broad radiographic patterns that distinguish normal lungs from those affected by pneumonia. The fine-grained textures are somewhat softened, but the structural changes associated with consolidation remain visible. This illustrates why compressed images often remain adequate for classification tasks: the key diagnostic features are large-scale density patterns, which are preserved even after substantial reduction in image resolution. The paired display therefore highlights both the radiological differences between normal and pneumonia cases and the practical effect of image compression within a machine-learning workflow.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#fitting-a-naive-neural-network-for-prediction-of-pneumonia",
    "href": "neural_networks.html#fitting-a-naive-neural-network-for-prediction-of-pneumonia",
    "title": "4  Neural networks and deep learning",
    "section": "4.19 Fitting a naive neural network for prediction of pneumonia",
    "text": "4.19 Fitting a naive neural network for prediction of pneumonia\nTo demonstrate the mechanics of training a convolutional neural network (CNN), we now walk through a complete example using a toy subset of chest X-ray images. The objective is deliberately modest: the network is not intended to achieve clinical performance, but rather to illustrate the full computational workflow that takes us from pixel data to probability predictions.\nAt a high level, the modelling pipeline follows four steps: loading and labelling the images, converting each image into a numerical tensor, defining a minimal CNN architecture, and training the network using gradient-based optimisation. Even a very small model reveals the essential building blocks of deep learning for image analysis.\nData preparation All images in the pre-processed folder are listed and labelled according to their filenames, which contain either NORMAL or PNEUMONIA. The dataset is then shuffled and partitioned into training \\((70 \\%)\\), validation \\((15 \\%)\\), and test \\((15 \\%)\\). Because neural networks operate on numerical arrays, each image is loaded from disk, converted to grayscale, resized to \\(128 \\times 128\\) pixels, and normalised to the \\([0,1]\\) range. The resulting tensor has the shape\n\\[\n(128,128,1),\n\\]\ncorresponding to height, width, and channel count. These arrays form the inputs for Keras/TensorFlow.\nModel architecture The CNN itself is intentionally small. It begins with an explicit input layer\n\\[\nX \\in \\mathbb{R}^{128 \\times 128 \\times 1},\n\\]\nfollowed by two convolution-ReLU-pooling blocks. Convolutional layers learn spatial filters that respond to edges, textures, and coarse structures in the X-ray images, while max-pooling progressively reduces spatial resolution:\n\\[\n\\text { image → conv } \\text { → ReLU } \\text { → pool. }\n\\]\nAfter two rounds of convolution and pooling, the resulting activation maps are flattened and passed into dense layers:\n\\[\n\\text { Flatten → Dense(32) } \\rightarrow \\text { Dense(1). }\n\\]\nThe final layer uses a sigmoid activation, producing an estimated probability\n\\[\n\\hat{p}=P(\\text { pneumonia } \\mid X) .\n\\]\nAlthough compact, this network captures all core components of modern CNNs: convolution, nonlinearity, down-sampling, and fully connected classification.\nTraining the network\nThe model is trained using the Adam optimiser with a learning rate of \\(5 \\times 10^{-4}\\). We use binary crossentropy as the loss function:\n\\[\n\\mathcal{L}=-[y \\log (\\hat{p})+(1-y) \\log (1-\\hat{p})] .\n\\]\nDuring training, the network repeatedly performs forward passes to compute predictions, evaluates the loss, and uses backpropagation to update its filter weights. Validation accuracy and loss are monitored after each epoch to gauge generalisation.\nEven with this very small architecture and a tiny dataset, the training loop performs thousands of tensor operations. Students should be aware that deep learning workloads scale quickly and can become computationally heavy without GPU acceleration.\nModel performance After ten epochs, the model reaches good performance on both the training and test sets:\n\nTraining accuracy: 0.924\nTraining loss: 0.219\nTest accuracy: 0.908\nTest loss: 0.226\n\nThe confusion matrix on the held-out test set is:\n\n\n\ntruth  predicted\n0\n1\n\n\n\n\n0 (NORMAL)\n59\n10\n\n\n1 (PNEUMONIA)\n3\n69\n\n\n\nThe model detects pneumonia well (few false negatives), while still misclassifying a moderate number of NORMAL images as pneumonia (false positives). This behaviour is typical for small CNNs trained on imbalanced or limited datasets, and it highlights an essential lesson: even when accuracy seems high, practical clinical reliability requires much more data, better preprocessing, stronger architectures, and rigorous evaluation.\nSaving the model\nOnce training is complete, the model and all diagnostics (training history, predictions, metrics) are saved to disk. Saving models is a crucial habit in any deep-learning workflow, as training may take considerable time and should not be repeated unnecessarily. Stored models can be reloaded instantly for future experiments, teaching demonstrations, or interpretability analyses.\n\nlibrary(reticulate)\nuse_condaenv(\"r-keras\", required = TRUE)\n\nlibrary(tensorflow)\nlibrary(keras)\nlibrary(magick)\nlibrary(dplyr)\n\nimg_dir &lt;- \"data/chest_toy_250/resized\"\nfiles &lt;- list.files(img_dir, full.names = TRUE, pattern = \"png$\")\n\nget_label &lt;- function(x) {\n  if (grepl(\"PNEUMONIA\", basename(x), ignore.case = TRUE)) 1 else 0\n}\n\ndf &lt;- data.frame(\n  file = files,\n  label = sapply(files, get_label)\n)\n\nset.seed(123)\ndf &lt;- df[sample(nrow(df)), ]\n\nn &lt;- nrow(df)\ntrain_df &lt;- df[1:floor(0.7*n), ]\nval_df   &lt;- df[(floor(0.7*n)+1):floor(0.85*n), ]\ntest_df  &lt;- df[(floor(0.85*n)+1):n, ]\n\nload_array &lt;- function(path) {\n  img &lt;- image_read(path)\n  img &lt;- image_convert(img, colorspace = \"gray\")\n  img &lt;- image_resize(img, \"128x128!\")\n  arr &lt;- as.numeric(img[[1]]) / 255\n  array(arr, dim = c(128, 128, 1))\n}\n\nx_train &lt;- array(0, dim = c(nrow(train_df), 128, 128, 1))\nx_val   &lt;- array(0, dim = c(nrow(val_df),   128, 128, 1))\nx_test  &lt;- array(0, dim = c(nrow(test_df),  128, 128, 1))\n\nfor (i in seq_len(nrow(train_df))) x_train[i,,,] &lt;- load_array(train_df$file[i])\nfor (i in seq_len(nrow(val_df)))   x_val[i,,,]   &lt;- load_array(val_df$file[i])\nfor (i in seq_len(nrow(test_df)))  x_test[i,,,]  &lt;- load_array(test_df$file[i])\n\ny_train &lt;- train_df$label\ny_val   &lt;- val_df$label\ny_test  &lt;- test_df$label\n\nbuild_model &lt;- function() {\n  inputs &lt;- layer_input(shape = c(128L,128L,1L))\n  x &lt;- inputs %&gt;%\n    layer_conv_2d(filters = 8, kernel_size = 3, activation = \"relu\", padding = \"same\") %&gt;%\n    layer_max_pooling_2d(pool_size = 2) %&gt;%\n    layer_conv_2d(filters = 16, kernel_size = 3, activation = \"relu\", padding = \"same\") %&gt;%\n    layer_max_pooling_2d(pool_size = 2) %&gt;%\n    layer_flatten() %&gt;%\n    layer_dense(units = 32, activation = \"relu\") %&gt;%\n    layer_dense(units = 1, activation = \"sigmoid\")\n  model &lt;- keras_model(inputs = inputs, outputs = x)\n  model %&gt;% compile(\n    optimizer = optimizer_adam(learning_rate = 0.0005),\n    loss = \"binary_crossentropy\",\n    metrics = c(\"accuracy\")\n  )\n  model\n}\n\nmodel &lt;- build_model()\n\nhistory &lt;- model %&gt;%\n  fit(\n    x_train, y_train,\n    validation_data = list(x_val, y_val),\n    epochs = 10,\n    batch_size = 8\n  )\n\nmetrics_train &lt;- model %&gt;% evaluate(x_train, y_train)\nprint(metrics_train)\n\n\n\nmetrics_test &lt;- model %&gt;% evaluate(x_test, y_test)\nprint(metrics_test)\n\npred_test &lt;- model %&gt;% predict(x_test)\npred_class &lt;- ifelse(pred_test &gt; 0.5, 1, 0)\nprint(table(truth = y_test, predicted = pred_class))\n\nsaveRDS(history, \"cnn_history.rds\")\nsaveRDS(metrics_test, \"cnn_metrics_test.rds\")\nsaveRDS(pred_test, \"cnn_predictions_test.rds\")\nsaveRDS(metrics_train, \"cnn_metrics_train.rds\")\nmodel$save(\"cnn_pneumonia.keras\")\n\n\n4.19.1 By with a little from Python\nAlongside the R-based workflow, it is useful to illustrate how the same modeling pipeline can be implemented in Python using PyTorch. The script below trains several neural-network architectures on the PneumoniaMNIST dataset and evaluates their performance under a unified interface. While the code is not executed inside the textbook, it provides a complete and reproducible reference for readers who want to explore the models in Python.\nThe workflow begins by loading the dataset through MedMNIST, a lightweight benchmark collection for medical image classification. Images are provided as 28×28 grayscale tensors with binary labels (normal vs pneumonia). Because different neural-network architectures expect different input shapes, the script includes dataset wrappers that optionally convert grayscale images to RGB and resize them when required (e.g., for ResNet-50 and EfficientNet-B0).\nModel specification relies on PyTorch and torchvision. Four architectures are included:\n\nTinyCNN – a simple two-layer convolutional network designed to illustrate baseline performance.\nResNet-18 – a residual architecture that uses skip connections to stabilise training in deeper models.\nResNet-50 – a larger residual network with bottleneck blocks and substantially more capacity.\nEfficientNet-B0 – a modern architecture that applies compound scaling to depth, width, and resolution.\n\nEach model is adapted to binary classification by replacing the final fully-connected layer with a single output neuron. All models are trained with binary cross-entropy (via BCEWithLogitsLoss) and optimised with Adam. The training loops share the same structure: enable training mode, iterate through mini-batches, compute forward and backward passes, and update the parameters. After training, model weights are saved to disk to ensure that the evaluation script can reload them later without retraining.\nEvaluation uses a common helper function that computes predictions, applies a sigmoid threshold at 0.5, and returns both accuracy and a confusion matrix. These metrics allow a like-for-like comparison across architectures. A small utility function based on matplotlib and seaborn visualises the confusion matrices.\nStructurally, the script demonstrates a full supervised-learning pipeline:\n\ndataset loading and preprocessing\nmodel definition and customisation\ntraining using stochastic gradient descent methods\ninference and metric computation\npersistence of trained weights\nvisualisation of performance diagnostics\n\nThe Python implementation complements the R material by showing how deep-learning models are typically organised in PyTorch workflows, how pretrained architectures can be fine-tuned for medical imaging tasks, and how performance varies across models of different capacities.\n\n\n4.19.2 A short note on PyTorch\nThroughout the examples in this chapter we rely on PyTorch, one of the most widely used open-source frameworks for deep learning. PyTorch is built around a simple idea: treat tensors as first-class numerical objects and allow models to be expressed as ordinary Python code. This makes experimentation fast and transparent—layers, losses, and optimizers behave like regular Python objects that can be inspected, modified, or replaced.\nPyTorch offers:\nDynamic computation graphs (“define-by-run”), which evaluate operations as they are executed. This style is particularly intuitive when building or debugging neural networks.\nA rich library of modules for convolutional layers, normalization, pooling, activation functions, and loss functions. Most architectures used in modern computer vision can be reproduced with a handful of lines.\nGPU acceleration through CUDA integration, allowing models to scale from small educational datasets (like PneumoniaMNIST) to clinical-grade imaging repositories.\nTorchVision, a companion package providing pretrained models (ResNet, EfficientNet, DenseNet, ViT), image transformations, and standardized evaluation pipelines.\nDataLoader abstractions, which handle batching, shuffling, and parallel data fetching. For image-based pipelines this is essential: models rarely process images one at a time, and appropriately designed loaders ensure smooth throughput.\nIn the comparison code above, PyTorch orchestrates every step: loading and batching images, defining lightweight (TinyCNN) and heavyweight (ResNet, EfficientNet) networks, applying loss functions, optimizing parameters, and finally evaluating predictive performance. Even when the architecture becomes complex, the overall workflow remains the same: tensors in, tensors out, gradients computed automatically, and model weights updated by optimizers.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport medmnist\nfrom medmnist import INFO\n\n# ------------------------------------------------------------\n# Load PneumoniaMNIST (binary labels)\n# ------------------------------------------------------------\ndata_flag = \"pneumoniamnist\"\ninfo = INFO[data_flag]\nDataClass = getattr(medmnist, info['python_class'])\n\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\ntrain_ds = DataClass(split=\"train\", download=True, transform=transform)\ntest_ds  = DataClass(split=\"test\",  download=True, transform=transform)\n\n# ------------------------------------------------------------\n# Safe label extraction (no deprecation warnings)\n# ------------------------------------------------------------\ndef extract_scalar(lbl):\n    # lbl may be array([0]) or array([1])\n    # This safely extracts the single value no matter the dtype\n    return float(lbl.squeeze().item())\n\ntrain_x = torch.stack([img for img, lbl in train_ds])\ntrain_y = torch.tensor([extract_scalar(lbl) for _, lbl in train_ds])\n\ntest_x = torch.stack([img for img, lbl in test_ds])\ntest_y = torch.tensor([extract_scalar(lbl) for _, lbl in test_ds])\n\n\nclass PneumoniaBinary(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\n\ntrain_loader = DataLoader(PneumoniaBinary(train_x, train_y), batch_size=64, shuffle=True)\ntest_loader  = DataLoader(PneumoniaBinary(test_x, test_y), batch_size=64, shuffle=False)\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, models\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nimport medmnist\nfrom medmnist import INFO\n\nimport numpy as np\nimport matplotlib.pyplot as plt\ndata_flag = \"pneumoniamnist\"\ninfo = INFO[data_flag]\nDataClass = getattr(medmnist, info['python_class'])\n\ntransform = transforms.Compose([\n    transforms.ToTensor()\n])\n\ntrain_ds = DataClass(split=\"train\", download=True, transform=transform)\ntest_ds  = DataClass(split=\"test\",  download=True, transform=transform)\n\n# FIX LABELS: PneumoniaMNIST returns a dict-like label, we extract scalar\ntrain_x = torch.stack([img for img, lbl in train_ds])\ntrain_y = torch.tensor([int(lbl) for _, lbl in train_ds]).float()\n\ntest_x = torch.stack([img for img, lbl in test_ds])\ntest_y = torch.tensor([int(lbl) for _, lbl in test_ds]).float()\n\nclass PneumoniaBinary(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntrain_loader = DataLoader(PneumoniaBinary(train_x, train_y), batch_size=64, shuffle=True)\ntest_loader  = DataLoader(PneumoniaBinary(test_x, test_y), batch_size=64, shuffle=False)\n\nprint(\"Train:\", len(train_loader.dataset))\nprint(\"Test:\", len(test_loader.dataset))\n\n\n\n\nclass TinyCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # 28→14→7 (MedMNIST images are 28x28)\n        self.fc1 = nn.Linear(32 * 7 * 7, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = x.view(-1, 32 * 7 * 7)\n        x = torch.relu(self.fc1(x))\n        return torch.sigmoid(self.fc2(x))\n\ntiny = TinyCNN()\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(tiny.parameters(), lr=0.001)\n\nepochs = 5\nprint(\"Training Tiny CNN...\\n\")\n\nfor epoch in range(epochs):\n    tiny.train()\n    total_loss = 0\n\n    loop = tqdm(train_loader, desc=f\"TinyCNN Epoch {epoch+1}/{epochs}\", ncols=100)\n    for images, labels in loop:\n        labels = labels.unsqueeze(1)\n\n        optimizer.zero_grad()\n        preds = tiny(images)\n        loss = criterion(preds, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Epoch {epoch+1} Mean Loss: {total_loss/len(train_loader):.4f}\")\n\n\n\ntiny.eval()\ncorrect, total = 0, 0\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        labels = labels.unsqueeze(1)\n        preds = (tiny(images) &gt; 0.5).float()\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\ntiny_acc = correct / total\nprint(f\"Tiny CNN Test Accuracy: {tiny_acc:.4f}\")\ntorch.save(tiny.state_dict(), \"tinycnn_pneumonia.pt\")\nprint(\"TinyCNN saved!\")\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import models\nfrom tqdm import tqdm\n\n# -------------------------\n# Convert grayscale → RGB\n# -------------------------\ndef rgb_transform(x):\n    # x = (batch, 1, 28, 28)\n    return x.repeat(1, 3, 1, 1)\n\n# Wrap loaders\ndef convert_to_rgb(loader):\n    for imgs, lbls in loader:\n        yield rgb_transform(imgs), lbls.unsqueeze(1)\n        \ntrain_rgb = convert_to_rgb(train_loader)\ntest_rgb  = convert_to_rgb(test_loader)\n\n# -------------------------\n# Load ResNet-18 pretrained\n# -------------------------\nresnet18 = models.resnet18(weights=\"DEFAULT\")\nresnet18.fc = nn.Linear(resnet18.fc.in_features, 1)\n\ncriterion_res = nn.BCEWithLogitsLoss()\noptimizer_res = optim.Adam(resnet18.parameters(), lr=0.0005)\n\n# -------------------------\n# Train ResNet-18\n# -------------------------\nepochs = 3  # use fewer epochs to keep fast\n\nprint(\"\\nTraining ResNet-18...\\n\")\nfor epoch in range(epochs):\n    resnet18.train()\n    total_loss = 0\n\n    loop = tqdm(train_rgb, desc=f\"ResNet18 Epoch {epoch+1}/{epochs}\", ncols=100)\n    for images, labels in loop:\n        optimizer_res.zero_grad()\n        preds = resnet18(images)\n        loss = criterion_res(preds, labels)\n        loss.backward()\n        optimizer_res.step()\n\n        total_loss += loss.item()\n        loop.set_postfix(loss=float(loss))\n\n    print(f\"Epoch {epoch+1} Mean Loss: {total_loss:.4f}\")\n\n# -------------------------\n# Evaluate ResNet-18\n# -------------------------\nresnet18.eval()\ncorrect, total = 0, 0\n\nwith torch.no_grad():\n    for images, labels in test_rgb:\n        preds = (torch.sigmoid(resnet18(images)) &gt; 0.5).float()\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\nresnet18_acc = correct / total\nprint(f\"\\nResNet-18 Test Accuracy: {resnet18_acc:.4f}\\n\")\n\n\n# -----------------------\n# Save model\n# -----------------------\ntorch.save(resnet18.state_dict(), \"resnet18_pneumonia.pt\")\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, models\nfrom tqdm import tqdm\n\n# ============================================================\n# 0. Fix: Resize + convert grayscale → RGB for ResNet-50\n# ============================================================\n\nresize_to_224 = transforms.Resize((224, 224))\n\ndef rgb_transform(batch):\n    # batch: [B, 1, 28, 28]\n    batch = resize_to_224(batch)          # -&gt; [B, 1, 224, 224]\n    return batch.repeat(1, 3, 1, 1)       # -&gt; [B, 3, 224, 224]\n\nclass RGBLoader:\n    \"\"\"Wraps an existing DataLoader and yields resized RGB tensors\"\"\"\n    def __init__(self, loader):\n        self.loader = loader\n\n    def __iter__(self):\n        for imgs, lbls in self.loader:\n            rgb_imgs = rgb_transform(imgs)\n            yield rgb_imgs, lbls.unsqueeze(1)\n\n    def __len__(self):\n        return len(self.loader)\n\n# Create RGB loaders\ntrain_rgb = RGBLoader(train_loader)\ntest_rgb  = RGBLoader(test_loader)\n\n# ============================================================\n# 1. Build ResNet-50 binary classifier\n# ============================================================\nresnet50 = models.resnet50(weights=\"DEFAULT\")\nresnet50.fc = nn.Linear(resnet50.fc.in_features, 1)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer_res50 = optim.Adam(resnet50.parameters(), lr=5e-4)\n\n# ============================================================\n# 2. Train ResNet-50\n# ============================================================\nepochs = 5\nprint(\"\\nTraining ResNet-50...\\n\")\n\nfor epoch in range(epochs):\n    resnet50.train()\n    total_loss = 0\n\n    loop = tqdm(train_rgb, desc=f\"ResNet-50 Epoch {epoch+1}/{epochs}\", ncols=100)\n\n    for images, labels in loop:\n        optimizer_res50.zero_grad()\n        preds = resnet50(images)\n        loss = criterion(preds, labels)\n        loss.backward()\n        optimizer_res50.step()\n\n        total_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Epoch {epoch+1} Loss: {total_loss/len(train_rgb):.4f}\")\n\n# ============================================================\n# 3. Evaluate ResNet-50\n# ============================================================\nresnet50.eval()\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for images, labels in test_rgb:\n        logits = resnet50(images)\n        preds = (torch.sigmoid(logits) &gt; 0.5).float()\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\nresnet50_acc = correct / total\nprint(f\"\\nResNet-50 Accuracy: {resnet50_acc:.4f}\")\n\n# ============================================================\n# 4. Save model\n# ============================================================\ntorch.save(resnet50.state_dict(), \"resnet50_pneumonia.pt\")\nprint(\"Saved: resnet50_pneumonia.pt\")\n\n\n# ------------------------------------------------------------\n# EfficientNet-B0 for PneumoniaMNIST\n# ------------------------------------------------------------\nfrom torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n\nprint(\"\\nLoading EfficientNet-B0...\")\n\n# Load pretrained model\nefficientnet = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n\n# Replace classifier for binary output\nefficientnet.classifier[1] = nn.Linear(\n    efficientnet.classifier[1].in_features, 1\n)\n\ncriterion = nn.BCEWithLogitsLoss()\noptimizer_eff = optim.Adam(efficientnet.parameters(), lr=5e-4)\n\nepochs = 5\nprint(\"\\nTraining EfficientNet-B0...\\n\")\n\nfor epoch in range(epochs):\n    efficientnet.train()\n    total_loss = 0\n\n    loop = tqdm(train_rgb, desc=f\"EfficientNet-B0 Epoch {epoch+1}/{epochs}\", ncols=100)\n    \n    for images, labels in loop:\n        optimizer_eff.zero_grad()\n\n        preds = efficientnet(images)\n        loss = criterion(preds, labels)\n\n        loss.backward()\n        optimizer_eff.step()\n\n        total_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Epoch {epoch+1} Mean Loss: {total_loss/len(train_rgb):.4f}\")\n\n\n# ------------------------------------------------------------\n# Evaluation\n# ------------------------------------------------------------\nefficientnet.eval()\ncorrect, total = 0, 0\n\nwith torch.no_grad():\n    for images, labels in test_rgb:\n        preds = torch.sigmoid(efficientnet(images))\n        preds = (preds &gt; 0.5).float()\n\n        correct += (preds == labels).sum().item()\n        total += labels.size(0)\n\neff_acc = correct / total\nprint(f\"\\nEfficientNet-B0 Test Accuracy: {eff_acc:.4f}\")\n\n\n# ------------------------------------------------------------\n# Save model\n# ------------------------------------------------------------\ntorch.save(efficientnet.state_dict(), \"efficientnet_pneumonia.pt\")\nprint(\"\\nModel saved as efficientnet_pneumonia.pt\")\nimport torch\nimport torch.nn as nn\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nimport numpy as np\n\n# ---------------------------\n# Reload dataset (same as before)\n# ---------------------------\ndef load_pneumonia_mnist():\n    import medmnist\n    from medmnist import INFO\n\n    data_flag = \"pneumoniamnist\"\n    info = INFO[data_flag]\n    DataClass = getattr(medmnist, info['python_class'])\n\n    transform = transforms.Compose([transforms.ToTensor()])\n\n    train_ds = DataClass(split=\"train\", download=True, transform=transform)\n    test_ds  = DataClass(split=\"test\",  download=True, transform=transform)\n\n    X_train = torch.stack([img for img, lbl in train_ds])\n    y_train = torch.tensor([int(lbl) for _, lbl in train_ds]).float()\n\n    X_test  = torch.stack([img for img, lbl in test_ds])\n    y_test  = torch.tensor([int(lbl) for _, lbl in test_ds]).float()\n\n    return X_train, y_train, X_test, y_test\n\nX_train, y_train, X_test, y_test = load_pneumonia_mnist()\n\nclass PneumoniaDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntest_loader = DataLoader(PneumoniaDataset(X_test, y_test), batch_size=64, shuffle=False)\n\n# ---------------------------\n# TinyCNN model definition\n# ---------------------------\nclass TinyCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(32 * 7 * 7, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = x.view(-1, 32*7*7)\n        x = torch.relu(self.fc1(x))\n        return torch.sigmoid(self.fc2(x))\n\ntiny = TinyCNN()\ntiny.load_state_dict(torch.load(\"tinycnn_pneumonia.pt\", map_location=\"cpu\"))\ntiny.eval()\n\n\n\n4.19.3 Evaluating all models fitted in Python\nWe use the following code to evaluate the models.\n\n# ============================================================\n# 0. Imports\n# ============================================================\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import Dataset, DataLoader\n\nimport medmnist\nfrom medmnist import INFO\n\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n\n# ============================================================\n# 1. Load PneumoniaMNIST\n# ============================================================\ndef load_pneumonia_mnist():\n    data_flag = \"pneumoniamnist\"\n    info = INFO[data_flag]\n    DataClass = getattr(medmnist, info['python_class'])\n    transform = transforms.Compose([transforms.ToTensor()])\n\n    test_ds = DataClass(split=\"test\", download=True, transform=transform)\n\n    X_test  = torch.stack([img for img, lbl in test_ds])\n    y_test  = torch.tensor([int(lbl) for _, lbl in test_ds]).float()\n\n    print(\"Loaded PneumoniaMNIST test set:\", len(X_test))\n    return X_test, y_test\n\nX_test, y_test = load_pneumonia_mnist()\n\nclass PneumoniaDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = X\n        self.y = y\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntest_loader_gray = DataLoader(\n    PneumoniaDataset(X_test, y_test),\n    batch_size=64, shuffle=False\n)\n\n\n# --- Convert to RGB (for ResNet and EfficientNet) ---\ndef to_rgb(batch):\n    return batch.repeat(1, 3, 1, 1)\n\nclass PneumoniaRGB(Dataset):\n    def __init__(self, X, y):\n        self.X = to_rgb(X)\n        self.y = y\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\ntest_loader_rgb = DataLoader(\n    PneumoniaRGB(X_test, y_test),\n    batch_size=64, shuffle=False\n)\n\n\n# ============================================================\n# 2. Evaluate model + confusion matrix\n# ============================================================\ndef evaluate_binary_model(model, loader):\n    model.eval()\n    preds_all = []\n    labels_all = []\n\n    with torch.no_grad():\n        for images, labels in loader:\n            logits = model(images)\n            probs = torch.sigmoid(logits)\n            preds = (probs &gt; 0.5).float()\n\n            preds_all.extend(preds.squeeze().cpu().numpy())\n            labels_all.extend(labels.cpu().numpy())\n\n    preds_all = np.array(preds_all).astype(int)\n    labels_all = np.array(labels_all).astype(int)\n\n    acc = accuracy_score(labels_all, preds_all)\n    cm  = confusion_matrix(labels_all, preds_all)\n\n    return acc, cm\n\n\ndef plot_cm(cm, title):\n    plt.figure(figsize=(5,4))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n                xticklabels=[\"Normal\", \"Pneumonia\"],\n                yticklabels=[\"Normal\", \"Pneumonia\"])\n    plt.title(title)\n    plt.ylabel(\"True Label\")\n    plt.xlabel(\"Predicted Label\")\n    plt.show()\n\n\n# ============================================================\n# 3. TinyCNN (GRAY images)\n# ============================================================\nprint(\"\\n=== Evaluating TinyCNN ===\")\n\nclass TinyCNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.pool = nn.MaxPool2d(2,2)\n        self.fc1 = nn.Linear(32*7*7, 64)\n        self.fc2 = nn.Linear(64, 1)\n\n    def forward(self, x):\n        x = self.pool(torch.relu(self.conv1(x)))\n        x = self.pool(torch.relu(self.conv2(x)))\n        x = x.view(-1, 32*7*7)\n        x = torch.relu(self.fc1(x))\n        return self.fc2(x)\n\ntiny = TinyCNN()\ntiny.load_state_dict(torch.load(\"tinycnn_pneumonia.pt\", map_location=\"cpu\"))\n\nacc_tiny, cm_tiny = evaluate_binary_model(tiny, test_loader_gray)\nplot_cm(cm_tiny, f\"TinyCNN – Accuracy {acc_tiny:.3f}\")\n\n\n# ============================================================\n# 4. ResNet-18 (RGB images)\n# ============================================================\nprint(\"\\n=== Evaluating ResNet-18 ===\")\n\nres18 = models.resnet18(weights=None)\nres18.fc = nn.Linear(res18.fc.in_features, 1)\nres18.load_state_dict(torch.load(\"resnet18_pneumonia.pt\", map_location=\"cpu\"))\n\nacc_r18, cm_r18 = evaluate_binary_model(res18, test_loader_rgb)\nplot_cm(cm_r18, f\"ResNet-18 – Accuracy {acc_r18:.3f}\")\n\n\n# ============================================================\n# 5. ResNet-50 (RGB images)\n# ============================================================\nprint(\"\\n=== Evaluating ResNet-50 ===\")\n\nres50 = models.resnet50(weights=None)\nres50.fc = nn.Linear(res50.fc.in_features, 1)\nres50.load_state_dict(torch.load(\"resnet50_pneumonia.pt\", map_location=\"cpu\"))\n\nacc_r50, cm_r50 = evaluate_binary_model(res50, test_loader_rgb)\nplot_cm(cm_r50, f\"ResNet-50 – Accuracy {acc_r50:.3f}\")\n\n\n# ============================================================\n# 6. EfficientNet-B0 (RGB images)\n# ============================================================\nprint(\"\\n=== Evaluating EfficientNet-B0 ===\")\n\nfrom torchvision.models import efficientnet_b0\n\neffnet = efficientnet_b0(weights=None)\neffnet.classifier[1] = nn.Linear(effnet.classifier[1].in_features, 1)\neffnet.load_state_dict(torch.load(\"efficientnet_pneumonia.pt\", map_location=\"cpu\"))\n\nacc_eff, cm_eff = evaluate_binary_model(effnet, test_loader_rgb)\nplot_cm(cm_eff, f\"EfficientNet-B0 – Accuracy {acc_eff:.3f}\")\n\n\n# ============================================================\n# 7. Summary\n# ============================================================\nprint(\"\\n=== FINAL ACCURACY COMPARISON ===\")\nprint(f\"TinyCNN         : {acc_tiny:.4f}\")\nprint(f\"ResNet-18       : {acc_r18:.4f}\")\nprint(f\"ResNet-50       : {acc_r50:.4f}\")\nprint(f\"EfficientNet-B0 : {acc_eff:.4f}\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#comparing-naive-approach-and-other-methods-fitted-in-python",
    "href": "neural_networks.html#comparing-naive-approach-and-other-methods-fitted-in-python",
    "title": "4  Neural networks and deep learning",
    "section": "4.20 Comparing naive approach and other methods fitted in Python",
    "text": "4.20 Comparing naive approach and other methods fitted in Python\nThe evaluation on the PneumoniaMNIST test set (624 images) highlights how architectural choices shape performance, even when all models are trained on the same data and evaluated under identical conditions.\nThe TinyCNN, our minimal baseline, reaches an accuracy of 0.625. This is a reasonable result for a handcrafted network with only a few convolutional filters and no pretraining. Its errors tend to reflect exactly what we expect from a shallow architecture: limited ability to capture higher-order texture patterns and susceptibility to overfitting or underfitting depending on the training regime.\nPerformance improves dramatically once we step into pretrained architectures. ResNet-18 achieves 0.870, showing how even a moderately deep residual network can extract richer features from the grayscale radiographs after conversion to RGB. Residual blocks help optimization, and the pretrained ImageNet weights provide a strong initialization that transfers surprisingly well to medical images.\nThe anomaly in the comparison is ResNet-50, with an accuracy of 0.389. Under normal circumstances, ResNet-50 should outperform ResNet-18 or at least match it. When it does not, this usually indicates a training or preprocessing issue: an incompatible input size, an unbalanced learning rate, insufficient convergence time, or a mismatch between grayscale data and the expected RGB distribution. This makes the ResNet-50 result a useful teaching moment: deeper models are not automatically better, and heavier architectures can become extremely sensitive to input formatting and optimization choices.\nEfficientNet-B0, which applies compound scaling rules to balance depth, width, and resolution, lands at 0.556. Its lower performance compared with ResNet-18 likely stems from the same preprocessing limitations: EfficientNet architectures expect standardized ImageNet-like color statistics, and medical images—even when converted to RGB—do not naturally match this distribution. Without extensive fine-tuning and augmentation, EfficientNet’s representational power is under-used.\nTaken together, these results illustrate a core theme in applied deep learning: model architecture matters, but data handling and preprocessing matter just as much. Residual networks tend to be robust to imperfect input pipelines, while more complex or more delicately scaled models can fail quietly if the inputs are not fully aligned with their design assumptions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#economic-and-regulatory-considerations-in-deploying-deep-learning-for-medical-imaging",
    "href": "neural_networks.html#economic-and-regulatory-considerations-in-deploying-deep-learning-for-medical-imaging",
    "title": "4  Neural networks and deep learning",
    "section": "4.21 Economic and Regulatory Considerations in Deploying Deep Learning for Medical Imaging",
    "text": "4.21 Economic and Regulatory Considerations in Deploying Deep Learning for Medical Imaging\nAlthough deep learning has become a central technique in medical image analysis, its practical adoption is shaped just as much by economic and regulatory forces as by algorithmic performance. Hospitals do not deploy models simply because they achieve high accuracy on a benchmark dataset; they adopt them only when the overall system is economically viable, clinically trustworthy, auditable, and aligned with health-care regulatory frameworks. Understanding these dimensions is essential when discussing real-world applications.\nA major economic driver is the cost of image annotation. Deep learning models, particularly convolutional neural networks trained for classification, segmentation, or detection, require large volumes of labelled data. In medical imaging, each annotation is typically generated by a trained radiologist whose time is scarce and expensive. Complex tasks such as delineating tumour margins or identifying subtle patterns in chest radiographs may require multiple annotators for consensus or adjudication. As a result, the annotation pipeline itself can dominate the total cost of developing a clinically deployable model. Institutions often underestimate this aspect: acquiring the images is straightforward, but transforming them into structured training data is a slow, expertise-intensive process whose cost scales nonlinearly with task complexity.\nAnother economic element concerns computational resources. Even relatively small convolutional networks can require hundreds of thousands of parameters and many epochs of optimisation, and deeper architectures depend on access to modern GPUs or cloud-based accelerators. For research settings, training time is merely an inconvenience; for clinical deployment, however, the cost becomes recurrent. Models must often be retrained or recalibrated when imaging protocols change, when scanners are replaced, or when the patient population shifts. Maintaining the computational infrastructure for continuous validation and retraining can represent a substantial and ongoing operational expense.\nThese economic considerations intertwine with regulatory expectations. Medical AI systems, especially those intended for diagnostic support, are expected to meet standards of robustness, interpretability, and traceability comparable to traditional medical devices. Regulatory agencies increasingly require evidence that models perform consistently across demographic groups, imaging devices, and clinical sites. This often necessitates access to heterogeneous datasets, increasing the demand,and cost,for curated multi-centre image collections. Moreover, regulators expect manufacturers to document how training data were collected, labelled, and quality-controlled. Annotation protocols therefore become part of the regulatory dossier, not just an internal research artefact.\nAnother regulatory challenge is model drift. A deep learning system that performs well at launch may degrade over time as clinical practice evolves or as hospital equipment changes. The burden falls on developers and clinical sites to monitor model performance using well-designed post-market surveillance plans. This has both economic consequences, due to the need for ongoing data collection and evaluation, and ethical implications, since undetected drift could lead to harm.\nOn the ethical side, transparency remains central. Even though modern deep learning models can achieve impressive predictive accuracy, they are often criticised for their opacity. Hospitals and regulators increasingly expect mechanisms that help clinicians understand why a model reached a particular conclusion. Methods such as saliency maps or integrated gradients provide partial visibility into the model’s behaviour, but they also introduce their own uncertainties. Ethical deployment requires not only producing explanations but also communicating their limitations so that clinicians do not over-trust or misinterpret them.\nFinally, any deployment in a clinical environment must address responsibility and liability. If an automated system misses a diagnosis or generates an incorrect recommendation, determining accountability is complex. Most regulatory frameworks emphasise that AI systems should remain assistive rather than autonomous, keeping the clinician in the decision loop. This human-in-the-loop model reduces liability risks but places additional demands on interface design, workflow integration, and training for clinical staff,all of which carry economic implications.\nTaken together, these considerations illustrate why high accuracy alone does not guarantee real-world adoption of deep learning systems in medical imaging. The true cost of deploying such models extends far beyond the compute cycles used during training; it includes annotation labour, infrastructure maintenance, compliance with regulatory standards, and the ethical expectation that automated systems must enhance rather than undermine clinical safety and trust.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "neural_networks.html#another-example-of-usage-of-neural-networks-in-large-datasets",
    "href": "neural_networks.html#another-example-of-usage-of-neural-networks-in-large-datasets",
    "title": "4  Neural networks and deep learning",
    "section": "4.22 Another example of usage of neural networks in large datasets",
    "text": "4.22 Another example of usage of neural networks in large datasets\nMulti-response neural networks for simultaneous prediction of continuous and binary outcomes\nAs a final neural-network example in this book, we take advantage of the fact that these models are naturally suited for problems involving more than one outcome. In many biomedical datasets, it is common to encounter situations where different types of responses are recorded for the same individuals: a continuous biological measurement, a binary clinical event, a score, a biomarker panel. Instead of fitting separate predictive models for each target, neural networks allow us to build a single architecture that learns shared patterns across predictors and produces multiple outputs in parallel.\nIn our chemotherapy dataset we previously modelled the tumour-shrinkage percentage as a continuous response, and the “high response” indicator (defined as at least 30 percent reduction) as a binary response. Until now, these tasks were handled using two different models—one regression and one classification. However, both outcomes describe the same underlying clinical phenomenon, and both are shaped by the same set of explanatory variables such as patient demographics, baseline tumour size, treatment allocation, dose intensity, and the large gene-expression input space. Treating these tasks independently ignores the fact that they are statistically related. A multi-response neural network captures these connections directly.\nThe basic idea is to construct a shared backbone of dense layers that extracts a joint representation from all predictors. This backbone learns how the gene features interact with tumour characteristics, how treatment and dose influence response, and how patient-level covariates modulate these effects. From this common representation, the network branches into two specialised output heads: one with a sigmoid activation to estimate the probability of high response, and another with a linear activation to predict the continuous shrinkage percentage. The architecture is simple, but the learning dynamics are richer than in the single-task case. Gradients from both outcomes flow through the shared layers, guiding the representation toward features that are useful for both tasks, while the task-specific layers refine the predictions according to their respective loss functions.\nThe model can be trained in the familiar way, using the same train/validation/test split adopted in earlier chapters. Losses for the binary and continuous components are optimised jointly, and early-stopping or checkpointing behave exactly as in the single-output setting. After training, the model yields two predictions for each patient: a probability of high response and a continuous tumour-reduction estimate. These can be evaluated separately—using accuracy or AUC for the binary output, and RMSE or MAE for the continuous output—while being produced by a single, unified model.\nMulti-response networks offer several benefits. Because they learn a single shared representation, they tend to generalise better when tasks are related, especially in moderate sample sizes. They also produce predictions that are more internally consistent: the binary head learns a natural thresholding of the continuous shrinkage prediction, and the continuous output benefits from the stabilising influence of the binary signal. Most importantly, they simplify the analytical pipeline. Instead of managing two models with different preprocessing steps, different hyperparameters, and different sets of saved files, we maintain a single coherent architecture.\n\n# ===================================================================\n# 0. Configure Python environment (MUST BE FIRST!)\n# ===================================================================\n\nlibrary(reticulate)\n\n# Use the clean environment you created\nuse_condaenv(\"r-keras-clean\", required = TRUE)\n\n# Disable RStudio’s buggy callback\nSys.setenv(KERAS_VIEW_METRICS = \"0\")\n\n# Load Keras/TensorFlow AFTER selecting env\nlibrary(keras)\nlibrary(tensorflow)\nlibrary(dplyr)\n\n# Create models directory if needed\nif (!dir.exists(\"models\")) dir.create(\"models\")\n\n\n# ===================================================================\n# 1. Load dataset\n# ===================================================================\n\ntrial_ct &lt;- readRDS(\"~/att_ai_ml/data/trial_ct_chemo_cont.rds\")\n\n\n# ===================================================================\n# 2. Select features (same as AULA 02 + AULA 03)\n# ===================================================================\n\ndf &lt;- trial_ct %&gt;% \n  select(\n    high_response,\n    response_percent,\n    patient_age,\n    tumor_grade,\n    performance_score,\n    baseline_tumor_mm,\n    treatment,\n    dose_intensity,\n    starts_with(\"gene_\")\n  )\n\n\n# ===================================================================\n# 3. Prepare data\n# ===================================================================\n\ndf$treatment   &lt;- as.numeric(as.factor(df$treatment))\ndf$tumor_grade &lt;- as.numeric(as.factor(df$tumor_grade))\n\nset.seed(123)\nn &lt;- nrow(df)\n\ntrain_idx &lt;- sample(n, size = 0.7 * n)\nval_idx   &lt;- sample(setdiff(1:n, train_idx), size = 0.15 * n)\ntest_idx  &lt;- setdiff(1:n, c(train_idx, val_idx))\n\ntrain &lt;- df[train_idx, ]\nval   &lt;- df[val_idx, ]\ntest  &lt;- df[test_idx, ]\n\nx_train &lt;- as.matrix(select(train, -high_response, -response_percent))\nx_val   &lt;- as.matrix(select(val,   -high_response, -response_percent))\nx_test  &lt;- as.matrix(select(test,  -high_response, -response_percent))\n\ny_train &lt;- list(\n  high = as.matrix(train$high_response),\n  cont = as.matrix(train$response_percent)\n)\n\ny_val &lt;- list(\n  high = as.matrix(val$high_response),\n  cont = as.matrix(val$response_percent)\n)\n\n\n# ===================================================================\n# 4. Build Multi-Output Neural Network\n# ===================================================================\n\ninput_dim &lt;- ncol(x_train)\n\ninput &lt;- layer_input(shape = input_dim)\n\nshared &lt;- input %&gt;%\n  layer_dense(128, activation = \"relu\") %&gt;%\n  layer_dropout(0.2) %&gt;%\n  layer_dense(64, activation = \"relu\")\n\nout_high &lt;- shared %&gt;%\n  layer_dense(1, activation = \"sigmoid\", name = \"high\")\n\nout_cont &lt;- shared %&gt;%\n  layer_dense(1, activation = \"linear\", name = \"cont\")\n\nmodel &lt;- keras_model(\n  inputs  = input,\n  outputs = list(high = out_high, cont = out_cont)\n)\n\nsummary(model)\n\n\n# ===================================================================\n# 5. Compile\n# ===================================================================\n\nmodel %&gt;% compile(\n  optimizer = \"adam\",\n  loss = list(\n    high = \"binary_crossentropy\",\n    cont = \"mse\"\n  ),\n  metrics = list(\n    high = c(\"accuracy\", \"AUC\"),\n    cont = c(\"mae\", \"mse\")\n  )\n)\n\n\n# ===================================================================\n# 6. Callbacks (save best model)\n# ===================================================================\n\ncheckpoint_path &lt;- \"models/multioutput_best.keras\"\n\ncallbacks_list &lt;- list(\n  callback_model_checkpoint(\n    filepath = checkpoint_path,\n    monitor = \"val_loss\",\n    save_best_only = TRUE,\n    mode = \"min\",\n    verbose = 1\n  ),\n  callback_early_stopping(\n    monitor = \"val_loss\",\n    patience = 6,\n    restore_best_weights = TRUE\n  )\n)\n\n\n# ===================================================================\n# 7. Train model\n# ===================================================================\n\nhistory &lt;- keras::fit(\n  object = model,\n  x = x_train,\n  y = y_train,\n  validation_data = list(x_val, y_val),\n  epochs = 30,\n  batch_size = 32,\n  callbacks = callbacks_list,\n  view_metrics = FALSE\n)\n\n\nplot(history)\n\n\n# ===================================================================\n# 8. Save final model (optional)\n# ===================================================================\n\nsave_model_hdf5(model, \"models/multioutput_final.h5\")\nsave_model_tf(model, \"models/multioutput_tf\")\n\n\n# ===================================================================\n# 9. Evaluate\n# ===================================================================\n\nmodel %&gt;% evaluate(\n  x_test,\n  list(\n    high = as.matrix(test$high_response),\n    cont = as.matrix(test$response_percent)\n  )\n)\n\n\n# ===================================================================\n# 10. Predict\n# ===================================================================\n\npred &lt;- model %&gt;% predict(x_test)\n\nhead(pred$high)\nhead(pred$cont)\n\n\n# ===================================================================\n# 11. Evaluation metrics (RMSE, Accuracy, AUC)\n# ===================================================================\n\nlibrary(pROC)      # AUC\nlibrary(yardstick) # opcional (não será usado diretamente)\n\n# Extract predictions (ensure numeric vectors)\npred_high &lt;- as.numeric(pred$high)      # probabilities (0-1)\npred_cont &lt;- as.numeric(pred$cont)      # continuous predictions\n\n# True labels (ensure numeric)\ntrue_high &lt;- as.numeric(test$high_response)\ntrue_cont &lt;- as.numeric(test$response_percent)\n\n# ------------------------------------------------\n# RMSE (continuous response)\n# ------------------------------------------------\nrmse_value &lt;- sqrt(mean((true_cont - pred_cont)^2, na.rm = TRUE))\nrmse_value\n\n# MAE (optional)\nmae_value &lt;- mean(abs(true_cont - pred_cont), na.rm = TRUE)\nmae_value\n\n# ------------------------------------------------\n# Accuracy (binary)\n# ------------------------------------------------\n# choose threshold = 0.5\npred_class &lt;- ifelse(pred_high &gt; 0.5, 1, 0)\n\naccuracy_value &lt;- mean(pred_class == true_high)\naccuracy_value\n\n# ------------------------------------------------\n# AUC (binary)\n# ------------------------------------------------\nauc_value &lt;- pROC::auc(true_high, pred_high)\nauc_value\n\n\n4.22.1 Comparing multi-response neural networks with single-response models\nThe multi-output neural network fitted above marks a natural progression from the uni-response models developed earlier in this chapter. In the regression setting, models such as OLS, Ridge, Lasso, Elastic Net, Random Forest and XGBoost were trained to predict tumour-shrinkage percentage using the same clinical and gene-expression features. Each of these approaches captured different aspects of the structure in the data. OLS provided a transparent baseline but struggled in the presence of thousands of correlated gene predictors, producing relatively high test error. Ridge improved stability through L2 shrinkage but preserved all features, while Lasso and Elastic Net selectively removed irrelevant genes and delivered substantial gains in prediction accuracy. XGBoost ultimately provided the strongest performance for the continuous outcome, combining nonlinearity, structured regularisation, and residual fitting to recover predictive signal that linear methods could not capture.\nA similar pattern appeared in the classification task. Logistic regression established a reasonably strong linear baseline, while tree-based models—especially pruned CART—captured nonlinear interactions. Random forest improved further by aggregating decorrelated trees, and XGBoost again achieved the highest accuracy and AUC among single-task models. These models, however, treat the two outcomes independently. The binary “high response” is simply a discretisation of the continuous shrinkage percentage, yet in the uni-response framework the models do not share information; each model is optimised separately, and any relationship between the two tasks is implicitly ignored.\nThe multi-response neural network addresses this limitation directly. Instead of decomposing the problem into two unrelated predictive tasks, the network learns a single shared representation across all features and uses that representation to drive both the continuous and binary outputs simultaneously. This coupling alters the learning dynamics in meaningful ways. Gradients from the binary classification head influence how the shared layers evolve, nudging the model toward features that sharpen the separation between responders and non-responders. At the same time, the continuous head contributes a richer and more fine-grained signal, capturing subtleties that cannot be conveyed by a binary indicator alone. During training, both objectives pull on the same backbone, and this shared pressure typically produces a more stable and more expressive set of learned features.\nThe performance metrics reflect this complementary structure. The multi-output network achieves an RMSE of roughly 3.40 and an MAE near 2.8 for the continuous task—a level of accuracy that is competitive with the linear baselines but below the performance of XGBoost, which remains the strongest single-task method for tumour-shrinkage prediction. The classification head, however, performs exceptionally well, reaching accuracy around 0.81 and an AUC close to 0.997, surpassing even the high-performing single-task XGBoost classifier. This combination is noteworthy: although the continuous prediction is not as precise as the best regression model, the classifier benefits substantially from the additional structure encoded in the shared representation, leading to superior discrimination.\nThis divergence in performance illustrates an important aspect of multi-task learning. The network does not aim to dominate each individual task when measured in isolation. Instead, it seeks a representation that jointly supports both tasks, balancing the signals coming from each output. The continuous regression task encourages nuanced modelling of the underlying biological variability, whereas the classification task emphasises separation between clinically defined responder groups. The network integrates both perspectives, and the resulting representation improves tasks that rely on relative distinctions, such as classification, sometimes at a modest cost to tasks requiring precise absolute prediction.\nMore broadly, the contrast between the single-task models and the multi-response network highlights the flexibility of neural architectures in settings with related outcomes. In the uni-response framework, each model operates within its own objective function, without awareness of the other task. In the multi-response framework, the network learns to reconcile multiple predictive goals at once, discovering common structure that traditional models cannot exploit. The result is a model that is not only compact—because the feature extraction occurs once rather than twice—but also more aligned with the biological reality of the data, where multiple outcomes arise from shared mechanisms.\nFrom a teaching perspective, this example demonstrates how neural networks extend naturally from single-output prediction to richer forms of multi-task learning. Students can observe how performance changes across different modelling paradigms, how shared backbones redistribute predictive strength across tasks, and why multi-output strategies may outperform or underperform single-output methods depending on the metric considered. In practical biomedical applications, where continuous biomarkers, categorical endpoints, toxicity grades, and longitudinal measurements often coexist, these architectures become increasingly valuable. The multi-response network presented here offers a first glimpse into that broader modelling philosophy.\n\n\n4.22.2 Summary\n\n4.22.2.1 Continuous outcome — tumour shrinkage percentage\n\n\n\n\n\n\n\n\n\nModel\nMAE\nRMSE\nNotes\n\n\n\n\nOLS\n1.83\n2.28\nLinear baseline; unstable with high-dim gene features\n\n\nRidge\n1.78\n2.22\nL2 stabilisation; keeps all genes; moderate improvement\n\n\nLasso\n1.53\n1.92\nSparse model; selects informative genes; interpretable\n\n\nElastic Net\n1.53\n1.92\nBalanced L1/L2 shrinkage; good for correlated genes\n\n\nRandom Forest\n5.54\n6.80\nPoor performance in ultra-high-dimensional settings\n\n\nXGBoost\n1.34\n1.79\nBest single-task performance; captures nonlinearities\n\n\nMulti-output Neural Network\n2.80\n3.40\nShared representation; regression head less precise\n\n\n\n\n\n4.22.2.2 B) Binary outcome — high response (≥30%)\n\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nSensitivity\nSpecificity\nAUC\nNotes\n\n\n\n\nLogistic Regression\n0.907\n0.897\n0.913\n0.939\nLinear baseline; interpretable\n\n\nPruned CART Tree\n—\n—\n—\n0.984\nSimple nonlinear rules; moderate variance\n\n\nRandom Forest\n—\n—\n—\n0.996\nStrong ensemble; robust low-variance learner\n\n\nXGBoost\n—\n—\n—\n0.998\nBest single-task classifier; high accuracy\n\n\nMulti-output Neural Network\n0.807\n—\n—\n0.997\nShared backbone; classification greatly benefits\n\n\n\n\n\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 770–78. https://arxiv.org/abs/1512.03385.\n\n\nMooney, Paul Timothy. 2018. “Chest x-Ray Images (Pneumonia).” https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia.\n\n\nTan, Mingxing, and Quoc V. Le. 2019. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” In International Conference on Machine Learning (ICML), 6105–14. https://arxiv.org/abs/1905.11946.\n\n\nYang, Jiancheng, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. 2023. “MedMNIST V2-a Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification.” Scientific Data 10 (1): 41.\n\n\nYang, Jiancheng, Rui Shi, Donglai Wei, Lequan Yu, Zaiyi Zhang, Liwei Wang, Dong Ni, and Pheng-Ann Heng. 2021. “MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis.” Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 14817–28. https://doi.org/10.1109/ICCV48922.2021.01454.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Neural networks and deep learning</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html",
    "href": "introd_bayesian.html",
    "title": "5  Introduction to Bayesian methods",
    "section": "",
    "text": "5.1 Motivation\n# -----------------------------------------------------------\n# Reload example\n# -----------------------------------------------------------\n\nwearable_complete &lt;- readRDS(\"~/att_ai_ml/att_book/data/dct_wearable_complete.rds\")\nstr(wearable_complete)\n\n'data.frame':   500 obs. of  10 variables:\n $ age                  : num  63 47 50 49 66 49 74 26 58 53 ...\n $ baseline_inflammation: num  4.08 3.51 3.54 3.86 3 ...\n $ frailty              : num  0.453 0.441 0.346 0.539 0.454 ...\n $ resting_hr           : num  83.8 63.7 69.8 67.5 83.6 ...\n $ hrv                  : num  29.2 30 20.9 45.5 44 ...\n $ skin_temp            : num  36 34.4 34.7 34.4 34.7 ...\n $ activity_index       : num  27.1 60.7 27.3 37.2 38.5 ...\n $ stress_score         : num  101.6 95.9 96.8 91.3 85.9 ...\n $ event_risk           : num  0.993 0.976 0.976 0.982 0.971 ...\n $ event_occurred       : int  1 1 1 1 1 1 1 1 1 1 ...\nIn this chapter we work with a simulated dataset representing a decentralised clinical trial in which participants are monitored remotely through wearable devices. Although simulated, the structure mirrors the data routinely collected in contemporary digital-health and pharmaceutical pipelines. Each participant contributes baseline clinical information such as age, frailty, and an inflammation score derived from laboratory markers. Alongside these features, the dataset includes continuous physiological measurements obtained from a wearable sensor: resting heart rate, heart rate variability, skin temperature, and a general activity index capturing daily mobility. A composite stress score summarises overall physiological load, and the dataset also contains a modelled probability of clinical deterioration and a binary indicator of adverse events.\nExamples like this matter because wearable-derived variables have become central to modern clinical research. Remote monitoring gives investigators the ability to observe physiological dynamics as they unfold in real life rather than only during scheduled clinic visits. Signals such as HRV, resting heart rate, skin temperature, and activity levels are already used as exploratory or surrogate endpoints in ongoing studies by major pharmaceutical companies, and several digital biomarkers are beginning to receive regulatory attention. These measurements also tend to co-move in structured, physiologically meaningful ways: inflammation and frailty are associated with increases in resting heart rate and skin temperature; HRV declines with age and clinical burden; activity levels fall as frailty rises; and an overall stress score reflects these interacting components.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html#understanding-probabilities-models-and-likelihood",
    "href": "introd_bayesian.html#understanding-probabilities-models-and-likelihood",
    "title": "5  Introduction to Bayesian methods",
    "section": "5.2 Understanding Probabilities, Models, and Likelihood",
    "text": "5.2 Understanding Probabilities, Models, and Likelihood\nBefore discussing Bayesian reasoning, we need a shared language for uncertainty. A probability distribution describes how likely different outcomes or parameter values are. For example, resting heart rate recorded by a wearable sensor fluctuates across individuals and days, and part of that variability can be captured by a distribution such as a Normal or Log-Normal.\nA probability distribution provides:\n\nA sample space: all possible values (e.g., heart rate 40–120 bpm)\nA way to assign likelihood to values\nA mathematical structure for modelling variation\nKnowing a distribution allows us to compute quantities such as:\n\nthe probability that resting heart rate exceeds 80 bpm\nthe expected average HRV in a population\nthe density at specific values\n\n\nExample in R:\n\n# probability heart rate &gt; 80 under a Normal(70,8)\n1 - pnorm(80, mean = 70, sd = 8)\n\n[1] 0.1056498\n\n# density at HR = 75\ndnorm(75, mean = 70, sd = 8)\n\n[1] 0.04102012\n\n\nMany physiological measurements in decentralised clinical trials—such as resting heart rate, HRV, skin temperature, or daily activity—are well approximated by a normal distribution. A probability density curve summarises how likely different values are. The peak indicates the most plausible range (around the mean), and the spread reflects biological variability (captured by the standard deviation).\n\nlibrary(ggplot2)\n\n# Parameters for illustration\nmu  &lt;- 70     # mean\nsd  &lt;- 8      # standard deviation\ngrid &lt;- seq(mu - 4*sd, mu + 4*sd, length.out = 800)\n\ndf_norm &lt;- data.frame(\n  x = grid,\n  density = dnorm(grid, mean = mu, sd = sd)\n)\n\nggplot(df_norm, aes(x = x, y = density)) +\n  geom_line(color = \"#1f77b4\", linewidth = 1.5) +\n  geom_vline(xintercept = mu, color = \"darkred\", linetype = \"dashed\", linewidth = 1.1) +\n  annotate(\"text\", x = mu, y = max(df_norm$density)*0.95,\n           label = paste(\"mean =\", mu), color = \"darkred\", hjust = -0.2) +\n  labs(\n    title = \"Normal Distribution (μ = 70, σ = 8)\",\n    x = \"Value\",\n    y = \"Density\"\n  ) +\n  theme_minimal(base_size = 15) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5)\n  )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html#what-is-statistical-inference",
    "href": "introd_bayesian.html#what-is-statistical-inference",
    "title": "5  Introduction to Bayesian methods",
    "section": "5.3 What is statistical inference?",
    "text": "5.3 What is statistical inference?\nStatistical inference concerns the process of learning about unknown aspects of a population or a data-generating process using observed data. The central objective is to use measurements collected in a sample to draw conclusions about quantities that cannot be observed directly.\nIn the context of wearable-sensor data from a decentralised clinical trial, several inferential questions arise naturally:\n– What is the true mean resting heart rate in the trial population? – How strongly is frailty associated with heart-rate variability? – What is the probability that an individual will experience an adverse event, given their physiological profile?\nTo answer such questions, we adopt a statistical model describing how the observed measurements arise. Statistical inference then consists of using the model and the observed data to learn about the unknown model parameters.\nTwo major inferential paradigms are commonly used.\n\n5.3.1 Frequentist inference.\nHere, the parameters of the model are treated as fixed but unknown constants. Randomness arises only from the repeated sampling of data. Estimation therefore focuses on identifying the single parameter values that best explain the observed data.\n\n\n5.3.2 Bayesian inference.\nIn the Bayesian framework, the data are treated as fixed once observed, and uncertainty is expressed through probability distributions over the parameters themselves. The aim is to describe this uncertainty before and after observing the data.\nDespite their conceptual differences, both approaches rely on the same fundamental building block: the likelihood.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html#the-likelihood-function",
    "href": "introd_bayesian.html#the-likelihood-function",
    "title": "5  Introduction to Bayesian methods",
    "section": "5.4 The likelihood function",
    "text": "5.4 The likelihood function\nThe likelihood function quantifies how compatible a set of parameter values is with the observed data. It is derived from the assumed probability model for the measurements.\nSuppose resting heart rate in the trial is modelled as\n\\[\ny_i \\sim \\mathcal{N}\\left(\\mu, \\sigma^2\\right),\n\\]\nwhere \\(\\mu\\) is the population mean and \\(\\sigma^2\\) is the measurement variability. For a sample \\(y_1, \\ldots, y_n\\), the likelihood of \\(\\mu\\) is\n\\[\nL(\\mu)=\\prod_{i=1}^n f\\left(y_i \\mid \\mu\\right),\n\\]\nwhere \\(f(\\cdot)\\) is the Normal density. Although \\(f(\\cdot)\\) is interpreted as a probability density when viewed as a function of \\(y_i\\), it becomes a likelihood when viewed as a function of \\(\\mu\\) with the data held fixed. Higher likelihood values indicate that a given value of \\(\\mu\\) provides a better explanation of the observed data.\nA simple example in R illustrates this idea:\n\nlikelihood &lt;- function(mu, y, sigma) {\n  prod(dnorm(y, mean = mu, sd = sigma))\n}\n\nset.seed(1)\ny &lt;- rnorm(10, mean = 72, sd = 8)\n\nlikelihood(70, y, sigma = 8) \n\n[1] 2.95254e-15\n\nlikelihood(72, y, sigma = 8)  # larger: μ = 72 fits better\n\n[1] 5.616298e-15\n\n\nBecause likelihoods often take extremely small values, it is common to work with the log-likelihood:\n\\[\n\\ell(\\mu)=\\log L(\\mu)=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^2\\right)-\\frac{1}{2 \\sigma^2} \\sum_{i=1}^n\\left(y_i-\\mu\\right)^2 .\n\\]\n\nlibrary(ggplot2)\n\nset.seed(1)\ny &lt;- rnorm(20, mean = 72, sd = 8)\nsigma &lt;- 8\n\n# grid of candidate means\nmu_grid &lt;- seq(60, 85, length.out = 400)\n\n# compute likelihood of each μ (up to a proportionality constant)\nlik_values &lt;- sapply(mu_grid, function(mu) {\n  prod(dnorm(y, mean = mu, sd = sigma))\n})\n\n# normalise for visualisation\nlik_values_scaled &lt;- lik_values / max(lik_values)\n\ndf_lik &lt;- data.frame(\n  mu   = mu_grid,\n  like = lik_values_scaled\n)\n\nggplot(df_lik, aes(x = mu, y = like)) +\n  geom_line(linewidth = 1.4, color = \"#ff7f0e\") +\n  labs(\n    title = \"Likelihood Function for μ\",\n    x = expression(mu),\n    y = \"Likelihood (scaled to 1)\"\n  ) +\n  theme_minimal(base_size = 15) +\n  theme(plot.title = element_text(face = \"bold\", hjust = 0.5))",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html#maximum-likelihood-estimation-mle-one-of-the-fundamental-frequentist-strategies-is-to-choose-the-parameter-value-that-maximises-the-likelihood",
    "href": "introd_bayesian.html#maximum-likelihood-estimation-mle-one-of-the-fundamental-frequentist-strategies-is-to-choose-the-parameter-value-that-maximises-the-likelihood",
    "title": "5  Introduction to Bayesian methods",
    "section": "5.5 Maximum likelihood estimation (MLE) One of the fundamental frequentist strategies is to choose the parameter value that maximises the likelihood:",
    "text": "5.5 Maximum likelihood estimation (MLE) One of the fundamental frequentist strategies is to choose the parameter value that maximises the likelihood:\n\\[\n\\hat{\\mu}_{\\mathrm{MLE}}=\\arg \\max _\\mu L(\\mu) .\n\\]\nFor the Normal model with known \\(\\sigma\\), the MLE has a closed form:\n\\[\n\\hat{\\mu}_{\\mathrm{MLE}}=\\bar{y} .\n\\]\nThis result is intuitive: the value of \\(\\mu\\) that makes the observed data most probable is the sample mean.\nIn R:\n\nybar &lt;- mean(y)\nybar  # MLE of μ\n\n[1] 73.52419\n\n\nFor more complex models-such as logistic regression for event risk or hierarchical models describing physiological dependencies-likelihood maximisation may require numerical optimisation (e.g., via optim ( ) or specialized software). Regardless of complexity, the principle is the same: the best-fitting parameter values are those that produce the highest likelihood.\nThe Bayesian approach, introduced in the next section, expands on this idea by combining the likelihood with prior information to produce a full posterior distribution over the parameters rather than a single best value.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html#introducing-bayesian-reasoning",
    "href": "introd_bayesian.html#introducing-bayesian-reasoning",
    "title": "5  Introduction to Bayesian methods",
    "section": "5.6 Introducing Bayesian reasoning",
    "text": "5.6 Introducing Bayesian reasoning\nBayesian modelling provides a coherent framework for reasoning under uncertainty, particularly in digital-health and wearable-sensor settings where physiological signals vary across individuals, time, and context. The central idea is that parameters—such as the average resting heart rate in a decentralised clinical trial—are treated as uncertain quantities, and this uncertainty is represented through probability distributions. Once the data have been collected, they are considered fixed, and all remaining uncertainty lies in the parameters themselves.\nThis perspective contrasts with the frequentist view but relies on the same building blocks: a probabilistic model for the measurements and the likelihood. What distinguishes Bayesian reasoning is the explicit incorporation of prior knowledge and a formal rule for updating uncertainty as new data arrive.\n\n5.6.1 Bayes Theorem\nBayesian inference is grounded in a single identity-Bayes’ theorem-which combines prior information with the likelihood contributed by the data:\n\\[\np(\\theta \\mid y)=\\frac{p(y \\mid \\theta) p(\\theta)}{p(y)} .\n\\]\nWhere: - \\(p(\\theta)\\) is the prior distribution, representing what is known (or assumed) before observing any wearable data; - \\(p(y \\mid \\theta)\\) is the likelihood, describing how plausible the observed measurements are for different values of \\(\\theta\\); - \\(p(\\theta \\mid y)\\) is the posterior distribution, reflecting updated beliefs after the data are observed; - \\(p(y)\\) is the marginal likelihood, ensuring that the posterior integrates to one.\nIn proportional form:\n\\[\np(\\theta \\mid y) \\propto p(y \\mid \\theta) p(\\theta),\n\\]\nwhich makes Bayesian inference appear as a learning rule: new evidence reshapes prior beliefs.\nA small numerical example of Bayes’ theorem in R To illustrate how Bayes’ theorem works in practice, consider estimating the probability that a participant is highly frail after observing an elevated resting heart rate (&gt;85 bpm).\nWe assume: - prior probability of high frailty: \\(p(F=1)=0.30\\); - probability of high HR given frailty: \\(p(H R&gt;85 \\mid F=1)=0.55\\); - probability of high HR given low frailty: \\(p(H R&gt;85 \\mid F=0)=0.12\\).\nUsing Bayes’ rule:\n\np_frail &lt;- 0.30\np_notfrail &lt;- 1 - p_frail\n\np_hr_given_frail &lt;- 0.55\np_hr_given_notfrail &lt;- 0.12\n\nposterior &lt;- (p_hr_given_frail * p_frail) /\n  (p_hr_given_frail * p_frail + p_hr_given_notfrail * p_notfrail)\n\nposterior\n\n[1] 0.6626506\n\n\nThis calculation yields a posterior probability of approximately 66%, showing how relevant physiological evidence (elevated HR) increases our belief that an individual is frail.\nBayesian updating in physiological monitoring This updating mechanism mirrors clinical reasoning. A clinician assessing resting heart rate or HRV carries implicit prior expectations (based on age, frailty, or inflammatory status) and adjusts these expectations as new wearable-sensor data accumulate. Bayesian models offer a transparent and principled way to represent this process. Priors stabilise inference when data are sparse or noisy; as evidence grows, the likelihood takes over and the posterior becomes increasingly data-driven.\nTo illustrate Bayesian updating more formally, consider estimating the mean resting heart rate \\(\\mu\\) in the trial. Assume that the observed values \\(y_1, \\ldots, y_n\\) follow a Normal model with known measurement variability. A prior distribution encodes initial beliefs about \\(\\mu_1\\), and the wearable data update these beliefs through the likelihood, yielding a posterior distribution.\nThe figure below shows the prior, the likelihood scaled for comparison, and the posterior.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr)\n\nset.seed(42)\n\n# -------------------------------------\n# Simulated dataset (resting heart rate)\n# -------------------------------------\ny &lt;- rnorm(50, mean = 72, sd = 8)\nn  &lt;- length(y)\nybar &lt;- mean(y)\nsigma &lt;- 8  # assumed known measurement variability\n\n# -------------------------------------\n# Prior: μ ~ Normal(70, 10^2)\n# -------------------------------------\nmu_prior_mean &lt;- 70\nmu_prior_sd   &lt;- 10\n\n# -------------------------------------\n# Posterior parameters (Normal-Normal updating)\n# -------------------------------------\nposterior_mean &lt;- ( (mu_prior_mean / mu_prior_sd^2) + (n * ybar / sigma^2) ) /\n                  ( (1 / mu_prior_sd^2) + (n / sigma^2) )\n\nposterior_sd &lt;- sqrt( 1 / ( (1 / mu_prior_sd^2) + (n / sigma^2) ) )\n\n# -------------------------------------\n# Build grid\n# -------------------------------------\ngrid &lt;- seq(50, 90, length.out = 800)\n\nprior_density      &lt;- dnorm(grid, mu_prior_mean,   mu_prior_sd)\nposterior_density  &lt;- dnorm(grid, posterior_mean, posterior_sd)\n\n# Likelihood of μ (scaled for plotting)\nlikelihood_raw &lt;- dnorm(ybar, mean = grid, sd = sigma/sqrt(n))\nlikelihood_density &lt;- likelihood_raw / max(likelihood_raw) * max(prior_density)\n\n# -------------------------------------\n# Build tidy dataframe\n# -------------------------------------\ndf &lt;- data.frame(\n  mu = grid,\n  Prior      = prior_density,\n  Likelihood = likelihood_density,\n  Posterior  = posterior_density\n) |&gt;\n  pivot_longer(cols = c(\"Prior\", \"Likelihood\", \"Posterior\"),\n               names_to = \"Distribution\",\n               values_to = \"Density\")\n\n# -------------------------------------\n# ggplot\n# -------------------------------------\nggplot(df, aes(x = mu, y = Density, color = Distribution)) +\n  geom_line(linewidth = 1.4) +\n  scale_color_manual(values = c(\n    \"Prior\" = \"#1f77b4\",\n    \"Likelihood\" = \"#ff7f0e\",\n    \"Posterior\" = \"#2ca02c\"\n  )) +\n  labs(\n    title = \"Bayesian Updating: Prior, Likelihood, and Posterior\",\n    x = expression(mu),\n    y = \"Density (scaled)\"\n  ) +\n  theme_minimal(base_size = 15) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    legend.position = \"top\",\n    legend.title = element_blank()\n  )",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html#bayesian-linear-regression",
    "href": "introd_bayesian.html#bayesian-linear-regression",
    "title": "5  Introduction to Bayesian methods",
    "section": "5.7 Bayesian linear regression",
    "text": "5.7 Bayesian linear regression\nOnce we understand Bayesian updating in the one-parameter case, the next step is to extend the framework to regression. Regression is a natural modelling tool in decentralised clinical trials: we may wish to quantify how frailty influences resting heart rate, how inflammation shifts HRV, or how physiological variables interact.\nIn the frequentist framework, a regression yields point estimates of the regression coefficients. In the Bayesian framework, these coefficients—now denoted by weights w are treated as uncertain quantities with full posterior distributions.\nThis section introduces Bayesian linear regression using the wearable dataset and compares it to classical (frequentist) regression while highlighting conceptual and interpretational differences.\n\n5.7.1 Frequentist vs Bayesian Regression\nA simple linear model for resting heart rate may be written as:\n\\[\n\\mathrm{HR}_i=w_0+w_1 \\cdot \\text { frailty }_i+\\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right) .\n\\]\nFrequentist OLS: - The weights \\(w_0\\) and \\(w_1\\) are fixed but unknown constants. - Data are random due to sampling variability. - Estimates \\(\\hat{w}_0, \\hat{w}_1\\) maximise the likelihood. - Uncertainty is summarised via confidence intervals and p-values.\nBayesian regression: - Data are fixed once observed. - The weights \\(w_0, w_1\\) and variance \\(\\sigma^2\\) are uncertain, with probability distributions. - The inferential target is the posterior:\n\\[\np\\left(w_0, w_1, \\sigma \\mid y, x\\right),\n\\]\nwhich describes all plausible values for the weights given the data and prior assumptions.\nThis allows Bayesian models to answer questions that classical models cannot-e.g., “What is the probability that frailty increases resting heart rate?” (i.e., \\(\\operatorname{Pr}\\left(w_1&gt;0 \\mid\\right.\\) data))\n\n\n5.7.2 Frequentist Regression Example\nWe begin with a traditional OLS model:\n\nfit_lm &lt;- lm(resting_hr ~ frailty, data = wearable_complete)\nsummary(fit_lm)\n\n\nCall:\nlm(formula = resting_hr ~ frailty, data = wearable_complete)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.863  -8.548   0.353   8.478  33.170 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   51.064      1.532   33.34   &lt;2e-16 ***\nfrailty       47.618      3.652   13.04   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.15 on 498 degrees of freedom\nMultiple R-squared:  0.2545,    Adjusted R-squared:  0.253 \nF-statistic:   170 on 1 and 498 DF,  p-value: &lt; 2.2e-16\n\n\nThis output includes: - point estimates for \\(w_0\\) and \\(w_1\\); - standard errors; - p-values; - confidence intervals.\nThese objects quantify sampling variability, but they do not provide probability statements about the weights themselves.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html#bayesian-linear-regression-model-specification",
    "href": "introd_bayesian.html#bayesian-linear-regression-model-specification",
    "title": "5  Introduction to Bayesian methods",
    "section": "5.8 Bayesian Linear Regression Model Specification",
    "text": "5.8 Bayesian Linear Regression Model Specification\nBefore fitting the Bayesian model, we explicitly state the probabilistic structure. For each individual \\(i\\) in the decentralised clinical trial, resting heart rate is modeled as\n\\[\n\\mathrm{HR}_i \\sim \\mathcal{N}\\left(w_0+w_1 \\text { frailty }_i, \\sigma^2\\right) .\n\\]\nThe Bayesian formulation assigns prior distributions to all unknown quantities:\n\\[\nw_0 \\sim \\mathcal{N}\\left(60,20^2\\right), \\quad w_1 \\sim \\mathcal{N}\\left(0,20^2\\right), \\quad \\sigma \\sim \\text { HalfNormal }(10) .\n\\]\nThese priors represent weak prior beliefs about baseline heart rate (around 60 bpm), an uncertain association between frailty and heart rate, and a plausible scale for residual variability.\nGiven the data \\(\\left(y_i, x_i\\right)\\), Bayes’ theorem yields a posterior distribution:\n\\[\np\\left(w_0, w_1, \\sigma \\mid y, x\\right) \\propto p\\left(y \\mid w_0, w_1, \\sigma, x\\right) p\\left(w_0\\right) p\\left(w_1\\right) p(\\sigma) .\n\\]\nThis posterior cannot be computed analytically; instead, it is approximated using Markov Chain Monte Carlo (MCMC), as implemented in rstanarm::stan_glm().\n\n# Refit Bayesian model (must run BEFORE posterior extraction)\nlibrary(rstanarm)\n\nLoading required package: Rcpp\n\n\nThis is rstanarm version 2.32.2\n\n\n- See https://mc-stan.org/rstanarm/articles/priors for changes to default priors!\n\n\n- Default priors may change, so it's safest to specify priors, even if equivalent to the defaults.\n\n\n- For execution on a local, multicore CPU with excess RAM we recommend calling\n\n\n  options(mc.cores = parallel::detectCores())\n\nfit_bayes &lt;- stan_glm(\n  resting_hr ~ frailty,\n  data = wearable_complete,\n  prior = normal(0, 20),               # prior for w1\n  prior_intercept = normal(60, 20),    # prior for w0\n  chains = 2, iter = 2000, refresh = 0\n)\nprint(fit_bayes)\n\nstan_glm\n family:       gaussian [identity]\n formula:      resting_hr ~ frailty\n observations: 500\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 51.7    1.5  \nfrailty     46.0    3.5  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 11.2    0.4  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n5.8.1 Understanding MCMC: how Bayesian posteriors are computed\nUntil now the Bayesian examples relied on stan_glm( ), which conveniently returns draws from the posterior. Behind this simplicity is an important computational idea: Monte Carlo sampling via Markov chains (MCMC).\nIn most realistic models-including regressions, hierarchical structures, or logistic models-the posterior distribution has no closed-form solution. Bayes’ theorem still holds, but the expression\n\\[\np\\left(w_0, w_1, \\sigma \\mid y, x\\right)\n\\]\ncannot be written analytically. MCMC approximates this distribution by generating samples:\n\\[\n\\left\\{w^{(1)}, w^{(2)}, \\ldots, w^{(S)}\\right\\} \\sim p(w \\mid y) .\n\\]\nThese samples allow the computation of: - posterior means and medians - credible intervals - probabilities such as \\(P\\left(w_1&gt;0 \\mid\\right.\\) data \\()\\) - predictive distributions\n\n\n5.8.2 The Stan approach: Hamiltonian Monte Carlo (HMC)\nThe rstanarm package relies on Stan, which implements Hamiltonian Monte Carlo (HMC) - an efficient MCMC algorithm that avoids slow random-walk behaviour.\nStan returns: - multiple chains - thousands of posterior draws - diagnostics for convergence\nThis ensures highly reliable approximations to the posterior.\n\n\n5.8.3 Inspecting the MCMC samples\n\nposterior &lt;- as.data.frame(fit_bayes)\nhead(posterior)\n\n  (Intercept)  frailty    sigma\n1    52.61372 44.30728 12.08584\n2    51.09142 48.67930 10.34534\n3    52.43520 44.39039 12.04483\n4    52.88922 44.30685 12.18157\n5    51.12819 46.69388 11.54471\n6    54.15898 41.84504 11.76282\n\n\nEach row is one posterior draw; each column corresponds to a parameter ( w0 , w1 , sigma ). Posterior probability that \\(w_1&gt;0\\)\n\n  mean(posterior$frailty &gt; 0)\n\n[1] 1\n\n\nInterpretation:\n“Given the data and the priors, the probability that frailty increases resting HR is 1%.”\nA frequentist model cannot provide this probability.\n\n\n5.8.4 MCMC Diagnostics\nEvaluating MCMC convergence is essential. #### Trace plots\nTrace plots show parameter trajectories across iterations. Good chains look like overlapping “hairy caterpillars”.\n\nlibrary(bayesplot)\n\nThis is bayesplot version 1.14.0\n\n\n- Online documentation and vignettes at mc-stan.org/bayesplot\n\n\n- bayesplot theme set to bayesplot::theme_default()\n\n\n   * Does _not_ affect other ggplot2 plots\n\n\n   * See ?bayesplot_theme_set for details on theme setting\n\nmcmc_trace(as.array(fit_bayes), pars = c(\"(Intercept)\", \"frailty\"))\n\n\n\n\n\n\n\n\nExpected behaviour:\n\nchains overlap\nno visible upward/downward drift\nno sticking\n\nAdditional diagnostic plots:\n\nposterior_draws &lt;- as.array(fit_bayes)\n\nmcmc_trace(posterior_draws, pars = c(\"(Intercept)\", \"frailty\"))\n\n\n\n\n\n\n\nmcmc_dens_overlay(posterior_draws, pars = c(\"(Intercept)\", \"frailty\"))\n\n\n\n\n\n\n\nmcmc_acf(posterior_draws, pars = c(\"(Intercept)\", \"frailty\"))\n\n\n\n\n\n\n\n\n\n5.8.4.1 R-hat (Gelman–Rubin diagnostic)\nR-hat close to 1 (≤ 1.01) indicates convergence.\n\nrhat_values &lt;- rstan::summary(fit_bayes$stanfit)$summary[ , \"Rhat\"]\nrhat_values\n\n  (Intercept)       frailty         sigma      mean_PPD log-posterior \n    1.0001283     0.9998629     0.9990025     0.9998934     1.0018865 \n\n\n\n\n\n5.8.5 Interpretation of Posterior Weights\n\n5.8.5.1 Interpretation of \\(w_0\\) (Intercept)\nPosterior median around 51.7 bpm, with low uncertainty. Represents predicted resting HR when frailty \\(=0\\) (least frail individuals).\n\n\n5.8.5.2 Interpretation of \\(w_1\\) (Frailty effect)\nPosterior median % 46 bpm increase per 1-unit increase in frailty. Credible interval entirely above zero ⇒ very strong evidence of a positive association. #### Noise parameter \\(\\sigma\\) Posterior median \\(\\boldsymbol{\\approx} \\mathbf{1 1 . 2 ~ b p m}\\), consistent with simulated residual noise.\n\n\n\n5.8.6 Posteriors of the weights\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\nposterior &lt;- as.data.frame(fit_bayes)\n\nposterior &lt;- posterior |&gt; \n  rename(\n    w0 = `(Intercept)`,\n    w1 = frailty\n  ) |&gt;\n  select(w0, w1) |&gt;\n  pivot_longer(cols = everything(),\n               names_to = \"weight\",\n               values_to = \"value\")\n\nposterior$weight &lt;- factor(\n  posterior$weight,\n  levels = c(\"w0\", \"w1\"),\n  labels = c(\"w₀ (Intercept)\", \"w₁ (Effect of Frailty)\")\n)\n\nggplot(posterior, aes(x = value, fill = weight)) +\n  geom_density(alpha = 0.6) +\n  facet_wrap(~ weight, scales = \"free\", ncol = 2) +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\") +\n  scale_fill_manual(values = c(\"#8dbdff\", \"#95d0a3\")) +\n  labs(\n    title = \"Posterior Distributions of Regression Weights\",\n    x = \"Weight value\",\n    y = \"Density\"\n  ) +\n  theme_minimal(base_size = 15) +\n  theme(\n    plot.title = element_text(face = \"bold\", hjust = 0.5),\n    legend.position = \"none\"\n  )\n\n\n\n\n\n\n\n\n\n\n5.8.7 Credible Intervals vs Confidence Intervals\nA 95% Bayesian credible interval means: There is a 95% probability that the true weight lies in this interval, given the data and the priors.\nA 95% frequentist confidence interval means: If we repeated the experiment infinitely many times, \\(95 \\%\\) of the computed intervals would contain the true value.\nIt does not express probability about the parameter.\n\n\n5.8.8 Frequentist vs Bayesian comparison table\n\n\n\n\n\n\n\n\nConcept\nBayesian credible interval\nFrequentist confidence interval\n\n\n\n\nWhat is uncertain?\nThe parameter (w)\nThe interval\n\n\nInterpretation\n\\(Prob((w) in interval | data)\\)\n95% of intervals would contain (w) under repeated sampling\n\n\nDepends on prior?\nYes\nNo\n\n\nCan answer “Is (w_1 &gt; 0)?”\nYes\nNo",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html#posterior-distributions-and-treatment-of-pure-risk-measures",
    "href": "introd_bayesian.html#posterior-distributions-and-treatment-of-pure-risk-measures",
    "title": "5  Introduction to Bayesian methods",
    "section": "5.9 Posterior distributions and treatment of pure risk measures",
    "text": "5.9 Posterior distributions and treatment of pure risk measures\nBefore introducing how Bayesian posterior distributions help quantify clinical risk, it is useful to clarify what we mean by risk in the first place. In clinical and digital-health settings, risk typically refers to the probability that an adverse outcome will occur. Whether the outcome is deterioration, infection, hospitalisation, or early physiological instability, the risk attached to a participant is fundamentally a probability statement. A higher probability indicates greater uncertainty about the future—but in a clinically dangerous direction.\nProbability is therefore a natural mathematical language for expressing risk. If a model gives a participant a 25% deterioration probability, we can interpret this directly: out of many individuals with similar physiology, about one in four would be expected to deteriorate. When probability distributions themselves are uncertain—because data are limited, noisy, or highly variable—the model should express that uncertainty as well. This is where Bayesian methods excel: rather than producing a single risk estimate, they produce an entire distribution describing all plausible values of that risk.\nFrom this perspective, Bayesian models do not simply output numbers; they yield risk objects: posterior distributions over clinically meaningful quantities. These risk objects fully encode our uncertainty, allow us to compute thresholds, credible intervals, and tail probabilities, and directly address the kinds of questions clinicians ask: “How likely is it that this participant will deteriorate?”, “What is the uncertainty around this risk?”, and “How does new data update that risk?”\n\n5.9.1 Why Bayesian risk modelling is more intuitive\nRisk measures are probabilities by definition or imply the usage of a precise measure of risk.\nA Bayesian posterior distribution gives probability statements about: - the likelihood that a physiologic burden exceed the threshold - the probability that a digital biomarker crosses a critical clinical value - the probability that an individual will experience an event, given their physiology - the probability that a model weight (such as the frailty effect) is positive or negative - the full predictive distribution of outcomes for a new patient\nThese are direct expressions of uncertainty, not the indirect repeated-sampling logic of frequentist inference.\nFor example: - A clinician does not want to know whether the association between HRV and adverse events is “significant at 0.05”. - They want to know: “What is the probability that this participant experiences an event within the next 7 days?” Posterior distributions answer these questions immediately.\n\n\n5.9.2 Bayesian modelling of event risk in the wearable-sensor dataset\nIn the DCT dataset used in this chapter, each participant has a simulated probability of deterioration ( event_risk ) influenced by: - frailty - stress score - inflammation - age\nA fully Bayesian logistic regression model treats these risk contributions as uncertain and estimates directly the posterior distribution of the probability:\n\\[\nP\\left(\\text { event }_i=1 \\mid \\text { physiology }_i, \\text { data }\\right) .\n\\]\nThe output is a distribution for each participant’s risk, not a single number. Thus we can compute: - posterior mean risk - posterior credible intervals for the risk - posterior probability that risk exceeds a clinically important threshold\nFor example, after fitting a Bayesian logistic model:\n\n# Bayesian event-risk model\nlibrary(rstanarm)\n\nfit_event &lt;- stan_glm(\n  event_occurred ~ stress_score + frailty + age + baseline_inflammation,\n  data = wearable_complete,\n  family = binomial(link = \"logit\"),\n  prior = normal(0, 2),\n  prior_intercept = normal(0, 5),\n  chains = 2, iter = 2000, refresh = 0\n)\n\nWe can obtain the posterior distribution of risk for any participant:\n\n# posterior risk for the first patient\nposterior_risk &lt;- posterior_epred(fit_event)[ , 1]\n\nEach of these directly addresses clinical or monitoring questions: - “What is the probability that this participant’s true risk exceeds a critical threshold?” - “What is the \\(95 \\%\\) credible interval for the deterioration risk?” - “How uncertain are we about the risk estimate itself?”\nThese are quantities clinicians can reason about naturally. Posterior predictive distributions as risk objects Bayesian models provide not only parameter posteriors but also posterior predictive distributions, which are essential in risk modelling. For a future observation:\n\\[\ny_{\\text {new }} \\sim p\\left(y_{\\text {new }} \\mid x_{\\text {new }}, \\text { data }\\right),\n\\]\nwe can simulate entire distributions of likely outcomes.\nFor a new participant with given physiology:\n\nnew_patient &lt;- data.frame(\n  age                  = 72,\n  baseline_inflammation = 4.1,\n  frailty              = 0.55,\n  resting_hr           = 82,\n  hrv                  = 45,\n  skin_temp            = 33.8,\n  activity_index       = 52,\n  stress_score         = 67\n)\n\n\n# Posterior event probability draws\nposterior_prob &lt;- posterior_epred(fit_event, newdata = new_patient)\n\n# Summary\nmean_risk &lt;- mean(posterior_prob)\nquantile_risk &lt;- quantile(posterior_prob, c(0.025, 0.5, 0.975))\n\nmean_risk\n\n[1] 0.9893163\n\nquantile_risk\n\n     2.5%       50%     97.5% \n0.9578845 0.9928815 0.9988782 \n\n# Posterior predictive binary outcomes\npp &lt;- posterior_predict(fit_event, newdata = new_patient)\n\n# Summaries\ntable(pp)\n\npp\n   0    1 \n  19 1981 \n\nmean(pp)   # predictive event probability\n\n[1] 0.9905\n\n\nThe previous code block computes posterior event-risk estimates for a new participant by passing their clinical and wearable-sensor profile through the Bayesian logistic regression model. For illustration, consider the following patient:\n\nage \\(=72\\)\nbaseline inflammation = 4.1\nfrailty \\(=0.55\\)\nresting \\(\\mathrm{HR}=82 \\mathrm{bpm}\\)\nHRV \\(=45 \\mathrm{~ms}\\)\nskin temperature \\(=33.8^{\\circ} \\mathrm{C}\\)\nactivity index = 52\nstress score \\(=67\\)\n\nUsing the fitted Bayesian model, the posterior distribution for this patient’s event probability is extremely concentrated near 1 . The posterior mean risk is approximately 0.99 , with a \\(95 \\%\\) credible interval spanning roughly 0.97-0.999. This reflects very strong evidence-given the model, data, and priors-that the patient belongs to a highrisk profile.\nThe posterior predictive distribution (which generates full simulated outcomes rather than probabilities) tells the same story. Across 2,000 posterior predictive draws, 1,974 simulated outcomes were events and 26 were non-events, yielding a predictive event rate of about 0.987 .\nBoth summaries communicate the same conclusion: given this physiological and clinical profile, the model assigns an extremely high probability of deterioration.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html#bayesian-tree-models",
    "href": "introd_bayesian.html#bayesian-tree-models",
    "title": "5  Introduction to Bayesian methods",
    "section": "5.10 Bayesian tree models",
    "text": "5.10 Bayesian tree models\nUp to this point we focused on Bayesian regression, where relationships between predictors and outcomes are represented through linear weights. However, physiological data collected in decentralised trials often involve nonlinear effects, threshold behaviours, and interactions that are difficult to capture with simple parametric models.\nDecision trees and random forests (introduced earlier in the course) address this by partitioning the predictor space into regions and fitting simple models within each leaf. Bayesian tree models follow the same philosophy but add a fully probabilistic framework, yielding posterior uncertainty, credible intervals for predictions, and principled regularisation.\nAmong such models, the most widely used is BART - Bayesian Additive Regression Trees.\nThe BART Model BART represents the response as a sum of many small regression trees:\n\\[\ny_i=\\sum_{t=1}^m g\\left(x_i ; T_t, M_t\\right)+\\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}\\left(0, \\sigma^2\\right) .\n\\]\nEach component includes:\n\n\\(T_t\\) : the structure of the \\(t\\)-th tree (its splits),\n\\(M_t\\) : the set of leaf parameters for that tree,\n\n- \\(g(\\cdot)\\) : a function that assigns the appropriate leaf mean to input \\(x_i\\).\nThe Bayesian specification places priors on all unknowns:\n1. Tree structure prior\nShallow trees are favoured. For a node at depth \\(d\\) :\n\\[\np(\\text { node splits })=\\alpha(1+d)^{-\\beta},\n\\]\n\\[\np(\\text { node splits })=\\alpha(1+d)^{-\\beta},\n\\]\nencouraging regularization via many small trees.\n2. Leaf parameter priors\n\\[\n\\mu_{t \\ell} \\sim \\mathcal{N}\\left(0, \\tau^2 / m\\right),\n\\]\nwhere \\(m\\) is the total number of trees (often 50-200), and \\(\\tau\\) controls shrinkage. 3. Error variance prior\n\\[\n\\sigma^2 \\sim \\operatorname{Inverse-Gamma}(\\nu / 2, \\nu \\lambda / 2) .\n\\]\nTogether, these priors ensure that no single tree dominates; instead, the model builds a smooth ensemble whose posterior is sampled via MCMC.\nThe resulting posterior is:\n\\[\np\\left(T_1, M_1, \\ldots, T_m, M_m, \\sigma \\mid y, X\\right)\n\\]\nand predictions are obtained by averaging over thousands of posterior tree ensembles.\n\n5.10.1 BART possibilities\nBayesian Additive Regression Trees offer a natural extension of Bayesian modelling when physiological relationships become too irregular, nonlinear, or interaction-driven for simple regression to capture. In decentralised clinical-trial datasets, wearable signals often show threshold behaviour—resting heart rate may rise smoothly with frailty until a particular physiological burden is reached, after which the change accelerates sharply. Skin temperature may react differently depending on whether inflammation is mild or severe, and HRV frequently interacts with both age and frailty in ways that defy linear specification. Models that assume a single straight-line relationship can miss these patterns.\nThis is precisely where BART excels. Instead of committing to one functional form, BART models the outcome as the sum of many small regression trees, each capturing a different local pattern in the data. The ensemble adapts automatically to nonlinearity, varying slopes, threshold effects, and interactions—without requiring the analyst to specify those structures in advance. Because the model is Bayesian, every prediction comes with a full posterior distribution. This allows us to express questions in clinically meaningful terms: not only what is the predicted event risk for this participant, but how certain are we about that risk, taking into account all nonlinear physiological interactions implied by the data.\nIn contrast to the linear Bayesian model introduced earlier—which focuses on uncertainty about specific regression weights such as w1w_1w1​—BART centres its uncertainty on the predictions themselves. This shift often aligns more naturally with clinical decision-making. For example, instead of asking whether frailty has a positive effect on resting heart rate, a BART model directly answers questions such as: Given the full physiological profile of a participant, what is the posterior distribution of their probability of deterioration? In digital-health applications where risk stratification matters more than explicit coefficient interpretation, BART provides a flexible, uncertainty-aware alternative.\n\n\n5.10.2 Fitting BART with R\n\n#install.packages(\"dbarts\")\nlibrary(dbarts)\n\n\nAttaching package: 'dbarts'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nX &lt;- wearable_complete[, c(\"frailty\", \"baseline_inflammation\")]\ny &lt;- wearable_complete$resting_hr\n\nbart_fit &lt;- dbarts::bart(\n  x.train = X,\n  y.train = y,\n  ntree   = 50,      # number of trees\n  ndpost  = 2000,    # posterior draws\n  nskip   = 1000,    # burn-in\n  keeptrees = TRUE\n)\n\n\nRunning BART with numeric y\n\nnumber of trees: 50\nnumber of chains: 1, default number of threads 1\ntree thinning rate: 1\nPrior:\n    k prior fixed to 2.000000\n    degrees of freedom in sigma prior: 3.000000\n    quantile in sigma prior: 0.900000\n    scale in sigma prior: 0.003912\n    power and base for tree prior: 2.000000 0.950000\n    use quantiles for rule cut points: false\n    proposal probabilities: birth/death 0.50, swap 0.10, change 0.40; birth 0.50\ndata:\n    number of training observations: 500\n    number of test observations: 0\n    number of explanatory variables: 2\n    init sigma: 8.854226, curr sigma: 8.854226\n\nCutoff rules c in x&lt;=c vs x&gt;c\nNumber of cutoffs: (var: number of possible c):\n(1: 100) (2: 100) \nRunning mcmc loop:\niteration: 100 (of 2000)\niteration: 200 (of 2000)\niteration: 300 (of 2000)\niteration: 400 (of 2000)\niteration: 500 (of 2000)\niteration: 600 (of 2000)\niteration: 700 (of 2000)\niteration: 800 (of 2000)\niteration: 900 (of 2000)\niteration: 1000 (of 2000)\niteration: 1100 (of 2000)\niteration: 1200 (of 2000)\niteration: 1300 (of 2000)\niteration: 1400 (of 2000)\niteration: 1500 (of 2000)\niteration: 1600 (of 2000)\niteration: 1700 (of 2000)\niteration: 1800 (of 2000)\niteration: 1900 (of 2000)\niteration: 2000 (of 2000)\ntotal seconds in loop: 0.257905\n\nTree sizes, last iteration:\n[1] 2 2 2 4 3 2 2 2 2 4 3 3 2 3 2 2 2 2 \n2 1 3 2 5 2 2 1 2 3 2 2 2 2 3 3 3 2 3 2 \n3 2 2 2 2 2 2 2 4 3 2 2 \n\nVariable Usage, last iteration (var:count):\n(1: 39) (2: 30) \nDONE BART\n\n\n\n# posterior predictions for the training data\nbart_pred &lt;- bart_fit$yhat.train\n\n# posterior mean and 95% credible interval for first individual\nmean_pred &lt;- mean(bart_pred[, 1])\nci_pred   &lt;- quantile(bart_pred[, 1], c(0.025, 0.975))\n\nmean_pred\n\n[1] 78.34089\n\nci_pred\n\n    2.5%    97.5% \n74.50905 82.38365 \n\n\nIn the previous section we introduced Bayesian Additive Regression Trees (BART) as a flexible, fully Bayesian non-parametric regression model. After fitting the model with the wearable-sensor features frailty and baseline_inflammation as predictors of resting_hr, the BART sampler produced 2,000 posterior draws after a 1,000-iteration burn-in phase. These draws represent the posterior uncertainty over the entire ensemble of regression trees.\nBART naturally provides posterior predictive samples for each individual in the dataset. Extracting these samples allows us to summarise the expected resting heart rate for any participant and to compute uncertainty bands directly from the posterior.\nFor illustration, the following summary corresponds to the first participant in the dataset:\n\nPosterior mean prediction: approximately 78.3 bpm\n95% posterior credible interval: [74.3, 82.5] bpm\n\nThis interval quantifies genuine uncertainty about the individual’s true resting heart rate under the model—incorporating nonlinear effects, interactions, and the full posterior over tree structures. Unlike classical intervals derived from linear regression, BART’s uncertainty directly reflects the model’s flexibility and the posterior distribution of all trees.\nThese posterior predictive summaries are directly usable in clinical decision-support settings. Instead of producing a single point forecast, BART offers a full distribution describing all plausible physiological values, making it naturally aligned with risk-aware modelling of wearable-trial data.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "introd_bayesian.html#bayesian-tree-models-complementing-random-forests-and-xgboost",
    "href": "introd_bayesian.html#bayesian-tree-models-complementing-random-forests-and-xgboost",
    "title": "5  Introduction to Bayesian methods",
    "section": "5.11 Bayesian Tree Models: Complementing Random Forests and XGBoost",
    "text": "5.11 Bayesian Tree Models: Complementing Random Forests and XGBoost\nIn the previous chapter we examined classical tree-based ensemble methods — Random Forests and Gradient Boosted Trees (XGBoost). Both approaches achieve strong predictive performance by aggregating many simple decision trees, and both are well suited to nonlinearities and complex interaction structures commonly observed in wearable-derived physiological signals. What they do not provide, however, is a coherent way to quantify uncertainty. Their predictions are deterministic functions of the training sample, and uncertainty must be approximated through ad-hoc resampling or assumptions that do not fully reflect the model structure.\nBayesian Additive Regression Trees (BART) extends the same foundational idea — representing a regression function as the sum of many small trees — but places the entire ensemble inside a Bayesian framework. This makes BART particularly appealing for clinical and digital-health applications, where risk quantification, uncertainty intervals, and probability statements are more clinically meaningful than point estimates alone.\nRevisiting the Structure of a Tree Ensemble Random Forests and XGBoost rely on a common representation:\n\\[\nf(x)=\\sum_{t=1}^m g\\left(x ; T_t, M_t\\right),\n\\]\nwhere each \\(T_t\\) defines a set of splits, and the leaf parameters \\(M_t\\) give the predicted value within each region. These methods differ in how the trees are constructed - RF uses bootstrap samples and decorrelated predictors, XGBoost uses sequential boosting - but the structure is always an additive sum of trees.\nBART adopts exactly the same form, but interprets all unknowns as random variables with prior distributions. Instead of producing a single fitted ensemble, BART samples thousands of different ensembles from the posterior, each a plausible explanation of the data. This makes its predictions inherently probabilistic.\nWhy BART is Different Where Random Forests and XGBoost return a single prediction, BART returns a posterior predictive distribution. For every participant in a decentralised trial, we obtain a distribution of likely physiological values or event risks, not just a central estimate.\nThis improves modelling in several ways: - nonlinearities such as abrupt increases in resting HR at high frailty levels are learned in a fully datadriven way; - interactions among physiology measures emerge naturally (e.g., inflammation modifying the effect of frailty); - uncertainty is preserved and propagated through all layers of the model, making predictions interpretable as risk distributions rather than isolated numbers.\nIn contexts where clinical decisions depend not only on expected values but on the range of plausible outcomes, BART offers a more intuitive probabilistic interpretation than traditional ensembles.\n\n5.11.1 Running RF and Xgboosst\n\nlibrary(randomForest)\n\nrandomForest 4.7-1.2\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n# Random Forest model\nrf_fit &lt;- randomForest(\n  x = X, y = y,\n  ntree = 500,\n  mtry  = 2\n)\n\n# XGBoost model\ndmat &lt;- xgb.DMatrix(data = as.matrix(X), label = y)\n\nxgb_fit &lt;- xgboost(\n  data = dmat,\n  objective = \"reg:squarederror\",\n  nrounds = 300,\n  eta = 0.05,\n  max_depth = 3,\n  verbose = 0\n)\n\nrf_pred  &lt;- predict(rf_fit, X)\nxgb_pred &lt;- predict(xgb_fit, as.matrix(X))\n\n\n# RMSE helper\nrmse &lt;- function(truth, pred) {\n  sqrt(mean((truth - pred)^2))\n}\n\n# BART posterior mean prediction for each individual\nbart_mean_pred &lt;- rowMeans(bart_pred)\n\n# RMSEs\nrmse_bart &lt;- rmse(y, bart_mean_pred)\nrmse_rf   &lt;- rmse(y, rf_pred)\nrmse_xgb  &lt;- rmse(y, xgb_pred)\n\nrmse_bart\n\n[1] 12.90669\n\nrmse_rf\n\n[1] 4.350984\n\nrmse_xgb\n\n[1] 6.349438\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nRMSE (↓ better)\nHandles Nonlinearity\nUncertainty Quantification\nInteractions Automatically\nPosterior Predictive Dist.\nNotes\n\n\n\n\nBART\n12.89\nYes\nFull posterior\nYes\nYes\nLower accuracy but richest uncertainty + Bayesian structure\n\n\nRandom Forest\n4.35\nYes\nNo\nYes\nNo\nBest predictive RMSE but no probabilistic interpretation\n\n\nXGBoost\n6.35\nYes\nNo\nYes (via splits)\nNo\nStrong performance, tunable, deterministic\n\n\n\nAlthough Random Forests and XGBoost do not provide native posterior uncertainty, approximate measures can sometimes be obtained through resampling. In practice this is typically done by:\nBootstrapping the training data (fit many RF/XGB models on resampled datasets)\nEstimating variability across predictions\nHowever, these procedures are not Bayesian, they do not propagate structural model uncertainty, and they lack a coherent probabilistic interpretation. The resulting intervals are often wider, unstable, or sensitive to tuning choices. This stands in contrast to BART, where uncertainty arises directly from the posterior distribution over entire ensembles of trees.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Bayesian methods</span>"
    ]
  },
  {
    "objectID": "intro_missing.html",
    "href": "intro_missing.html",
    "title": "6  Introduction to missing data",
    "section": "",
    "text": "6.1",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to missing data</span>"
    ]
  },
  {
    "objectID": "intro_missing.html#motivation",
    "href": "intro_missing.html#motivation",
    "title": "6  Introduction to missing data",
    "section": "6.2 Motivation",
    "text": "6.2 Motivation\nThis chapter introduces the statistical foundations and practical tools for analysing datasets with missing values, using the amputated version of the wearable-sensor dataset introduced in the previous chapter. Although simulated, the dataset mirrors the types of incomplete streams routinely encountered in decentralised clinical trials, where remote monitoring depends on device adherence, sensor quality, and participant characteristics.\n\n  #install.packages(\"tidyverse\")\n  library(tidyverse)\n  wear &lt;- readRDS(\"~/att_ai_ml/att_book/data/dct_wearable_amputated.rds\")\n  wear_complete &lt;- readRDS(\"~/att_ai_ml/att_book/data/dct_wearable_complete.rds\")\n\n## Why missing data matters\nMissing data is often introduced as a technical inconvenience - something that “just happens” in realworld datasets. But its implications are far deeper. To appreciate why careful methods are needed, we begin with the simplest possible statistical quantity: the sample mean.\nEven this basic calculation becomes ambiguous as soon as one value is missing. 1. The mean requires all values to be known\nThe sample mean is defined as:\n\\[\n  \\bar{x}=\\frac{1}{n} \\sum_{i=1}^n x_i= x_1+x_2+....+n_n\n  \\]\nThis formula contains an implicit assumption: every value \\(x_i\\) exists and is observed.\nIf one value is missing, the formula does not tell us what to do. There is no instruction in: \\[\n  \\sum_{i=1}^n x_i\n  \\]\nthat explains how to add something that does not exist.\n\n  x &lt;- c(10, 12, 14, 16)\n  mean(x)\n\n[1] 13\n\n\nThe computation is straightforward:\n\\[\n  \\bar{x}=\\frac{10+12+14+16}{4}=13\n  \\]\nEverything works.\n\n  x &lt;- c(10, 12, NA, 16)\n  mean(x)\n\n[1] NA\n\n\nBecause mathematically we are now trying to compute:\n\\[\n  \\bar{x}=\\frac{10+12+?+16}{4}\n  \\]\nThere is no rule for the missing term. The mean is effectively undefined without further assumptions.\nIf we ignore the missing value, we change the problem\nR allows this option:\n\n  mean(x, na.rm = TRUE)\n\n[1] 12.66667\n\n\nThis corresponds to:\n\\[\n  \\frac{10+12+16}{3}=12.67\n  \\]\nBut notice: - one value was dropped, - the total number of values changed from 4 to 3, - and the resulting mean answers a different question.\nThis is equivalent to assuming the data are MCAR (Missing Completely At Random), a concept we will discuss later in this chapter. If that assumption is not true, the result is biased - even for a simple mean.\n### Arbitrary replacements produce arbitrarily different means\nSuppose someone fills the missing value with zero:\n::: {.cell}\nx_zero &lt;- ifelse(is.na(x), 0, x)\nmean(x_zero)\n::: {.cell-output .cell-output-stdout}\n[1] 9.5\n::: :::\nWe obtain:\n\\[\n  (10+12+0+16) / 4=9.5\n  \\]\nNow the mean has changed drastically simply because we chose an arbitrary replacement.\nOr we could replace the missing value with the observed mean:\n\n  x_meanfilled &lt;- ifelse(is.na(x), mean(x, na.rm = TRUE), x)\n  mean(x_meanfilled)\n\n[1] 12.66667\n\n\nThis returns the same mean as the complete-case mean (12.67), but again, this is not the true mean of the original data — it is a constructed quantity.\nEven the simplest statistic — the mean — becomes:\n\nundefined\nsensitive to assumptions\nbiased if assumptions are incorrect\nas soon as missing data appear.\n\nThis motivates why missing data mechanisms (MCAR, MAR, MNAR) must be understood and why naive fixes such as dropping rows or plugging in arbitrary values rarely produce valid results. We will address these terms later.\nThis simple example sets up the core idea:\nIf we cannot define a mean without assumptions, then regression models — which involve multiple variables and complex relationships — are even more sensitive to missing data.\nThis naturally leads into your next section that show how missingness may affect modelling.\n## Naive approaches to handling missing data\nBefore introducing principled statistical approaches, it is helpful to examine the simplistic strategies that analysts often apply when confronted with missing values. These methods are appealing because they are easy to use and require no modelling assumptions. However, as the examples below demonstrate, they frequently distort statistical relationships and lead to biased or unstable results.\nOne of the most common strategies is listwise deletion, sometimes called complete-case analysis. In this approach, any row that contains at least one missing value is entirely removed from the dataset. Many statistical functions in R use this strategy by default, so it often happens silently. To illustrate how aggressively this approach reduces the number of observations, consider the amputated wearable dataset:\n\n  nrow(wear)                     # original number of rows\n\n[1] 500\n\n  sum(!complete.cases(wear))    # number of rows with any missing\n\n[1] 134\n\n  nrow(na.omit(wear))           # rows remaining after listwise deletion\n\n[1] 366\n\n\nThis shows how many complete observations remain after removing all incomplete ones. The key point is that the resulting dataset is smaller and may no longer reflect the population originally sampled. Even simple descriptive statistics can change after discarding incomplete cases. For example:\n\n  mean(wear$hrv, na.rm = TRUE)                 # using all available HRV values\n\n[1] 43.67949\n\n  mean(na.omit(wear)$hrv)                      # HRV mean among complete cases only\n\n[1] 46.40513\n\n\nThese two values are rarely identical. The second is computed on a substantially reduced subset of participants.\nA slightly more flexible naive strategy is pairwise deletion, which uses each available pair of variables when computing correlations or covariances. Instead of removing a whole row, pairwise deletion removes only the missing component for the calculation at hand. This often yields more data points for each correlation but leads to inconsistencies because each correlation may be based on a different subset of individuals. Consider the following comparisons:\n\n  cor(wear$hrv, wear$frailty, use = \"pairwise.complete.obs\")\n\n[1] -0.4985907\n\n  cor(wear$hrv, wear$resting_hr, use = \"pairwise.complete.obs\")\n\n[1] -0.7745626\n\n  cor(wear$frailty, wear$resting_hr, use = \"pairwise.complete.obs\")\n\n[1] 0.4838572\n\n\nEach correlation uses a different sample size:\n\n  n_obs_hrv_frailty     &lt;- sum(complete.cases(wear[, c(\"hrv\", \"frailty\")]))\n  n_obs_hrv_rhr         &lt;- sum(complete.cases(wear[, c(\"hrv\", \"resting_hr\")]))\n  n_obs_frailty_rhr     &lt;- sum(complete.cases(wear[, c(\"frailty\", \"resting_hr\")]))\n  \n  c(n_obs_hrv_frailty, n_obs_hrv_rhr, n_obs_frailty_rhr)\n\n[1] 452 427 470\n\n\nBecause the number of observations differs across pairs, the resulting covariance matrix can become internally inconsistent, which can cause multivariate methods (e.g. PCA, factor analysis, some ML algorithms) to fail or behave erratically.\nAnother widely used naive strategy is single-value imputation, where missing values are replaced with a constant such as zero, the mean, or the median. This makes the dataset look complete, but at the cost of altering its statistical structure. For instance, replacing missing values with the mean collapses variability and suppresses natural relationships between variables. Below is an example using HRV:\n\n  hrv_mean &lt;- mean(wear$hrv, na.rm = TRUE)\n  hrv_imputed &lt;- ifelse(is.na(wear$hrv), hrv_mean, wear$hrv)\n  \n  sd(wear$hrv, na.rm = TRUE)      # true variability\n\n[1] 20.98012\n\n  sd(hrv_imputed)                 # variability after mean substitution\n\n[1] 19.94555\n\n\nThe second standard deviation is always smaller, because this imputation pushes missing values toward a single central point. The effect on correlations can also be demonstrated:\n\n  cor(wear$hrv, wear$frailty, use = \"complete.obs\")  # using true observed values\n\n[1] -0.4985907\n\n  cor(hrv_imputed, wear$frailty, use = \"complete.obs\")  # after mean imputation\n\n[1] -0.4819104\n\n\nMean imputation often shrinks correlations toward zero, weakening relationships that may be scientifically meaningful.\nUsing a fixed constant such as zero can create even more distortion, especially when the variable lacks a natural zero point. For example:\n\n  hrv_zero &lt;- ifelse(is.na(wear$hrv), 0, wear$hrv)\n  summary(hrv_zero)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -13.06   23.49   40.12   39.49   57.99   99.40 \n\n\nSuddenly, the dataset contains physiologically implausible values, which can create artificial clusters or misleading model behaviour downstream.\nMedian substitution works similarly:\n\n  hrv_median &lt;- median(wear$hrv, na.rm = TRUE)\n  hrv_median_imp &lt;- ifelse(is.na(wear$hrv), hrv_median, wear$hrv)\n  summary(hrv_median_imp)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -13.06   29.69   43.67   43.68   57.99   99.40 \n\n\nAlthough more robust to outliers, median imputation still collapses variability and reduces correlation strength.\nThese naive strategies all share a common problem: they treat missing values through arbitrary conventions rather than statistical reasoning. Removing rows, dropping individual entries, or filling gaps with constants alters the distribution of the data and breaks relationships among variables. Even before we discuss more formal missing data frameworks, the R examples above already suggest that each naive strategy produces a dataset that differs in meaningful ways from the original measurements.\n\n  library(ggplot2)\n  library(dplyr)\n  library(patchwork)   # to arrange side-by-side\n  \n  # Data for pairwise (only remove missing frailty)\n  df_pair &lt;- wear %&gt;%\n    select(frailty, hrv) %&gt;%\n    filter(!is.na(frailty))\n  \n  # Data for listwise (remove rows missing frailty OR HRV)\n  df_list &lt;- df_pair %&gt;%\n    filter(!is.na(hrv))\n  \n  # --- Plot A: Pairwise available --------------------------------------------\n  p_pair &lt;- ggplot(df_pair, aes(x = frailty, y = hrv)) +\n    geom_point(alpha = 0.6, color = \"#1f77b4\") +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n    labs(\n      title = \"Pairwise Available\",\n      subtitle = \"Uses all available Frailty–HRV pairs\",\n      x = \"Frailty\",\n      y = \"HRV (ms)\"\n    ) +\n    theme_minimal(base_size = 13)\n  \n  # --- Plot B: Listwise deletion ---------------------------------------------\n  p_list &lt;- ggplot(df_list, aes(x = frailty, y = hrv)) +\n    geom_point(alpha = 0.6, color = \"#d62728\") +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n    labs(\n      title = \"Listwise Deletion\",\n      subtitle = \"Rows with ANY missing value are removed\",\n      x = \"Frailty\",\n      y = \"HRV (ms)\"\n    ) +\n    theme_minimal(base_size = 13)\n  \n  # --- Combine side-by-side --------------------------------------------------\n  p_pair + p_list\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 48 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 48 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe first pair of scatterplots shows how two common deletion strategies reshape the observed relationship between Frailty and HRV.\nThe pairwise deletion panel uses every available Frailty–HRV pair, regardless of whether the same participant has missing values elsewhere. Because almost all observed pairs are retained, the point cloud preserves the full physiological range of both variables. The negative association between frailty and HRV remains clearly visible, and the fitted regression line reflects the true structure of the data.\nIn the listwise deletion panel, any participant with any missing variable is removed entirely. This eliminates many individuals who tend to be older, frailer, or have low HRV — precisely the groups more prone to missingness in this simulated dataset. The resulting point cloud is therefore a biased slice of the population, concentrated in a narrower region. The regression line shifts accordingly, reflecting the selective loss of extreme values. The figure makes explicit that listwise deletion is not just a reduction in sample size; it changes the analytic population itself.\n\n  library(tidyverse)\n  library(patchwork)\n  \n  hrv_mean &lt;- mean(wear$hrv, na.rm = TRUE)\n  \n  df_imp &lt;- wear %&gt;%\n    mutate(\n      hrv_original = hrv,\n      hrv_meanimp  = ifelse(is.na(hrv), hrv_mean, hrv)\n    )\n  \n  # Plot A: Original HRV distribution\n  p_orig &lt;- ggplot(df_imp, aes(x = hrv_original)) +\n    geom_histogram(bins = 30, fill = \"#1b9e77\", alpha = 0.8, color = \"white\") +\n    labs(\n      title = \"Original HRV Distribution\",\n      x = \"HRV (ms)\", y = \"Count\"\n    ) +\n    theme_minimal(base_size = 14)\n  \n  # Plot B: Mean-imputed HRV distribution\n  p_imp &lt;- ggplot(df_imp, aes(x = hrv_meanimp)) +\n    geom_histogram(bins = 30, fill = \"#7570b3\", alpha = 0.8, color = \"white\") +\n    labs(\n      title = \"Mean-Imputed HRV Distribution\",\n      subtitle = \"All missing values accumulate at one point\",\n      x = \"HRV (ms)\", y = \"Count\"\n    ) +\n    theme_minimal(base_size = 14)\n  \n  # Combine side by side\n  p_orig + p_imp\n\nWarning: Removed 48 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nThe second pair of histograms demonstrates what happens when missing HRV values are filled in with a single constant — the sample mean. In the original distribution, HRV shows a wide, natural spread driven by true physiological variation between participants.\nAfter mean imputation, however, all missing HRV values collapse to a single point. This produces a tall, unnatural spike in the histogram and compresses the variability of the variable. The imputed distribution no longer reflects real measurements but instead a mixture of genuine values and an artificial pile-up at the mean. This collapse reduces variance, distorts density, and weakens downstream correlations.\nThe figure highlights the core issue: single-value imputation does not reconstruct the missing information — it injects new artefacts into the data.\nThis is why more principled methods—such as likelihood-based approaches, fully conditional specification, and multiple imputation—have become standard in modern statistical practice. Before reaching those techniques, the next section illustrates how these naive methods affect regression modelling, showing that even a correctly specified linear model can yield biased coefficients when missing values are handled in simplistic ways.\n## Missingness affect models\nTo understand how missing data affects real modelling tasks, we compare the behaviour of a simple linear regression under two conditions:\n\nA complete dataset, where all variables are fully observed\nAn amputated dataset, where several variables contain missing values due to MCAR and MAR mechanisms\n\nThe model of interest predicts the continuous physiological endpoint stress_score from three key predictors: frailty, HRV, and resting heart rate.\nThe two models are fitted as follows:\n\n    # Regression on complete dataset (true underlying coefficients)\n    fit_full &lt;- lm(stress_score ~ frailty + hrv + resting_hr, data = wear_complete)\n    \n    # Regression on amputated dataset (listwise deletion)\n    fit_drop &lt;- lm(stress_score ~ frailty + hrv + resting_hr, data = wear)\n    \n    summary(fit_full)    # reference model\n\n\nCall:\nlm(formula = stress_score ~ frailty + hrv + resting_hr, data = wear_complete)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-24.1154  -4.4675  -0.0522   4.8940  24.8463 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 51.76130    3.79907  13.625  &lt; 2e-16 ***\nfrailty     11.06184    2.84225   3.892 0.000113 ***\nhrv         -0.51474    0.02552 -20.170  &lt; 2e-16 ***\nresting_hr   0.67950    0.04163  16.321  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.365 on 496 degrees of freedom\nMultiple R-squared:  0.8731,    Adjusted R-squared:  0.8723 \nF-statistic:  1138 on 3 and 496 DF,  p-value: &lt; 2.2e-16\n\n    summary(fit_drop)    # biased estimates under missingness\n\n\nCall:\nlm(formula = stress_score ~ frailty + hrv + resting_hr, data = wear)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.9040  -4.5572  -0.0739   4.7204  24.6786 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 50.27216    4.21299  11.933  &lt; 2e-16 ***\nfrailty      9.40451    3.18137   2.956  0.00329 ** \nhrv         -0.50889    0.02815 -18.078  &lt; 2e-16 ***\nresting_hr   0.70808    0.04670  15.161  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.496 on 423 degrees of freedom\n  (73 observations deleted due to missingness)\nMultiple R-squared:  0.8665,    Adjusted R-squared:  0.8655 \nF-statistic: 915.1 on 3 and 423 DF,  p-value: &lt; 2.2e-16\n\n\nThe outputs below illustrate clearly how missingness distorts model estimation. We begin with the regression fitted on the wear_complete dataset, where all variables are fully observed. The estimated coefficients represent the true underlying relationships from the simulated system: the intercept is approximately 51.76, frailty has a strong positive effect of about 11.06 units on the stress score, HRV has a negative effect of roughly –0.515, and resting heart rate contributes positively with a coefficient of 0.679. These values align perfectly with the data-generating mechanism, in which frailty and resting HR increase physiological load while HRV reduces it. Because the regression formula matches the structure of the simulation, the model fits extremely well, with an R² around 0.87 and stable residual behaviour.\nWhen we fit the same regression to the amputated dataset—using R’s default behaviour of listwise deletion—the picture changes. The estimated coefficients shift to (Intercept) ≈ 50.27, frailty ≈ 9.40, HRV ≈ –0.509, and resting HR ≈ 0.708. At first glance, the results might appear similar, but the changes are meaningful. R silently removes 73 observations from the dataset, reducing the sample size from 500 to 427. Crucially, these dropped observations are not a random slice of the population. HRV is missing disproportionately among older individuals, resting heart rate is missing more frequently among frail participants, and skin temperature tends to be missing when stress levels are high. Only the missingness in the activity index is MCAR, and this alone is not enough to preserve the overall population structure.\nBecause these missingness mechanisms are MAR rather than MCAR, listwise deletion effectively fits the regression on a reweighted subset of individuals—those who happen, by the rules of the simulation, to be younger, less frail, and under less physiological stress. As a result, the model systematically underestimates the effect of frailty: in the complete dataset, frailty has a coefficient of 11.06, but under listwise deletion it drops to 9.40. This downward bias arises because the frailest participants are precisely those most likely to have missing HR or HRV measurements, and therefore the sample used in the regression underrepresents the part of the distribution where the frailty–stress relationship is strongest. The other coefficients also shift, though less dramatically: HRV changes from –0.5147 to –0.5089, and resting HR moves from 0.6795 to 0.7081. These are smaller distortions but still reflect the same underlying issue—regression coefficients estimated from a non-random subset of the data no longer represent population-level effects.\nModel fit deteriorates as well. The residual standard error increases from 7.36 to 7.50, the degrees of freedom fall from 496 to 423, and R² decreases slightly from 0.873 to 0.866. These changes arise from two sources: a loss of information due to discarding valid observations, and a distortion of the sample because the missingness mechanism selectively removes specific groups of individuals. Both consequences are typical of listwise deletion under MAR or MNAR conditions.\nThe broader lesson is that missing data can bias model estimates even when the model is correctly specified and even when the proportion of missingness is not very large. Listwise deletion wastes information, is only unbiased under the strong MCAR assumption, and often yields coefficients that differ from the truth in subtle or non-obvious ways. It alters the composition of the analytic sample and produces results that may no longer reflect the underlying population relationships. These issues motivate more principled approaches such as multiple imputation, Bayesian treatments of missingness, and model-based procedures such as EM or fully conditional specification. The comparison between the complete-data regression and the listwise-deletion regression shows that ignoring missing data is not a benign choice—it fundamentally alters the scientific conclusions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to missing data</span>"
    ]
  },
  {
    "objectID": "intro_missing.html#missing-mechanisms",
    "href": "intro_missing.html#missing-mechanisms",
    "title": "6  Introduction to missing data",
    "section": "6.3 Missing mechanisms",
    "text": "6.3 Missing mechanisms\nOnce we have seen that naïve approaches distort even the simplest statistical quantities, we arrive at a more foundational question: why do observations go missing in the first place? This question is not a nuisance or a side issue; it is the core of the missing-data problem. From a statistical point of view, the process that generates missingness is itself a random variable. It has its own probability distribution, just like the physiological variables in the wearable dataset, and should be modelled accordingly. This viewpoint connects naturally with the Bayesian perspective discussed in the previous chapter: unobserved values are uncertain quantities, and the act of becoming missing is also governed by uncertainty that can, in principle, be described probabilistically.\nFormally, we represent missingness with an indicator \\(R_{\\text {, }}\\) where \\(R_i=1\\) if an observation is present and \\(R_i=0\\) if it is missing. The full dataset consists of the observed values \\(Y_{\\text {obs }}\\), the unobserved values \\(Y_{\\text {mis, }}\\) and the missingness indicators \\(R\\). Understanding the mechanism means describing the conditional distribution \\(P\\left(R \\mid Y_{\\text {obs }}, Y_{\\text {mis }}, X\\right)\\), where \\(X\\) may contain other covariates. Rubin’s framework distinguishes three broad types of mechanisms, which differ in how missingness relates to the data themselves and ultimately determine which analysis methods are valid.\nThe most restrictive mechanism is Missing Completely At Random (MCAR). Under MCAR, the probability of a value being missing does not depend on anything in the dataset-not on observed measurements, not on unobserved ones, and not on any auxiliary in \\({ }^{\\prime} \\downarrow\\) nation. Mathematically this corresponds to \\(P(R \\mid Y_{\\text {obs }}, Y_{\\text {mis }, X}=P(R)\\). Missingness is thus pure random noise. When MCAR holds, the observedsample is essentially a random subsample of the full dataset, which implies that complete-case analyses remain unbiased. In practice, MCAR is rare. In your simulated wearable dataset, the only MCAR process appears in the activity index, where a set of observations was removed by random selection without reference to any physiological variable. A diagnostic plot comparing frailty between observed and missing groups for activity index typically shows nearly identical distributions, which visually reinforces the MCAR assumption.\nA more realistic mechanism for most real-world datasets is Missing At Random (MAR). Here, the probability of missingness may depend on observed quantities but not on the missing value itself. The formal expression is \\(P\\left(R \\mid Y_{\\text {obs }}, Y_{\\text {mis }}, X\\right)=P\\left(R \\mid Y_{\\text {obs }}, X\\right)\\). The key idea is that once we condition on what we have already measured, the missingness process no longer depends on unobserved values. This is still an assumption, but it is often plausible. The wearable dataset provides natural examples. Heart-rate variability is missing more frequently among older participants; resting heart rate tends to be missing among frailer individuals; and skin temperature drops out more often when stress levels are high. In all of these cases, the missingness is related to variables that are themselves observed. Visualizing these relationships is straightforward: we can contrast the distribution of age across observed versus missing HRV, or the distribution of stress for observed versus missing skin temperature. The boxplots show clear separation between groups, indicating that missingness is systematically linked to measured covariates. This MAR structure is precisely what caused the bias in the earlier regression example: listwise deletion removed older, frailer, and highly stressed individuals at higher rates, producing a distorted analytic sample.\nThe most complex mechanism is Missing Not At Random (MNAR). Under MNAR, the probability of missingness depends on unobserved information, including the missing value itself. In formal terms, \\(P\\left(R \\mid Y_{\\text {obs }}, Y_{\\text {mis }}, X\\right)\\) cannot be simplified to depend only on \\(Y_{\\text {obs }}\\) and \\(X\\). This arises, for instance, if low HRV values fail to record because the device malfunctions specifically at low HRV, or if individuals with extremely high stress deliberately hide their sensor data. In such cases, neither complete-case analysis nor multiple imputation under MAR will fully correct the bias, because the source of missingness is tied directly to values we do not get to observe. Addressing MNAR typically requires explicit modelling of the missingness mechanism itself, using selection models, pattern-mixture models, or fully Bayesian joint models in which missingness is part of the likelihood.\nThese mechanisms matter because each one leads to different analytical consequences. Under MCAR, simply deleting incomplete rows yields unbiased estimates, although with reduced efficiency. Under MAR, deletion becomes biased, but likelihood-based methods and multiple imputation remain valid because the missingness can be explained by variables already present in the dataset. Under MNAR, neither deletion nor standard imputation methods are reliable, since the missingness process depends on information we never observe. The wearable simulation, by including MCAR and MAR mechanisms but avoiding MNAR, provides an ideal environment for demonstrating both the failure of naïve methods and the success of principled approaches such as multiple imputation.\n\n    ## -----------------------------------------------------------\n    ## Diagnostics for Missing Data Mechanisms\n    ## Applied to the DCT wearable dataset\n    ## -----------------------------------------------------------\n    #install.packages(\"naniar\")\n    library(tidyverse)\n    library(naniar)\n    \n    # Reload datasets (in case this chunk runs independently)\n    #wear &lt;- readRDS(\"~/att_ai_ml/att_book/data/dct_wearable_amputated.rds\")\n    #wear_complete &lt;- readRDS(\"~/att_ai_ml/att_book/data/dct_wearable_complete.rds\")\n    \n    \n    ## -----------------------------------------------------------\n    ## 2. MCAR example: Activity Index missing completely at random\n    ## -----------------------------------------------------------\n    \n    wear %&gt;%\n      mutate(miss_act = is.na(activity_index)) %&gt;%\n      ggplot(aes(x = miss_act, y = frailty)) +\n      geom_boxplot(fill = \"#66c2a5\") +\n      labs(\n        title = \"Activity Index Missingness vs Frailty\",\n        subtitle = \"Distributions are similar → consistent with MCAR\",\n        x = \"Activity Index Missing?\",\n        y = \"Frailty\"\n      ) +\n      theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n  ## -----------------------------------------------------------\n  ## MCAR Probability Curve (Activity Index)\n  ## Missingness does NOT depend on frailty\n  ## -----------------------------------------------------------\n  \n  library(tidyverse)\n  \n  # Build dataset: frailty + missingness indicator\n  df_mcar &lt;- wear %&gt;%\n    mutate(miss_act = is.na(activity_index)) %&gt;%\n    # group frailty for smoothing\n    mutate(frailty_bin = cut(frailty, breaks = 30)) %&gt;%\n    group_by(frailty_bin) %&gt;%\n    summarise(\n      frailty_mid = mean(frailty, na.rm = TRUE),\n      p_miss = mean(miss_act)\n    ) %&gt;%\n    drop_na()\n  \n  # Plot probability curve\n  ggplot(df_mcar, aes(x = frailty_mid, y = p_miss)) +\n    geom_line(color = \"#66c2a5\", size = 1.2) +\n    labs(\n      title = \"Probability of Activity Index Missingness\",\n      subtitle = \"Flat pattern → consistent with MCAR\",\n      x = \"Frailty\",\n      y = \"Pr(Missing)\"\n    ) +\n    theme_minimal(base_size = 14)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThe code above performs an exploratory diagnostic aimed at assessing whether the activity index follows a Missing Completely At Random (MCAR) mechanism. The logic is simple: we construct an indicator marking whether the activity index is missing and then compare the distribution of frailty between the missing and observed groups using a boxplot. If missingness truly behaves like random noise, these two distributions should look similar.\nIn this example, the resulting figure shows that the frailty distributions overlap almost perfectly. The medians align, the interquartile ranges are comparable, and there is no systematic shift that would suggest frailty influences the probability of missing activity data. This pattern is compatible with an MCAR process, where the missingness is independent of participant characteristics.\nIt is important, however, to emphasize that such a visual comparison is only exploratory. Boxplots offer a convenient summary, but they are not a formal test of the missingness mechanism. Even if the distributions appear similar, this does not prove MCAR; it merely suggests an absence of obvious violations. If stronger evidence were required, we could complement the visual inspection with statistical comparisons—such as a t-test, a Wilcoxon rank-sum test, or a logistic regression predicting missingness from frailty—but even these tests cannot definitively confirm MCAR. Missingness mechanisms refer to underlying population processes, and no finite sample can conclusively validate the assumption.\nIn this simulated dataset, the similarity between groups is expected, since the activity index was deliberately designed to drop out at random. Real-world data seldom display such clean patterns. The example therefore provides a helpful baseline for what MCAR might look like in practice while also illustrating why genuine MCAR mechanisms are rare and why conclusions should be drawn cautiously.\n\n    ## -----------------------------------------------------------\n    ## 3. MAR example: HRV missing more often at older ages\n    ## -----------------------------------------------------------\n    \n    wear %&gt;%\n      mutate(miss_hrv = is.na(hrv)) %&gt;%\n      ggplot(aes(x = miss_hrv, y = age)) +\n      geom_boxplot(fill = \"#fc8d62\") +\n      labs(\n        title = \"HRV Missingness vs Age\",\n        subtitle = \"Older participants have more missing HRV → MAR\",\n        x = \"HRV Missing?\",\n        y = \"Age\"\n      ) +\n      theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n  ## -----------------------------------------------------------\n  ## MAR Probability Curve (HRV Missingness vs Age)\n  ## Missingness increases with age → MAR\n  ## -----------------------------------------------------------\n  \n  df_mar &lt;- wear %&gt;%\n    mutate(miss_hrv = is.na(hrv)) %&gt;%\n    # group Age for smoothing\n    mutate(age_bin = cut(age, breaks = 30)) %&gt;%\n    group_by(age_bin) %&gt;%\n    summarise(\n      age_mid = mean(age, na.rm = TRUE),\n      p_miss = mean(miss_hrv)\n    ) %&gt;%\n    drop_na()\n  \n  # Plot MAR curve\n  ggplot(df_mar, aes(x = age_mid, y = p_miss)) +\n    geom_line(color = \"#fc8d62\", size = 1.2) +\n    labs(\n      title = \"Probability of HRV Missingness\",\n      subtitle = \"Increasing probability with age → MAR\",\n      x = \"Age\",\n      y = \"Pr(Missing)\"\n    ) +\n    theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nThe code above performs an exploratory diagnostic aimed at understanding whether the missingness of HRV depends on participant age. We construct an indicator variable marking whether each HRV value is missing and compare the age distribution between the missing and observed groups using a boxplot. The resulting figure shows a substantial and systematic difference: participants with missing HRV measurements are noticeably older, with both the median and the interquartile range shifted upward compared with those whose HRV is observed. This pattern is consistent with a Missing At Random (MAR) mechanism because the probability that HRV is missing appears to depend on another observed variable—age. Under MAR, missingness is not purely random but is explainable using information already present in the dataset.\nAs before, the boxplot provides an informative visualisation but should be treated as exploratory rather than definitive. A formal comparison could be carried out using statistical tests, such as a two-sample t-test, a Wilcoxon rank-sum test, or a logistic regression of the missingness indicator on age. These tests would quantify the difference more formally, although their interpretation must still be informed by domain knowledge about device behaviour and participant characteristics. The simulated dataset was explicitly designed so that HRV becomes more likely to be missing in older individuals, which is why the MAR pattern appears so cleanly here. In real-world wearable datasets, the structure is often more subtle, but this example illustrates clearly what a MAR signal looks like and why MAR mechanisms are far more common than true MCAR.\nThe next diagnostic examines missing values in resting heart rate. As before, we create a logical indicator marking whether the measurement is missing and then compare the distribution of frailty across missing and non-missing groups using a boxplot.\n\n    ## -----------------------------------------------------------\n    ## 4. MAR example: Resting HR missing at high frailty\n    ## -----------------------------------------------------------\n    \n    wear %&gt;%\n      mutate(miss_rhr = is.na(resting_hr)) %&gt;%\n      ggplot(aes(x = miss_rhr, y = frailty)) +\n      geom_boxplot(fill = \"#8da0cb\") +\n      labs(\n        title = \"Resting HR Missingness vs Frailty\",\n        subtitle = \"Missingness increases with frailty → MAR\",\n        x = \"Resting HR Missing?\",\n        y = \"Frailty\"\n      ) +\n      theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nThe resulting figure makes the pattern immediately apparent: participants with missing resting heart rate tend to have higher frailty scores. The median frailty in the missing group is clearly elevated, and the interquartile ranges do not overlap with those of the observed group. This signals that frailty influences the probability that resting HR is recorded. In other words, the missingness mechanism depends on an observed variable and is therefore consistent with Missing At Random (MAR).\nJust as in the previous examples, it is important to emphasise that the plot is not a formal test. Boxplots are descriptive tools: they reveal structure, highlight contrasts, and often alert us to important dependencies. But they cannot on their own confirm a statistical mechanism. A more rigorous assessment might involve modelling the missingness indicator as an outcome in a logistic regression:\n\n  glm(is.na(resting_hr) ~ frailty, data = wear, family = binomial)\n\n\nCall:  glm(formula = is.na(resting_hr) ~ frailty, family = binomial, \n    data = wear)\n\nCoefficients:\n(Intercept)      frailty  \n     -9.913       14.556  \n\nDegrees of Freedom: 499 Total (i.e. Null);  498 Residual\nNull Deviance:      227 \nResidual Deviance: 157.1    AIC: 161.1\n\n\nThe logistic regression output provides a simple numerical confirmation of what we saw visually in the boxplot. When we regress the missingness indicator for resting heart rate on frailty, the estimated slope is strongly positive (≈ 14.56) and highly significant relative to the null model. In practical terms, this means that as frailty increases, the log-odds of having a missing resting HR value increase sharply. The intercept is strongly negative (≈ –9.91), indicating that missingness is extremely unlikely among participants with very low frailty, but rises rapidly as frailty grows.\nThe reduction in deviance (from 227 to 157) shows that frailty explains a substantial proportion of the variation in missingness, which is reflected in the large drop in AIC as well. While this simple model does not prove the exact missing-data mechanism — no statistical test can definitively distinguish MAR from MNAR — it does provide quantitative evidence that missingness is systematically related to an observed variable. This pattern is characteristic of a Missing At Random (MAR) process, consistent with how the dataset was simulated.\n\n  ## -----------------------------------------------------------\n  ## 5. MAR example: Skin temperature missing at high stress\n  ## -----------------------------------------------------------\n  \n  wear %&gt;%\n    mutate(miss_temp = is.na(skin_temp)) %&gt;%\n    ggplot(aes(x = miss_temp, y = stress_score)) +\n    geom_boxplot(fill = \"#e78ac3\") +\n    labs(\n      title = \"Skin Temperature Missingness vs Stress Score\",\n      subtitle = \"Missingness increases at high stress → MAR\",\n      x = \"Skin Temperature Missing?\",\n      y = \"Stress Score\"\n    ) +\n    theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n6.3.1 Ilustrating MNAR mechanism\nTo illustrate a true MNAR mechanism in a controlled way, we introduce a synthetic variable called hydration_level. This variable is not used in the modelling of the stress score and serves purely didactic purposes. We then impose a missingness mechanism that depends directly on the unobserved values themselves: lower hydration levels are assigned a higher probability of being missing. Because missingness is a function of the value that is missing, this produces a textbook case of MNAR. No combination of the other variables (frailty, age, HRV, resting HR, etc.) can explain the pattern; the missingness is inherently tied to the variable being partially unobserved. This behaviour is rarely identifiable in real datasets and cannot be reliably corrected without modelling assumptions. The example is included to help students visualise an MNAR pattern and understand why principled statistical methods struggle in this setting.\n\n  ## -----------------------------------------------------------\n  ## NEW MNAR VARIABLE FOR DIDACTIC PURPOSES\n  ## hydration_level: fictitious physiological marker\n  ## MNAR mechanism: low hydration → higher probability of missingness\n  ## -----------------------------------------------------------\n  \n  set.seed(2027)\n  \n  # Create variable in complete dataset (no missing)\n  wear_complete$hydration_level &lt;- rnorm(\n    nrow(wear_complete),\n    mean = 50,\n    sd = 10\n  )\n  \n  # Copy to amputated dataset\n  wear$hydration_level &lt;- wear_complete$hydration_level\n  \n  # MNAR mechanism:\n  # lower hydration → higher Pr(missing)\n  p_miss &lt;- plogis(-(wear$hydration_level - 40) / 5)\n  \n  # Apply MNAR dropout\n  set.seed(2028)\n  wear$hydration_level[runif(nrow(wear)) &lt; p_miss] &lt;- NA\n  \n  # Construct helper dataset for plotting\n  df_hyd &lt;- wear %&gt;%\n    mutate(\n      miss_hydration = is.na(hydration_level),\n      p_miss = p_miss\n    )\n\n\n  df_hyd %&gt;%\n    ggplot(aes(x = miss_hydration, y = hydration_level)) +\n    geom_boxplot(fill = \"#8dd3c7\", na.rm = TRUE) +\n    labs(\n      title = \"Hydration Missingness vs Hydration Level\",\n      subtitle = \"Missingness increases at low hydration → MNAR\",\n      x = \"Hydration Missing?\",\n      y = \"Hydration Level\"\n    ) +\n    theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\n  df_hyd %&gt;%\n    ggplot(aes(x = hydration_level, y = p_miss)) +\n    geom_line(color = \"#1b9e77\", size = 1.2, na.rm = TRUE) +\n    labs(\n      title = \"Probability of Hydration Missingness\",\n      subtitle = \"Lower hydration levels have higher Pr(missing) → MNAR\",\n      x = \"Hydration Level\",\n      y = \"Pr(Missing)\"\n    ) +\n    theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nThe hydration variable was introduced solely for didactic purposes to demonstrate a Missing Not at Random (MNAR) mechanism, where the probability of missingness depends directly on the unobserved value itself. In this synthetic example, lower hydration levels have a higher probability of being missing.\nThe first figure compares hydration values between observed and missing groups: the missing group shows substantially lower values, a pattern incompatible with MCAR or MAR.\nThe second figure depicts the actual missingness probability encoded in the simulation. The curve shows a monotonic decline: as hydration decreases, Pr(missing) increases. This explicit dependency on the variable’s own value is the defining feature of MNAR and highlights why MNAR cannot be diagnosed from the observed data alone — here we know the mechanism only because we constructed it in simulation.\nIts usuall also to prsent the missing proportions for each variable.\n\n  ## -----------------------------------------------------------\n  ## 6. Missingness proportions (simple but extremely useful)\n  ## -----------------------------------------------------------\n  \n  missing_prop &lt;- round(colMeans(is.na(wear)), 3)\n  missing_prop*100\n\n                  age baseline_inflammation               frailty \n                  0.0                   0.0                   0.0 \n           resting_hr                   hrv             skin_temp \n                  6.0                   9.6                   6.2 \n       activity_index          stress_score            event_risk \n                  9.0                   0.0                   0.0 \n       event_occurred       hydration_level \n                  0.0                  20.2",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to missing data</span>"
    ]
  },
  {
    "objectID": "intro_missing.html#how-to-deal-with-missingness-rates",
    "href": "intro_missing.html#how-to-deal-with-missingness-rates",
    "title": "6  Introduction to missing data",
    "section": "6.4 How to deal with missingness rates?",
    "text": "6.4 How to deal with missingness rates?\nOnce missing data mechanisms are understood, the next practical question is how to handle the quantity of missingness in each variable. The proportion of missing values — the missingness rate — strongly influences whether imputation is advisable, whether a variable should be excluded from modelling, or whether specialised methods (such as multiple imputation or Bayesian models) are needed.\nA low missingness rate does not automatically imply that missingness is harmless; in the previous examples, even modest MAR patterns produced measurable bias. Still, the missingness rate often guides decisions in applied data analysis, and it is helpful to understand how different levels of missingness tend to affect model performance and inference.\nAt very low missingness levels, typically below five percent, imputation rarely introduces substantial artefacts. In such settings, imputing values using principled methods such as predictive mean matching generally stabilises the analysis and avoids the small but systematic distortions introduced by listwise deletion. Most modern statistical workflows treat this range as safe for imputation, particularly when missingness follows MCAR or MAR patterns.\nBetween five and twenty percent missingness, imputation becomes more clearly necessary. At this level, removing incomplete cases would disproportionately reduce the effective sample size and amplify bias under non-MCAR mechanisms. Multiple imputation is well suited to this regime, as it preserves sample size, propagates uncertainty, and stabilises regression estimates. The examples in this chapter illustrate how deleting only fifteen percent of rows can already shift regression coefficients in non-trivial ways.\nWhen missingness approaches thirty percent or more, decisions become more delicate. High missingness does not automatically disqualify a variable; some variables are scientifically indispensable and must be retained despite large gaps. However, high missingness increases uncertainty, inflates variance in imputed values, and requires a careful assessment of the imputation model. In this regime, imputation is still possible but demands more attention to diagnostics, model compatibility, and convergence of iterative imputation algorithms.\nBeyond fifty percent missingness, analysts face a decision that is as much scientific as it is statistical. A variable with more missing data than observed data carries limited direct information about its distribution, and imputation necessarily relies heavily on assumptions and auxiliary variables. If scientific justification is strong — for example, if a key biomarker is missing in half the participants because a device failed during the second half of a trial — the variable may still be retained and imputed. If the variable is weakly related to the outcome or has many weakly correlated predictors, excluding it entirely may lead to more stable models. Importantly, extremely high missingness often indicates structural problems in data collection rather than random loss of information.\nThere is no universal threshold that determines whether to impute or discard a variable. Instead, the choice depends on a combination of missingness rate, scientific relevance, and the quality of information available for predicting the missing values. Nonetheless, in many applied fields, rough conventions have emerged. Variables with less than twenty percent missingness are typically imputed without hesitation. Variables between twenty and forty percent missingness are imputed if they are scientifically meaningful and sufficiently predictable from the remaining data. Variables above fifty percent missingness require strong justification for inclusion, as imputations will carry substantial uncertainty and may be sensitive to the specification of the imputation model.\nAnother important point is that missing data themselves can be viewed as random variables, governed by their own probability distribution defined by the missingness mechanism. This aligns naturally with the Bayesian perspective introduced earlier: the missing values and the process that generated them both have uncertainty that must be accounted for. Methods such as multiple imputation, fully conditional specification, and Bayesian joint modelling all treat missing values not as fixed artefacts to be replaced but as unobserved random quantities. In this sense, higher missingness rates do not simply influence whether we impute but also how much uncertainty must be propagated forward after imputation.\nIn practice, deciding how to handle missingness is a balance between statistical validity and scientific context. Understanding both the mechanism and the magnitude of missingness helps determine when imputation is safe, when it is necessary, and when it may not be justified at all.\n## Multiple Imputation: Theory and Intuition Missing data creates uncertainty. Multiple imputation (MI) is a principled way of representing and propagating that uncertainty through the entire analysis pipeline. Instead of filling each missing value with a single “best guess,” MI replaces every missing entry with several plausible values, each drawn from a statistical model that reflects the relationships observed in the data.\nAt its core, MI treats missing values as random variables with probability distributions. This idea connects naturally with the Bayesian view introduced earlier: if the observed data arise from some joint distribution, then the missing components have a conditional distribution given the observed ones, and this distribution can be sampled.\nThe classical description of MI comes from Rubin’s framework, which rests on three conceptual steps:\n\nImputation step (create M completed datasets)\n\nWe replace each missing value \\(M\\) times using draws from an imputation model. For example, if HRV is correlated with age, frailty, and resting heart rate, those predictors are used to generate plausible HRV values.\nThis yields datasets:\n\\[\n  \\left\\{D^{(1)}, D^{(2)}, \\ldots, D^{(M)}\\right\\}\n  \\]\nEach dataset reflects different possible realisations of the unobserved values. 2. Analysis step (fit the model on each dataset)\nWe analyse each completed dataset separately using the intended modelling procedure-for example, by fitting a linear regression:\n\\[\n  \\hat{\\beta}^{(m)}=f\\left(D^{(m)}\\right), \\quad m=1, \\ldots, M\n  \\]\nEach run yields slightly different estimates because the imputations differ. 3. Pooling step (combine estimates across datasets)\nRubin’s rules combine the estimates and their variances across the M analyses: - The average coefficient is the MI estimate. - The total variance incorporates both within-imputation variance and between-imputation variance, ensuring uncertainty from missingness is fully represented.\nThis yields final results that: - preserve correct uncertainty, - retain the full sample size, - avoid artificial distortion of correlations, - are valid under MAR and MCAR mechanisms.\nMultiple imputation is not “filling the dataset.” It is a full inferential framework that recognises that missing values are uncertain and that this uncertainty must flow into the final estimates.\nWhy Multiple Imputation Works The key insight is that MI produces valid inferences because it reconstructs the joint distribution of the data as if no observations were missing. Each imputed dataset is one plausible completion of the data. No single imputed value is trusted, but across many datasets, the variation among them reflects the uncertainty induced by missingness.\nCompared with listwise deletion: - MI reduces bias under MAR, - improves power by using the full sample - stabilises regression estimates, - and preserves covariance structure.\nCompared with single-value imputation (mean/median): - Ml restores variance, - avoids collapsing distributions, - produces correlations and regression slopes consistent with the full data, - and appropriately widens confidence intervals.\nAcross modern statistics, MI is now considered the standard approach for handling missing data under MCAR/MAR.\n## Imputing data with MICE R package\nTo illustrate how multiple imputation operates in real analyses, we now apply the method to the amputated wearable dataset using the mice package. The goal is to fit the regression model for stress_score while recovering the information lost through missingness and avoiding the biases demonstrated earlier. Each block of code below is followed by a narrative explanation of what it does and how to interpret its output.\nThe first step is to load the required packages: When mice is attached, R prints a message indicating that some base functions (cbind, rbind, filter) are masked. This is normal and simply reflects that mice defines its own versions for internal use. The tidyverse package is also loaded for convenience in data manipulation and plotting.\nWe then initiate the imputation with:\n\n  library(mice)\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n  library(tidyverse)\n  \n  # We will impute the amputated dataset\n  imp &lt;- mice(\n    data = wear,\n    m = 5,                     # number of imputed datasets\n    method = \"pmm\",            # predictive mean matching\n    maxit = 10,                # number of iterations\n    seed = 2025\n  )\n\n\n iter imp variable\n  1   1  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  1   2  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  1   3  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  1   4  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  1   5  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  2   1  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  2   2  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  2   3  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  2   4  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  2   5  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  3   1  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  3   2  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  3   3  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  3   4  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  3   5  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  4   1  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  4   2  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  4   3  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  4   4  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  4   5  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  5   1  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  5   2  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  5   3  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  5   4  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  5   5  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  6   1  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  6   2  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  6   3  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  6   4  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  6   5  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  7   1  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  7   2  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  7   3  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  7   4  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  7   5  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  8   1  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  8   2  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  8   3  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  8   4  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  8   5  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  9   1  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  9   2  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  9   3  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  9   4  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  9   5  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  10   1  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  10   2  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  10   3  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  10   4  resting_hr  hrv  skin_temp  activity_index  hydration_level\n  10   5  resting_hr  hrv  skin_temp  activity_index  hydration_level\n\n\nOnce executed, R prints the familiar imputations log. Each row corresponds to an iteration of the chained equations algorithm, and each column indicates which variable is being imputed. The output lists the five parallel imputed datasets (imp = 1 through imp = 5) being updated at each iteration. Seeing the same sequence of variables repeated in each iteration confirms that the chained equations are cycling through all incomplete variables: resting_hr, hrv, skin_temp, activity_index, and hydration_level. Convergence is usually judged informally by the stability of the traces, which we inspect next.\nA first visual diagnostic is produced by:\n\n  plot(imp)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe resulting plot displays the iteration history for each variable’s imputed values. For predictive mean matching, we expect the traces to oscillate around stable ranges rather than drift systematically. In this dataset, the lines settle quickly, suggesting that the algorithm has reached an acceptable equilibrium and that ten iterations are sufficient.\nA more granular view is obtained from the stripplot:\n\n  stripplot(imp, hrv ~ .imp, pch = 20, cex = 1.1)\n\n\n\n\n\n\n\n\nThis plot displays the distribution of hrv across the five imputed datasets (.imp = 1…5). The observed values appear as black points, while imputed values appear in coloured panels associated with each completed dataset. Ideally, the imputed values blend naturally into the observed distribution rather than forming implausible clusters. In our case, the imputed hrv values follow the same general range and variability as the observed measurements, indicating that the imputation model is compatible with the data.\nWe gain further insight from the density plot:\n\n  densityplot(imp, ~ hrv)\n\n\n\n\n\n\n\n\nThis shows the kernel density curves for both observed and imputed hrv. Predictive mean matching ensures that imputed values are sampled from the neighbourhood of observed ones, and consequently the curves should largely overlap. The diagnostic confirms this: the imputed densities track the observed ones closely, avoiding the artificial spikes or flattened tails that plagued single-value imputation earlier. This is precisely the behaviour that preserves the statistical structure of the data.\nWith imputation diagnostics completed, we proceed to fit the regression model across all imputed datasets:\n\n  fit_imp &lt;- with(\n    imp,\n    lm(stress_score ~ frailty + hrv + resting_hr)\n  )\n\nThe command fits the same linear model to each of the five completed datasets. The result is not yet a single pooled model but rather a list of regression fits, one per imputation. To obtain inference that accounts for both within-imputation and between-imputation variability, we pool the estimates using Rubin’s rules:\n\n  pooled &lt;- pool(fit_imp)\n  summary(pooled)\n\n         term   estimate  std.error  statistic       df      p.value\n1 (Intercept) 50.3163707 3.93329986  12.792406 480.0654 1.896494e-32\n2     frailty 11.3010872 2.92625091   3.861968 467.4432 1.283310e-04\n3         hrv -0.5044780 0.02675060 -18.858565 421.5302 5.748153e-58\n4  resting_hr  0.6914724 0.04293115  16.106542 490.5909 3.676140e-47\n\n\nThe summary() output displays the combined regression coefficients. For the intercept and three predictors, the pooled estimates are:\nIntercept ≈ 50.32\nFrailty ≈ 11.30\nHRV ≈ –0.504\nResting HR ≈ 0.691\nThese values are strikingly close to the true coefficients from the complete dataset analysis. In particular, the frailty slope — which listwise deletion biased downwards to about 9.40 — returns to approximately 11.30, almost exactly the complete-data truth (11.06). Standard errors, test statistics, degrees of freedom, and p-values are also provided. Crucially, the degrees of freedom differ from a standard linear model, reflecting the added uncertainty due to imputing missing values across multiple datasets.\nTo summarise and compare the three strategies — complete data, listwise deletion, and multiple imputation — we collect them into a single tibble:\n\n  library(broom)\n  \n  coef_full &lt;- tidy(fit_full)\n  coef_drop &lt;- tidy(fit_drop)\n  coef_mi   &lt;- summary(pooled)\n  \n  tibble(\n    Model = c(\"Complete data\", \"Listwise deletion\", \"Multiple imputation\"),\n    Frailty = c(coef_full$estimate[2], coef_drop$estimate[2], coef_mi$estimate[2]),\n    HRV     = c(coef_full$estimate[3], coef_drop$estimate[3], coef_mi$estimate[3]),\n    RestingHR = c(coef_full$estimate[4], coef_drop$estimate[4], coef_mi$estimate[4])\n  )\n\n# A tibble: 3 × 4\n  Model               Frailty    HRV RestingHR\n  &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n1 Complete data         11.1  -0.515     0.680\n2 Listwise deletion      9.40 -0.509     0.708\n3 Multiple imputation   11.3  -0.504     0.691\n\n\nThe resulting table makes the contrast explicit. Listwise deletion clearly underestimates the effect of frailty and slightly distorts the effects of HRV and resting heart rate. Multiple imputation, by contrast, recovers estimates almost indistinguishable from those obtained from the complete dataset. This is the core advantage of the method: it restores the statistical information that would otherwise be lost or distorted by naive approaches, while appropriately propagating uncertainty due to missingness.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to missing data</span>"
    ]
  },
  {
    "objectID": "intro_missing.html#other-imputation-methods-available-in-mice",
    "href": "intro_missing.html#other-imputation-methods-available-in-mice",
    "title": "6  Introduction to missing data",
    "section": "6.5 Other Imputation Methods Available in mice",
    "text": "6.5 Other Imputation Methods Available in mice\nPredictive mean matching is a robust default and works well for continuous physiological variables such as HRV, resting heart rate, or skin temperature. However, mice includes a broad family of imputation engines that can be matched to different data types, modelling goals, or complexity levels. Understanding these alternatives provides flexibility when designing imputation workflows, especially in larger clinical or biological datasets where variables have mixed scales, nonlinear behaviour, or structural constraints.\nAt its core, mice uses Fully Conditional Specification (FCS): each variable with missing values is imputed from a regression model conditional on the other variables. The “method” argument in mice() controls the regression model used for each variable. The choice of method should reflect both the scale of the variable and the assumptions underlying the analysis.\nLinear regression for continuous data (“norm”)\nThe simplest imputation model is ordinary least squares regression. The “norm” method draws imputed values from a normal distribution centered at the fitted mean with added residual noise. Although straightforward, it assumes linearity, homoscedasticity, and normally distributed residuals—conditions that may not hold for skewed biomarkers or device-derived digital features.\nThis approach is most appropriate when variables follow approximately Gaussian distributions and the primary aim is inferential modelling rather than distributional reconstruction.\nBayesian linear regression (“norm.nob” and “norm.post”)\nTwo Bayesian variants extend the linear model:\n“norm.nob” (no Bayesian step) imputes deterministically from the regression fit.\n“norm.post” adds uncertainty by drawing regression coefficients from their posterior distribution, producing more realistic imputations.\nThese are useful when the analyst wants to propagate model uncertainty formally, especially in low-sample settings or when residual variance is substantial.\nLogistic and multinomial regression (“logreg” and “polyreg”)\nBinary variables (e.g., event occurrence, device dropout logged as yes/no) can be imputed via logistic regression using “logreg”. Categorical variables with more than two levels use “polyreg”, which fits a multinomial logit model. These methods are natural choices for survey indicators, categorical clinical classifications, and device metadata.\nPredictive mean matching should not be used for categorical variables, so logistic/multinomial models become essential for mixed datasets.\nProportional odds models for ordered categories (“polr”)\nOrdinal scales—frailty categories, symptom severity scores, Likert responses, or manually adjudicated clinical ratings—benefit from an ordered logit/probit approach. “polr” preserves the ordinal structure, meaning that imputations respect category order rather than treating the variable as simply numeric or unordered.\nRandom forest imputation (“rf”)\nThe “rf” method uses Breiman’s random forests to learn complex nonlinear relationships and interactions. This model is particularly useful when:\nvariables have nonlinear associations,\npredictors include many weak signals (e.g., gene expression or sensor-derived features),\nthere are complicated dependencies that linear models miss.\nRandom forest imputation is flexible and robust but computationally more expensive. It also produces less transparent models, which is important to consider when interpretability matters.\nTwo-stage imputation for bounded continuous data (“norm.predict”, “cart”, “2l.norm”)\nDigital biomarkers and physiological outputs are often bounded (e.g., percentages, temperatures, accelerometry intensities). Standard linear regression can predict values outside the plausible range. Alternatives include:\n“cart”: classification and regression trees, which naturally enforce realistic bounds through partitioning rules.\n“norm.predict”: deterministic prediction without added noise, useful for sensitivity analyses.\n“2l.norm”: linear mixed-effects models suitable for clustered or longitudinal data.\nThese are attractive in decentralised clinical trial datasets where repeated measurements per participant introduce hierarchical structure.\nPassive imputation and derived variables (“~” and “-”)\nWhen derived quantities depend deterministically on other variables—for example, BMI from height and weight—it is advisable to avoid imputing them separately. Instead, mice allows passive imputation, in which a formula is specified and the variable is recalculated at each iteration rather than modelled:\nmethod[\"BMI\"] &lt;- \"~ I(weight / (height/100)^2)\"\nThis ensures consistency across imputations and prevents biologically impossible combinations.\n\n6.5.1 Choosing an Imputation Method: Practical Guidance\nIn most applied workflows, predictive mean matching (“pmm”) remains an excellent general-purpose choice for continuous variables. It preserves the distribution and resists outliers or model misspecification. Logistic/multinomial models are the default for categorical variables, and “polr” is ideal for ordinal ones.\nRandom forest imputation is useful when modelling highly nonlinear relationships but should be applied thoughtfully, as it may interfere with downstream model interpretability or introduce difficulties when pooling estimates.\nFinally, when working with longitudinal or multilevel data, two-level imputation models provide a principled way to incorporate within-participant correlation, which is especially relevant in wearable-sensor studies.\nSelecting an imputation strategy is not merely a technical choice; it reflects assumptions about the data-generating process and the degree of structure one expects in the relationships between variables. The mice framework provides enough flexibility to tailor imputation to variable type, scientific context, and analytical goals while maintaining the core principle of properly propagating uncertainty.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Introduction to missing data</span>"
    ]
  },
  {
    "objectID": "high_dims.html",
    "href": "high_dims.html",
    "title": "7  High Dimension Data Strategies",
    "section": "",
    "text": "7.1 Motivational example\nTo anchor the ideas in this chapter, we begin with a example dataset inspired by advanced therapeutic technologies in dermatology. The clinical setting is moderate-to-severe psoriasis, a chronic inflammatory skin disease characterised by keratinocyte hyperproliferation, immune dysregulation involving the IL-17/IL-23 axis, and substantial patient-to-patient heterogeneity. Psoriasis is particularly well suited for illustrating high-dimensional data challenges because its biology spans multiple layers: immune signalling in the skin, microbial community shifts, and environmental triggers that modulate disease activity.\nImagine a cohort of patients who initiate treatment with an anti–IL-17 biologic. Before therapy begins, an extensive set of measurements is collected. These measurements include gene expression profiles from lesional skin—about fifteen hundred genes capturing inflammatory pathways, barrier function, and cellular differentiation. Each gene defines one axis in the data space, and the genomics block alone positions each patient as a point in a fifteen-hundred-dimensional cloud.\nAlongside the genomic layer, the dataset incorporates three hundred features derived from the skin microbiome. These represent aggregated amplicon sequence variants describing the relative abundance of microbial taxa. Psoriasis frequently shows alterations in the cutaneous microbiota, with some patients displaying a more dysbiotic pattern and others retaining a commensal-dominant profile. These microbial dimensions interact with, but do not mirror, the genomic axes, increasing the complexity of the data geometry.\nTo capture external influences, the dataset includes environmental exposure variables such as NO₂, PM₁₀, ultraviolet index, local temperature, and humidity. Psoriasis severity and treatment response often fluctuate with environmental and seasonal factors, and this exposome block provides a complementary source of structured variability.\nClinical and biochemical markers provide additional anchors to the dataset: baseline PASI scores, dermatology-specific quality-of-life indices, and a panel of cytokines reflecting local immune activation. These lower-dimensional features help ground the high-dimensional layers in clinically interpretable signals.\nThe outcome of interest is the change in PASI after twelve weeks of therapy. This continuous measurement captures individual response to treatment and allows us to relate the high-dimensional predictors to a meaningful clinical endpoint.\nThis dataset is intentionally heterogeneous and high dimensional, placing each patient in a space with nearly two thousand axes. As we move to the next section, this geometric perspective motivates the need to reduce dimensionality: high-dimensional spaces behave counterintuitively, distances become unstable, and noisy or redundant variables obscure meaningful structure. The challenges of this dataset naturally set the stage for understanding the curse of dimensionality and why dimensionality reduction becomes essential.\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n df&lt;- readRDS(\"~/att_ai_ml/data/Dermato_ATT_simulated.rds\")\n str(df)\n\n'data.frame':   200 obs. of  1852 variables:\n $ patient_id    : chr  \"P1\" \"P2\" \"P3\" \"P4\" ...\n $ Outcome_PASI12: num  19.06 1.93 2.54 13.49 3.31 ...\n $ PASI_baseline : num  9.35 13.65 13.96 9.95 13.15 ...\n $ DLQI          : num  12.67 9.22 11.74 11.92 7.66 ...\n $ cytokine_1    : num  -2.613 0.222 3.56 -3.648 0.474 ...\n $ cytokine_2    : num  -2.06625 -0.21861 0.7805 -4.52195 -0.00698 ...\n $ cytokine_3    : num  -1.8888 0.411 -0.0255 -4.3303 0.577 ...\n $ cytokine_4    : num  -0.76 -0.981 2.755 -4.133 1.466 ...\n $ cytokine_5    : num  -3.402 -0.271 2.994 -5.152 1.477 ...\n $ cytokine_6    : num  -2.2422 1.4799 3.2168 -4.2629 0.0581 ...\n $ cytokine_7    : num  -3.288 -0.406 1.064 -5.879 0.327 ...\n $ cytokine_8    : num  -3.4514 -0.0117 0.4019 -5.0254 1.1754 ...\n $ cytokine_9    : num  -3.7626 0.6743 3.7283 -4.362 0.0832 ...\n $ cytokine_10   : num  -4.49 1.13 2.95 -5.21 1.48 ...\n $ cytokine_11   : num  -1.252 0.136 2.162 -5.204 0.858 ...\n $ cytokine_12   : num  -4.231 -0.257 0.443 -3.814 1.864 ...\n $ cytokine_13   : num  -3.105 1.83 1.159 -5.5 0.268 ...\n $ cytokine_14   : num  -1.96 -0.397 1.813 -3.58 1.262 ...\n $ cytokine_15   : num  -3.68 1.45 1.82 -2.84 1.41 ...\n $ cytokine_16   : num  -3.118 0.824 1.923 -5.446 0.753 ...\n $ cytokine_17   : num  -3.43 0.855 2.541 -3.279 1.203 ...\n $ cytokine_18   : num  -2.33 2.14 3.27 -4.5 2.86 ...\n $ NO2           : num  22.3 34.5 14.2 25.9 31.1 ...\n $ PM10          : num  12.67 13.81 15.17 20.88 9.57 ...\n $ UV_index      : num  4.52 4.37 3.64 3.69 5.63 ...\n $ temperature   : num  9.16 19.23 10.56 18.58 19.86 ...\n $ humidity      : num  69.6 73.3 71.4 54.3 63.6 ...\n $ expo_6        : num  2.49638 0.10679 -0.45004 0.00785 1.68004 ...\n $ expo_7        : num  0.813 -0.246 -0.349 0.906 -0.607 ...\n $ expo_8        : num  0.835 -1.528 -1.241 0.842 -1.4 ...\n $ expo_9        : num  -0.85287 -0.99771 -0.00603 -0.81976 -0.10098 ...\n $ expo_10       : num  2.528 0.203 -1.108 -1.319 2.991 ...\n $ expo_11       : num  -0.737 0.221 1.033 -0.13 -0.458 ...\n $ expo_12       : num  0.0812 0.1011 -0.4487 -1.9899 -0.521 ...\n $ expo_13       : num  -2.296 -0.276 0.482 0.876 -1.178 ...\n $ expo_14       : num  0.253 -1.479 1.033 0.626 1.153 ...\n $ expo_15       : num  1.114 -0.328 1.022 0.942 0.852 ...\n $ expo_16       : num  -1.356 0.689 -1.275 1.394 1.236 ...\n $ expo_17       : num  0.6997 -0.4043 -0.5038 -0.0914 -0.6555 ...\n $ expo_18       : num  0.033 0.349 0.122 0.211 0.72 ...\n $ expo_19       : num  1.202 -2.756 1.041 -0.625 1.678 ...\n $ expo_20       : num  0.9554 -0.8974 1.352 0.0425 0.6997 ...\n $ expo_21       : num  -1.885 0.458 -1.782 -0.546 2.11 ...\n $ expo_22       : num  -1.567 -1.076 -0.805 1.129 1.501 ...\n $ expo_23       : num  0.17 -0.795 -0.794 0.371 0.354 ...\n $ expo_24       : num  0.18 -1.847 1.091 -0.926 -2.242 ...\n $ expo_25       : num  1.3289 -0.0789 -1.9444 0.5659 1.8 ...\n $ expo_26       : num  0.6275 1.2035 -1.8594 -0.3295 -0.0342 ...\n $ expo_27       : num  2.188 1.451 -0.158 1.091 0.433 ...\n $ expo_28       : num  0.00325 0.08153 0.84316 -0.30213 -0.54475 ...\n $ expo_29       : num  0.221 -0.427 0.344 -0.138 0.655 ...\n $ expo_30       : num  2.447 -0.511 -0.776 -0.364 1.47 ...\n $ gene_1        : num  -0.0341 0.7265 0.8737 0.1497 1.2999 ...\n $ gene_2        : num  -1.071 0.043 0.355 -1.318 -0.136 ...\n $ gene_3        : num  -0.917 1.98 0.223 -1.293 0.719 ...\n $ gene_4        : num  -0.786 0.202 0.59 0.205 0.782 ...\n $ gene_5        : num  -1.009 0.231 1.44 -2.541 0.41 ...\n $ gene_6        : num  -0.0519 0.5597 1.9868 -1.2325 0.1317 ...\n $ gene_7        : num  -0.665 1.257 1.516 -2.056 1.147 ...\n $ gene_8        : num  0.128 0.634 0.259 0.241 0.656 ...\n $ gene_9        : num  -0.894 0.681 0.278 -1.199 0.133 ...\n $ gene_10       : num  -0.0476 0.6626 0.5964 -2.159 0.8182 ...\n $ gene_11       : num  -0.3866 0.9519 -0.0379 -0.5661 0.865 ...\n $ gene_12       : num  -0.865 0.471 0.534 -0.229 0.277 ...\n $ gene_13       : num  -0.6372 -0.0453 1.2612 -1.0254 0.4567 ...\n $ gene_14       : num  0.1027 0.5721 -0.0718 -0.0231 0.2311 ...\n $ gene_15       : num  -0.0256 1.1792 1.6511 -0.7759 0.1092 ...\n $ gene_16       : num  -0.116 1.194 0.995 -0.597 0.338 ...\n $ gene_17       : num  -1.264 0.512 0.732 -2.302 0.066 ...\n $ gene_18       : num  -1.053 0.29 0.941 -1.262 0.693 ...\n $ gene_19       : num  -0.715 0.667 0.641 -1.18 1.13 ...\n $ gene_20       : num  -0.7627 0.8478 1.4797 -1.4841 -0.0193 ...\n $ gene_21       : num  -0.107 0.451 0.922 -0.495 1.177 ...\n $ gene_22       : num  2.06e-01 9.17e-01 1.19 -1.87 -2.23e-05 ...\n $ gene_23       : num  -0.186 -0.133 0.952 -1.064 0.518 ...\n $ gene_24       : num  -0.666 0.552 0.474 -0.989 0.692 ...\n $ gene_25       : num  -0.381 0.912 1.676 -1.537 0.697 ...\n $ gene_26       : num  -0.809 -0.371 1.211 -1.426 0.191 ...\n $ gene_27       : num  -0.766 0.722 1.698 -1.528 0.298 ...\n $ gene_28       : num  -0.471 2.368 0.258 -1.767 1.252 ...\n $ gene_29       : num  0.144 0.746 0.424 -0.963 -0.231 ...\n $ gene_30       : num  -0.0554 0.7709 1.1042 -1.023 0.9466 ...\n $ gene_31       : num  -0.000831 1.001845 0.503228 -0.629817 0.835488 ...\n $ gene_32       : num  0.564 0.462 0.309 -0.199 0.362 ...\n $ gene_33       : num  -0.325 -0.407 0.135 -1.704 -0.095 ...\n $ gene_34       : num  -1.356 0.78 0.94 -0.907 1.105 ...\n $ gene_35       : num  -0.571 0.283 1.748 -0.917 -0.276 ...\n $ gene_36       : num  -1.185 0.704 2.394 -2.08 1.937 ...\n $ gene_37       : num  -0.66 1.08 1.01 -2.39 1.01 ...\n $ gene_38       : num  -0.1021 -0.0143 0.6307 -1.8735 0.2224 ...\n $ gene_39       : num  -1.129 1.435 0.609 -1.087 0.567 ...\n $ gene_40       : num  -0.971 0.543 1.691 -1.642 0.549 ...\n $ gene_41       : num  -0.474 -0.174 1.028 -1.187 0.502 ...\n $ gene_42       : num  -0.946 0.707 0.853 -1.725 -0.135 ...\n $ gene_43       : num  -0.55 0.282 0.41 -0.991 0.853 ...\n $ gene_44       : num  -0.0674 0.2369 0.5463 -0.831 0.6119 ...\n $ gene_45       : num  -0.234 0.114 1.184 -0.39 0.13 ...\n $ gene_46       : num  -0.659 0.289 1.178 -1.236 0.267 ...\n $ gene_47       : num  0.642 1.545 1.061 -1.265 0.455 ...\n  [list output truncated]",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "high_dims.html#what-is-a-dimension",
    "href": "high_dims.html#what-is-a-dimension",
    "title": "7  High Dimension Data Strategies",
    "section": "7.2 What is a dimension?",
    "text": "7.2 What is a dimension?\nIn short a dimension is each of the features present in a dataset. In other words the dimension of a dataset is identical to the number of variables (or columns) it contains.\nWhen working with multivariate datasets, it is often helpful to visualize how observations occupy space when only a handful of dimensions are considered. Although we cannot directly visualize the genomic, microbial, and exposome components of the psoriasis dataset—which together span nearly two thousand axes—we can project selected pairs or triplets of variables into two- or three-dimensional plots. These visualizations help reinforce the geometric interpretation of dimensions introduced above.\nIn low dimensions, distance, neighbourhood structure, and separation between patients behave intuitively. As more variables are added, this intuition quickly breaks down. Before we explore why this happens, the examples below illustrate how a dataset “looks” when restricted to two and three axes. These small visual slices help anchor the abstract notion of dimensions in concrete graphics.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nggplot(df, aes(x = PASI_baseline,\n               y = NO2,\n               colour = Outcome_PASI12)) +\n  geom_point(alpha = 0.8, size = 2) +\n  scale_colour_viridis_c(option = \"C\") +\n  labs(title = \"Patients represented in two dimensions\",\n       x = \"Baseline PASI\",\n       y = \"NO₂ exposure\",\n       colour = \"ΔPASI 12 weeks\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nThis 2D view shows how each variable defines one axis in the space.\n\nIn reality, the dataset contains thousands of such axes simultaneously—something impossible to visualize directly, but essential for understanding the geometry of high-dimensional data.\nWith three variables, each patient can be seen as a point in a three-dimensional cloud.\n\nThe example below uses PASI_baseline, NO₂, and a single gene (gene_1).\n\nIn an HTML Quarto book, this produces an interactive 3D plot that the student can rotate.\n\nlibrary(plotly)\n\nplot_ly(\n  df,\n  x = ~PASI_baseline,\n  y = ~NO2,\n  z = ~gene_1,\n  color = ~Outcome_PASI12,\n  colorscale = \"Viridis\",\n  type = \"scatter3d\",\n  mode = \"markers\",\n  marker = list(size = 3, opacity = 0.7)\n) |&gt;\n  layout(\n    title = \"Patients represented in three dimensions\",\n    scene = list(\n      xaxis = list(title = \"Baseline PASI\"),\n      yaxis = list(title = \"NO₂\"),\n      zaxis = list(title = \"gene_1\")\n    )\n  )\n\nWarning: 'scatter3d' objects don't have these attributes: 'colorscale'\nValid attributes include:\n'connectgaps', 'customdata', 'customdatasrc', 'error_x', 'error_y', 'error_z', 'hoverinfo', 'hoverinfosrc', 'hoverlabel', 'hovertemplate', 'hovertemplatesrc', 'hovertext', 'hovertextsrc', 'ids', 'idssrc', 'legendgroup', 'legendgrouptitle', 'legendrank', 'line', 'marker', 'meta', 'metasrc', 'mode', 'name', 'opacity', 'projection', 'scene', 'showlegend', 'stream', 'surfaceaxis', 'surfacecolor', 'text', 'textfont', 'textposition', 'textpositionsrc', 'textsrc', 'texttemplate', 'texttemplatesrc', 'transforms', 'type', 'uid', 'uirevision', 'visible', 'x', 'xcalendar', 'xhoverformat', 'xsrc', 'y', 'ycalendar', 'yhoverformat', 'ysrc', 'z', 'zcalendar', 'zhoverformat', 'zsrc', 'key', 'set', 'frame', 'transforms', '_isNestedKey', '_isSimpleKey', '_isGraticule', '_bbox'\n\n\n\n\n\n\nThe two introductory plots provide a first glimpse of how patients in our simulated psoriasis cohort distribute themselves when represented in only a handful of variables. These figures are intentionally simple: they show what happens when we restrict attention to two or three axes of variation and attempt to visualize clinical and environmental information directly.\nIn the two-dimensional plot, each patient is positioned according to baseline PASI and NO₂ exposure, with the treatment response (ΔPASI after twelve weeks) encoded through colour. Although the scatter suggests a broad spread along both axes, no obvious groupings emerge. Patients with similar clinical outcomes are dispersed throughout the space, and improvement or worsening does not align cleanly with either baseline severity or pollution levels. This lack of visible structure is not surprising. Disease burden, environmental exposures, and treatment response arise from highly multivariate processes, and a projection onto only two variables often hides most of the relevant variation.\nThe three-dimensional representation adds gene expression into the picture by using one inflammatory gene as an additional axis. Even with this extra dimension, the overall shape remains a diffuse cloud rather than a set of compact, separable groups. The colouring by ΔPASI again shows subtle gradients but no clear partitioning. A single molecular feature, even when combined with clinical and environmental measures, carries only a small fraction of the information available in the full dataset. When thousands of genomic and microbiome variables coexist with dozens of environmental and clinical covariates, any low-dimensional projection will necessarily obscure important structure.\nThese two figures serve as a useful reminder of the limits of direct visualization. Human intuition is well suited to perceiving structure in two or three dimensions, but modern datasets routinely occupy spaces with hundreds or thousands of variables. Within such spaces, distances behave differently, densities fall, collinearity increases, and meaningful patterns often reside in directions that are not aligned with any single observed variable. The diffuse appearance of both plots illustrates these issues clearly: even though the dataset contains strong simulated relationships, they are not apparent when viewed through only two or three coordinates.\nFor this reason, dimension-reduction methods become essential. Techniques such as principal component analysis, t-SNE, UMAP, self-organizing maps, and locally linear embedding allow us to project high-dimensional data into spaces where structure becomes visible. Before developing these methods in detail, it is important to understand what is meant by a “dimension” in a statistical sense and why high-dimensional settings pose unique challenges. Before moving to the methods of dimension reduction lets discuss a lit bit more on visualization of multivariate datasets.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "high_dims.html#measuring-distances-in-high-dimensional-space",
    "href": "high_dims.html#measuring-distances-in-high-dimensional-space",
    "title": "7  High Dimension Data Strategies",
    "section": "7.3 Measuring Distances in High-Dimensional Space",
    "text": "7.3 Measuring Distances in High-Dimensional Space\nMeasuring Distances in High-Dimensional Space Understanding how distances are computed is the geometric foundation of every technique in this chapter. Whether the goal is PCA, clustering, t-SNE, UMAP, SOM or LLE, everything begins with a single, essential question:\nHow far apart are two observations? In all introductory examples, we begin with the Euclidean distance, the straight-line distance familiar from basic geometry.\nIn two or three dimensions, Euclidean distance matches our intuition well: points that look close really are close.\nBut once the number of variables reaches dozens, hundreds, or thousands-as in multi-omics dermatology datasets-the behaviour of Euclidean distance changes dramatically.\nThis section builds intuition step-by-step:\n\nEuclidean distance in 2D\nEuclidean distance in 3D\nEuclidean distance in \\(p\\) dimensions\nWhy Euclidean distance breaks down in high dimensions\nWhy alternative distance metrics become necessary\n\n\n7.3.1 Euclidean Distance Between Two Points in Two Dimensions\nSuppose two patients are represented by two quantitative variables-for example, PASI_baseline and \\(\\mathbf{N O}_{\\mathbf{2}}\\) exposure.\nEach patient is a point in a 2-dimensional space. The straight-line (Euclidean) distance is:\n\\[\nd(A, B)=\\sqrt{\\left(x_B-x_A\\right)^2+\\left(y_B-y_A\\right)^2} .\n\\]\nThe plot below illustrates this geometry:\n\nlibrary(ggplot2)\n\n# Two points A and B in 2D\nA &lt;- c(1, 1)\nB &lt;- c(4, 3)\n\ndf_points &lt;- data.frame(\n  x = c(A[1], B[1]),\n  y = c(A[2], B[2]),\n  label = c(\"A\", \"B\")\n)\n\nggplot(df_points, aes(x, y)) +\n  geom_point(size = 4, colour = \"purple\") +\n  geom_text(aes(label = label), vjust = -1.1, size = 5) +\n  \n  # dashed horizontal/vertical components\n  geom_segment(aes(\n    x = A[1], y = A[2],\n    xend = B[1], yend = A[2]),\n    linetype = \"dashed\"\n  ) +\n  geom_segment(aes(\n    x = B[1], y = A[2],\n    xend = B[1], yend = B[2]),\n    linetype = \"dashed\"\n  ) +\n  \n  # straight-line Euclidean distance\n  geom_segment(aes(\n    x = A[1], y = A[2],\n    xend = B[1], yend = B[2]),\n    linewidth = 1.2, colour = \"blue\",\n    arrow = arrow(length = unit(0.25, \"cm\"))\n  ) +\n  \n  annotate(\"text\", x = 2.4, y = 2.1,\n           label = \"Euclidean distance\",\n           colour = \"blue\", size = 5) +\n  \n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Euclidean distance between two points in 2D\",\n    x = \"Var 1\", y = \"Var 2\"\n  )\n\nWarning in geom_segment(aes(x = A[1], y = A[2], xend = B[1], yend = A[2]), : All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = B[1], y = A[2], xend = B[1], yend = B[2]), : All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = A[1], y = A[2], xend = B[1], yend = B[2]), : All aesthetics have length 1, but the data has 2 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nThe dashed lines represent the horizontal and vertical components:\n\\[\n\\Delta x=x_B-x_A, \\quad \\Delta y=y_B-y_A,\n\\]\nand the blue arrow represents the Euclidean distance.\nTo compute the number in R:\n\nsqrt((B[1] - A[1])^2 + (B[2] - A[2])^2)\n\n[1] 3.605551\n\n\n\n\n7.3.2 Euclidean Distance in Three Dimensions\nNow suppose patients are described by three measurements-say PASI_baseline, \\(\\mathbf{N O}_{\\mathbf{2}}\\), and gene_1. Each patient becomes a point in 3D space, and Euclidean distance generalises to:\n\\[\nd(A, B)=\\sqrt{\\left(x_B-x_A\\right)^2+\\left(y_B-y_A\\right)^2+\\left(z_B-z_A\\right)^2} .\n\\]\nA geometric illustration:\n\nlibrary(plotly)\n\n# Define three 3D points\nA &lt;- c(1, 1, 1)\nB &lt;- c(4, 3, 2)\nC &lt;- c(2, 4, 3)\n\npoints &lt;- data.frame(\n  x = c(A[1], B[1], C[1]),\n  y = c(A[2], B[2], C[2]),\n  z = c(A[3], B[3], C[3]),\n  label = c(\"A\", \"B\", \"C\")\n)\n\n# Function to draw a segment between two points\nsegment3d &lt;- function(P, Q, color=\"blue\") {\n  list(\n    x = c(P[1], Q[1]),\n    y = c(P[2], Q[2]),\n    z = c(P[3], Q[3]),\n    type = \"scatter3d\",\n    mode = \"lines\",\n    line = list(color=color, width=6),\n    showlegend = FALSE\n  )\n}\n\nplot_ly() %&gt;%\n  add_markers(\n    data = points,\n    x = ~x, y = ~y, z = ~z,\n    text = ~label, textposition = \"top center\",\n    marker = list(size = 8,\n                  color = c(\"purple\", \"green\", \"orange\")),\n    showlegend = FALSE\n  ) %&gt;%\n  add_trace(segment3d(A, B, color=\"blue\")) %&gt;%\n  add_trace(segment3d(A, C, color=\"red\")) %&gt;%\n  add_trace(segment3d(C, B, color=\"darkgreen\")) %&gt;%\n  layout(\n    title = \"Triangle in 3D: Distances Between Points A, B, and C\",\n    scene = list(\n      xaxis = list(title=\"Var 1\", range=c(0,6)),\n      yaxis = list(title=\"Var 2\", range=c(0,6)),\n      zaxis = list(title=\"Var 3\", range=c(0,6)),\n      aspectmode=\"cube\",\n      camera=list(eye=list(x=1.7,y=1.7,z=1.5))\n    )\n  )\n\nWarning: No trace type specified and no positional attributes specified\n\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\nWarning: No trace type specified and no positional attributes specified\n\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\nWarning: No trace type specified and no positional attributes specified\n\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -&gt; https://plotly.com/r/reference/#scatter\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\n\n\n7.3.3 Euclidean Distance in \\(p\\) Dimensions\nIf each patient is represented by \\(p\\) quantitative variables:\n\\[\nA=\\left(x_1^{(A)}, \\ldots, x_p^{(A)}\\right), \\quad B=\\left(x_1^{(B)}, \\ldots, x_p^{(B)}\\right),\n\\]\nthen Euclidean distance becomes:\n\\[\nd(A, B)=\\sqrt{\\sum_{j=1}^p\\left(x_j^{(B)}-x_j^{(A)}\\right)^2} .\n\\]\nThis is exactly the same formula-just extended over more coordinates.\nHowever, as \\(p\\) grows: - Distances increase automatically - Small differences accumulate across hundreds or thousands of dimensions - Nearest and farthest neighbours become almost the same distance away\nThis distance instability is one of the core reasons PCA and nonlinear methods become necessary. Example in R :\n\nA_vec &lt;- c(1, 2, 0.5, 3, -1, 4)\nB_vec &lt;- c(2, 3, 1.1, 5, -0.2, 3)\n\nsqrt(sum((B_vec - A_vec)^2))\n\n[1] 2.828427\n\ndist(rbind(A_vec, B_vec))\n\n         A_vec\nB_vec 2.828427\n\n\n\n7.3.3.1 Why Euclidean Distance Breaks Down in High Dimensions\nThe formulas above generalise cleanly to any number of dimensions, but the geometry does not. As the number of variables increases—even when each variable is informative and well behaved—Euclidean distances become counterintuitive.\nTwo phenomena are responsible:\nDistance expansion Every additional dimension adds another squared difference. Even tiny differences accumulate.\nDistance concentration The ratio between the smallest and largest pairwise distances approaches 1. In other words, everything becomes equally far apart.\nBoth behaviours arise even in perfectly clean, simulated data, which makes the effect unavoidable in real multi-omics datasets containing thousands of features.\nThe following simulation illustrates distance concentration using random Gaussian points:\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nset.seed(1)\ndims &lt;- c(2, 5, 10, 50, 100, 500)\nn_points &lt;- 300\n\ndistance_summary &lt;- lapply(dims, function(p) {\n  \n  # simulate n points in p dimensions\n  X &lt;- matrix(rnorm(n_points * p), nrow = n_points)\n  \n  # compute all pairwise Euclidean distances\n  D &lt;- dist(X)\n  \n  tibble(\n    dimension = p,\n    min_dist  = min(D),\n    max_dist  = max(D),\n    ratio     = min(D) / max(D)\n  )\n}) |&gt; bind_rows()\n\ndistance_summary\n\n# A tibble: 6 × 4\n  dimension min_dist max_dist   ratio\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1         2   0.0121     7.35 0.00165\n2         5   0.277      7.94 0.0348 \n3        10   1.20       8.76 0.137  \n4        50   6.06      14.7  0.411  \n5       100  10.3       18.4  0.559  \n6       500  26.7       35.4  0.755  \n\n\nWhich can be view with the help of this graphic\n\nggplot(distance_summary, aes(x = dimension, y = ratio)) +\n  geom_line(colour = \"darkblue\", linewidth = 1.2) +\n  geom_point(size = 2.5, colour = \"darkred\") +\n  scale_x_continuous(trans = \"log10\") +\n  theme_minimal(base_size = 14) +\n  labs(\n    title = \"Distance concentration in high dimensions\",\n    x = \"Number of dimensions (log scale)\",\n    y = \"min(distance) / max(distance)\"\n  )\n\n\n\n\n\n\n\n\nAs dimension increases from 2 to 500, the smallest and largest distances become almost indistinguishable. In such settings, the meaning of “closest neighbour” becomes nearly arbitrary.\nThis motivates the transition from Euclidean distance to alternative definitions of similarity.\n\n\n\n7.3.4 Beyond Euclidean Distance: Alternative Metrics for High-Dimensional Data\nHigh-dimensional geometry requires distance measures that remain meaningful even when many variables contribute noise, redundancy, or irrelevant variation. Here we introduce three widely used metrics and illustrate them using small, interpretable examples.\n\nManhattan distance (L1 norm)\nCosine distance (angular similarity)\nMahalanobis distance (correlation-adjusted)\n\nEach has a different geometric interpretation and different strengths in multi-omics settings.\nManhattan Distance (L1) Manhattan distance replaces the \\(\\sqrt{ }\\) (sum of squares) with the sum of absolute differences:\n\\[\nd_{\\operatorname{Man}}(A, B)=\\sum_{j=1}^p\\left|x_j^{(B)}-x_j^{(A)}\\right| .\n\\]\nIt tends to behave more robustly when many dimensions include noise because differences do not get squared.\nExample with three variables:\n\ndf_small &lt;- data.frame(\n  PASI  = c(10, 12),\n  NO2   = c(20, 55),\n  gene1 = c(0.1, 0.9)\n)\n\ndist(df_small, method = \"manhattan\")\n\n     1\n2 37.8\n\n\nManhattan distance often produces more stable neighbour relationships in high-dimensional spaces than Euclidean distance. Cosine Distance (Angular Similarity) Cosine distance measures angle, not absolute magnitude. Two observations are considered similar when they point in the same direction in high-dimensional space —even if their overall magnitude differs.\n\\[\nd_{\\cos }(A, B)=1-\\frac{A \\cdot B}{\\|A\\|\\|B\\|} .\n\\]\nThis metric is extremely common in gene expression, text data, microbiome compositional profiles, and sparse features.\nExample (using the Isa package):\n\nlibrary(lsa)\n\nLoading required package: SnowballC\n\ndf_small &lt;- data.frame(\n  PASI  = c(10, 12),\n  NO2   = c(20, 55),\n  gene1 = c(0.1, 0.9)\n)\n\ncosine_distance &lt;- function(a, b) {\n  a &lt;- as.numeric(a)\n  b &lt;- as.numeric(b)\n  1 - sum(a * b) / (sqrt(sum(a^2)) * sqrt(sum(b^2)))\n}\n\ncosine_distance(df_small[1,], df_small[2,])\n\n[1] 0.03086137\n\n\nCosine distance is not affected by overall scaling-only by relative patterns.\nMahalanobis Distance: A Correlation-Aware Metric Mahalanobis distance accounts for: - correlations among variables, - differences in scale, - redundant dimensions.\nIt rescales the space by the inverse covariance matrix so that correlated variables do not artificially inflate distance.\n\\[\nd_{\\mathrm{Mah}}(A, B)=\\sqrt{(A-B)^{\\top} \\Sigma^{-1}(A-B)} .\n\\]\nThis metric is especially powerful in multi-omics settings, where large blocks of correlated genes or microbial taxa dominate Euclidean distance.\nExample:\n\nlibrary(MASS)   # for ginv()\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:plotly':\n\n    select\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n# ------------------------------------------------------------\n# Small example dataset (3 variables, 3 observations)\n# This guarantees that the covariance matrix is singular.\n# ------------------------------------------------------------\ndf_small &lt;- data.frame(\n  PASI  = c(10, 12, 18),\n  NO2   = c(20, 55, 45),\n  gene1 = c(0.1, 0.9, 0.5)\n)\n\n# Convert to matrix so rows behave as numeric vectors\nX &lt;- as.matrix(df_small)\n\n# ------------------------------------------------------------\n# Covariance matrix of the variables\n# Note: With only 3 observations, Sigma is singular (non-invertible)\n# ------------------------------------------------------------\nSigma &lt;- cov(X)\n\n# Inspect Sigma\nSigma\n\n          PASI NO2 gene1\nPASI  17.33333  35  0.40\nNO2   35.00000 325  7.00\ngene1  0.40000   7  0.16\n\n# ------------------------------------------------------------\n# Extract two patient vectors A and B\n# ------------------------------------------------------------\nA &lt;- X[1, ]\nB &lt;- X[2, ]\n\n# ------------------------------------------------------------\n# Mahalanobis distance with pseudoinverse\n# solve(Sigma) would fail here because Sigma is singular.\n# We use the Moore–Penrose pseudoinverse instead.\n# ------------------------------------------------------------\ninvSigma &lt;- ginv(Sigma)   # always safe, even if Sigma is singular\n\nd_mahal &lt;- sqrt( t(A - B) %*% invSigma %*% (A - B) )\nd_mahal\n\n     [,1]\n[1,]    2\n\n\n\n\n7.3.5 Distances in a nutshell!\nEach distance metric encodes a different notion of similarity, and in high-dimensional biomedical datasets these differences become substantial rather than cosmetic. Euclidean distance measures absolute differences along every axis, Manhattan accumulates deviations linearly, cosine distance emphasises direction rather than magnitude, and Mahalanobis adjusts for correlations and redundant structure in the data.\nIn low dimensions, these metrics often agree, but in multi-omics settings the neighbourhood structure can change dramatically depending on which one is used. Two patients may lie close under Euclidean distance yet appear far apart under cosine distance if their expression levels differ in magnitude but share the same pattern. Conversely, a block of highly correlated genomic variables can inflate Euclidean distances while Mahalanobis explicitly discounts such redundancy.\nThe three-variable example introduced earlier illustrates these contrasts. Euclidean distance reacts most strongly to large shifts in a single axis (e.g. NO₂), Manhattan evaluates overall divergence, cosine focuses on relative shapes, and Mahalanobis amplifies differences along decorrelated directions while down-weighting correlated ones. This diversity of geometric emphasis explains why PCA, t-SNE and UMAP uncover different structural views of the same dataset: each implicitly relies on a different distance behaviour.\nAs dimensionality grows, Euclidean distance deteriorates due to expansion and concentration: all points become far from one another, and nearest neighbours become indistinguishable. Neighbourhood-based methods then operate on unstable geometry. Although alternative metrics alleviate some problems, none escape the curse of dimensionality entirely; reducing the dimension of the data becomes unavoidable.\nTo visualise how different metrics behave, we can compare their distance matrices for the small toy dataset:\n\nlibrary(lsa)\nlibrary(MASS)\n\ndf_small &lt;- data.frame(\n  PASI  = c(10, 12, 18),\n  NO2   = c(20, 55, 45),\n  gene1 = c(0.1, 0.9, 0.5)\n)\n\nX &lt;- as.matrix(df_small)\n\n# Euclidean\nD_euc &lt;- dist(X, method = \"euclidean\")\n\n# Manhattan\nD_man &lt;- dist(X, method = \"manhattan\")\n\n# Cosine\ncosine_distance &lt;- function(a, b) 1 - cosine(a, b)\nD_cos &lt;- outer(1:3, 1:3,\n               Vectorize(function(i, j) cosine_distance(X[i, ], X[j, ])))\n\n# Mahalanobis using pseudoinverse\nSigma &lt;- cov(X)\ninvSigma &lt;- ginv(Sigma)\nd_mahal_pair &lt;- function(i, j) {\n  diff &lt;- X[i, ] - X[j, ]\n  sqrt(t(diff) %*% invSigma %*% diff)\n}\nD_mah &lt;- outer(1:3, 1:3, Vectorize(d_mahal_pair))\n\nD_euc\n\n         1        2\n2 35.06622         \n3 26.25186 11.66876\n\nD_man\n\n     1    2\n2 37.8     \n3 33.4 16.4\n\nD_cos\n\n            [,1]       [,2]        [,3]\n[1,] 0.000000000 0.03086137 0.003471099\n[2,] 0.030861366 0.00000000 0.013708971\n[3,] 0.003471099 0.01370897 0.000000000\n\nD_mah\n\n     [,1] [,2] [,3]\n[1,]    0    2    2\n[2,]    2    0    2\n[3,]    2    2    0\n\n\nEven in this tiny example the matrices differ meaningfully; in a dataset with thousands of variables, the differences become dramatic. Because PCA, t-SNE, UMAP, SOM and LLE are all distance-driven methods—each constructing neighbourhoods in its own way—any distortion in the metric propagates directly into the final embedding.\nLinear PCA implicitly assumes Euclidean geometry; t-SNE converts Euclidean distances into local Gaussian similarities; UMAP constructs a fuzzy nearest-neighbour graph; SOM updates lattice neurons using Euclidean distance; and LLE reconstructs each point from its neighbours. If the distance metric becomes unstable, PCA may emphasise noise, t-SNE may fragment the space arbitrarily, UMAP may form spurious clusters, SOM may impose patterns, and LLE may fail to unfold manifolds.\nUnderstanding the geometry of distance is therefore not optional: it determines the validity, stability and interpretability of all nonlinear representations that follow.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "high_dims.html#techniques-for-visualizing-multiple-dimensions-in-a-single-graphic",
    "href": "high_dims.html#techniques-for-visualizing-multiple-dimensions-in-a-single-graphic",
    "title": "7  High Dimension Data Strategies",
    "section": "7.4 Techniques for Visualizing Multiple Dimensions in a Single Graphic",
    "text": "7.4 Techniques for Visualizing Multiple Dimensions in a Single Graphic\nAs the number of variables in a dataset increases, direct visualization becomes progressively more challenging. While simple scatterplots or pairs plots can depict relationships among a handful of variables, they quickly lose interpretability once dozens or hundreds of measurements are involved. A few multivariate visualization techniques extend the capacity of graphical exploration and allow many dimensions to be represented simultaneously, each preserving different aspects of the underlying structure.\nOne approach is the parallel coordinate plot, in which each variable is drawn as a vertical axis and each observation is represented as a polyline crossing all axes. When many observations follow similar multivariate patterns, their lines tend to form coherent bundles, whereas unusual or distinct profiles appear as diverging trajectories. This style of plot is particularly useful for identifying latent groupings or outliers across large sets of features. In R, the GGally package provides convenient functions for producing such plots:\n\nlibrary(GGally)\nlibrary(dplyr)\n\ndf_small &lt;- df %&gt;% \n  dplyr::select(PASI_baseline, NO2, PM10, gene_1, gene_2, Outcome_PASI12)\n\nggparcoord(\n  data = df_small,\n  columns = 1:6,\n  scale = \"std\",\n  alphaLines = 0.4\n) +\n  theme_minimal() +\n  labs(title = \"Parallel coordinate plot of selected variables\")\n\n\n\n\n\n\n\n\n\n7.4.1 Lattice graphics for multivariate data\nThe lattice system in R provides an alternative framework for displaying multivariate relationships, structured around the idea of conditioning plots. Rather than attempting to represent many dimensions within a single panel, lattice graphics show how the relationship between two variables changes across levels or ranges of a third. This approach is particularly useful in high-dimensional clinical or molecular datasets, where interactions among patient characteristics, environmental exposures, and molecular markers may vary across subgroups.\nLattice plots are built on the notion of trellis displays: grids of panels, each panel corresponding to a subset of the data defined by conditioning variables. In a dataset combining genomic expression, exposome profiles, and clinical severity, conditioning can be used to reveal how associations differ across patient segments. For example, one may examine how NO₂ exposure relates to baseline PASI stratified by quartiles of an inflammatory gene, or how exposure–response patterns differ across microbiome clusters. This form of visualization is especially effective when the structure being investigated is not globally linear but instead varies across local regions of the dataset.\nThe following illustration uses lattice to examine the relationship between baseline PASI and NO₂ exposure, conditioned on quartiles of an inflammatory gene:\n\nlibrary(lattice)\nlibrary(dplyr)\n\n# Construct gene-based strata for conditioning\ndf_lattice &lt;- df %&gt;% \n  mutate(gene_band = ntile(gene_1, 4))\n\nxyplot(\n  NO2 ~ PASI_baseline | factor(gene_band),\n  data = df_lattice,\n  layout = c(2, 2),\n  pch = 16, col = \"darkred\",\n  xlab = \"Baseline PASI\",\n  ylab = \"NO₂ exposure\",\n  main = \"Conditioned scatterplots by quartiles of gene_1 expression\"\n)\n\n\n\n\n\n\n\n\nIn each panel, the basic exposure–severity pattern is retained, but the distribution shifts according to the gene expression stratum. This helps illustrate how a third variable, which might not be easy to represent directly in low-dimensional space, can modulate an otherwise diffuse relationship.\nThe lattice framework also supports multivariate conditioning with smoothers, which can help highlight structure that is not visually apparent in raw scatterplots. For instance:\n\nxyplot(\n  NO2 ~ PASI_baseline | factor(gene_band),\n  data = df_lattice,\n  layout = c(2, 2),\n  panel = function(x, y, ...) {\n    panel.xyplot(x, y, pch = 16, col = \"gray40\")\n    panel.loess(x, y, col = \"darkblue\", lwd = 2)\n  },\n  xlab = \"Baseline PASI\",\n  ylab = \"NO₂ exposure\"\n)\n\n\n\n\n\n\n\n\nBy adding a panel-specific smoother, each facet displays a local trend, making differences across strata easier to compare.\nFinally, lattice can also be used for conditioned density plots or conditioned histograms, which are valuable when exploring differences in molecular or environmental distributions across clinical outcome groups. For example:\n\ndensityplot(\n  ~ gene_1 | cut(Outcome_PASI12, breaks = 3),\n  data = df,\n  plot.points = FALSE,\n  col = \"purple\",\n  lwd = 2,\n  xlab = \"Gene 1 expression (scaled)\",\n  main = \"Conditioned density plots of inflammatory gene expression\"\n)\n\n\n\n\n\n\n\n\nTaken together, lattice graphics offer a flexible way to explore high-dimensional data through structured, conditioned displays. Although they do not circumvent the dimensionality problem, they provide an interpretable set of tools for examining how relationships among variables differ across biologically meaningful subspaces. These displays complement the dimension-reduction techniques introduced later in the chapter by offering an intermediate step: they preserve the original variables while still revealing structure that single-panel plots would miss.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "high_dims.html#the-curse-of-dimensionality",
    "href": "high_dims.html#the-curse-of-dimensionality",
    "title": "7  High Dimension Data Strategies",
    "section": "7.5 The curse of dimensionality",
    "text": "7.5 The curse of dimensionality\nHigh-dimensional datasets behave in ways that differ fundamentally from the low-dimensional settings we can visualize. As the number of variables increases, the geometry of the underlying data space changes in counterintuitive ways. Points become sparse, distances lose discriminatory power, correlations proliferate, and meaningful structure often becomes hidden along directions that are not directly observable. This collection of phenomena is known as the curse of dimensionality, a term originally associated with numerical approximation but now widely used to describe a broad suite of challenges in statistical learning.\nTo understand why high-dimensional analysis becomes so difficult, it helps to examine how distance, volume, and neighbourhood structure evolve as the dimension grows. In ordinary two- or three-dimensional spaces, intuition works reasonably well: nearby points truly feel close, clusters are compact, and the geometry is navigable. But in very high dimensions—such as those produced by genomic expression matrices, microbiome abundance vectors, or exposome profiles—these intuitive notions break down.\nIn high dimensions, the distance between the closest pair of points and the distance between the most distant pair become nearly identical. This undermines methods that rely on relative distances—nearest-neighbour classification, clustering, kernel methods, or manifold estimation—because the contrast between “near” and “far” disappears.\nThe simulation below illustrates this phenomenon: as we increase the number of dimensions p, the ratio between the minimum and maximum pairwise distances approaches one.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nset.seed(123)\n\ndims &lt;- c(2, 5, 10, 50, 100, 500)\nn_points &lt;- 200\n\ndistance_summary &lt;- lapply(dims, function(p) {\n  X &lt;- matrix(rnorm(n_points * p), nrow = n_points)\n  D &lt;- dist(X)\n  tibble(\n    dimension = p,\n    min_dist = min(D),\n    max_dist = max(D),\n    ratio = min(D) / max(D)\n  )\n}) %&gt;% bind_rows()\n\ndistance_summary\n\n# A tibble: 6 × 4\n  dimension min_dist max_dist   ratio\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1         2   0.0198     5.58 0.00355\n2         5   0.327      7.42 0.0440 \n3        10   1.27       8.45 0.150  \n4        50   6.53      14.3  0.458  \n5       100  10.3       17.9  0.578  \n6       500  28.1       35.2  0.797  \n\n\n\nggplot(distance_summary, aes(x = dimension, y = ratio)) +\n  geom_line(size = 1.1, colour = \"darkblue\") +\n  geom_point(size = 2, colour = \"darkred\") +\n  scale_x_continuous(trans = \"log10\") +\n  labs(\n    title = \"Concentration of distances in high dimensions\",\n    x = \"Dimension (log scale)\",\n    y = \"min(distance) / max(distance)\"\n  ) +\n  theme_minimal(base_size = 14)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nThis abstract simulation reflects precisely what happens in our psoriasis dataset. Although each patient lives in a space defined by nearly two thousand variables, the raw Euclidean distance between patients is dominated by noise contributed by unrelated dimensions. The genomic block alone—1500 correlated genes—causes distances to be large and nearly homogeneous across patients. The addition of microbiome features and exposome variables further increases the dimensionality, pushing distances toward a narrow band where meaningful biological differences no longer translate into measurable separation. Even patients with very different underlying inflammatory signatures may end up appearing similarly distant when measured in the full raw feature space.\nAnother consequence of high dimensionality is the rise of collinearity. Biological systems contain modules of coordinated activity: groups of genes co-regulated by the same pathway, microbial taxa that track together, or environmental exposures that co-vary seasonally. As the number of variables increases, so does the probability that many of them are redundant, linearly dependent, or nearly so. This reduces the effective dimensionality of the dataset, creates numerical instability, and complicates interpretation. In the psoriasis example, the IL-17/IL-23 axis produces clusters of highly correlated genes, and microbial communities often form compositional modules. These structures amplify the curse of dimensionality by introducing broad corridors of dependency within a vast space.\n\n7.5.1 Collinearity\nTaken together, these effects—sparsity, distance concentration, and collinearity—make high-dimensional spaces difficult to navigate and even harder to model. Visual inspection becomes uninformative, distances lose meaning, and the most relevant biological structure can remain hidden along composite directions that are not aligned with any single variable. For these reasons, reducing the dimensionality of the data becomes essential. Dimensionality reduction seeks to identify low-dimensional representations that preserve the key structure of the dataset—variation, neighbourhood relationships, clusters, or manifold geometry—while eliminating noise and redundancy. In other words reduction dimension techniques makes possible to represent the same information in a dataset with new created variables, in number lower than the original number of features.\nBefore turning to specific techniques such as principal component analysis, it is helpful to examine more closely how collinearity interacts with the curse of dimensionality and why addressing it is a natural first step in any high-dimensional workflow.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "high_dims.html#collinearity-and-redundancy-in-high-dimensions",
    "href": "high_dims.html#collinearity-and-redundancy-in-high-dimensions",
    "title": "7  High Dimension Data Strategies",
    "section": "7.6 Collinearity and Redundancy in High Dimensions",
    "text": "7.6 Collinearity and Redundancy in High Dimensions\nCollinearity refers to the situation in which two or more variables convey overlapping information because they move together in a systematic way. Formally, a set of predictors\n\\[\nX_1, X_2, \\ldots, X_p\n\\]\nis collinear when one or more of them can be expressed as an approximately linear combination of the others. In matrix notation, this means the covariance matrix\n\\[\n\\Sigma=\\frac{1}{n} X^{\\top} X\n\\]\ncontains near-redundant columns, leading to high pairwise correlations or even near-zero eigenvalues. The geometric implication is that the data cloud occupies a much lower-dimensional subspace than the number of measured variables would suggest.\nCollinearity is a defining feature of multi-omics data. Gene expression modules often arise from common transcriptional programs, generating blocks of highly correlated genes. Microbiome abundance features exhibit strong dependencies due to compositional constraints and ecological co-occurrence patterns. Exposome variables often move together through seasonal influences or shared environmental conditions. When such blocks coexist within the same dataset, the result is a geometry dominated by a handful of latent processes embedded in a very large number of measured variables.\nIn the psoriasis example, the IL-17/IL-23 inflammatory axis drives coordinated expression across dozens of genes, many of which contribute redundant information. To illustrate this, consider the correlation structure of the first 50 genes in the genomic block:\n\nlibrary(ggplot2)\nlibrary(reshape2)\n\ngene_subset &lt;- df %&gt;% \n  dplyr::select(starts_with(\"gene_\")) %&gt;% \n  dplyr::select(1:50)\n\ncor_mat &lt;- cor(gene_subset)\n\ncor_df &lt;- melt(cor_mat)\n\nggplot(cor_df, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"C\", limits = c(-1, 1)) +\n  coord_fixed() +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Correlation heatmap of 50 genomic features\",\n    x = \"\",\n    y = \"\",\n    fill = \"Correlation\"\n  )\n\n\n\n\n\n\n\n\nThe heatmap reveals correlated genes, reflecting shared underlying regulatory signals rather than independent biological drivers. When scaled to the full 1500-gene space, this dependence creates long corridors of redundancy where many axes contribute little unique information.\nCollinearity also manifests in the microbiome block. Even though microbial abundance features are generated independently in the simulation, they are anchored to latent dysbiosis and commensal signatures. This induces patterns that resemble ecological modules. A simple visualization using the first 30 microbiome features shows similar structure:\n\nmicro_subset &lt;- df %&gt;% \n dplyr::select(starts_with(\"ASV_\")) %&gt;% \n  dplyr::select(1:30)\n\nmicro_cor &lt;- cor(micro_subset)\n\nmicro_df &lt;- melt(micro_cor)\n\nggplot(micro_df, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = \"C\", limits = c(-1, 1)) +\n  coord_fixed() +\n  theme_minimal(base_size = 12) +\n  labs(\n    title = \"Correlation heatmap of 30 microbiome features\",\n    x = \"\",\n    y = \"\",\n    fill = \"Correlation\"\n  )\n\n\n\n\n\n\n\n\nThese dependencies have several consequences. Regressions that use raw variables become unstable because small perturbations in the data can lead to large changes in estimated coefficients. Distance-based procedures weigh correlated dimensions repeatedly, distorting neighbourhood structure. Clustering algorithms may split or merge groups based on redundant patterns rather than true latent structure. Interpretability also suffers: when many variables carry essentially the same information, it becomes difficult to discern which dimensions reflect genuine biological signals.\nDespite these challenges, collinearity offers a key opportunity: it reveals that the data often lie near a low-dimensional subspace. In other words, even though the measured feature space spans thousands of axes, the underlying variation is driven by a much smaller number of latent processes. Dimensionality-reduction methods exploit this fact. By identifying directions along which the data vary most, or by seeking low-dimensional manifolds that preserve neighbourhood relations, these techniques both stabilize analysis and reveal interpretable structure.\nThe presence of strong collinearity therefore provides a natural bridge to dimensionality reduction. Principal component analysis (PCA), in particular, is designed to extract the dominant axes of variation from high-dimensional spaces. Before applying nonlinear methods such as t-SNE, UMAP, or self-organizing maps, PCA offers the first principled approach to summarizing structure, stabilizing computations, and mitigating the effects of the curse of dimensionality.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "high_dims.html#principal-component-analysis",
    "href": "high_dims.html#principal-component-analysis",
    "title": "7  High Dimension Data Strategies",
    "section": "7.7 Principal Component Analysis",
    "text": "7.7 Principal Component Analysis\nPrincipal Component Analysis (PCA) is a method for re-expressing high-dimensional data in terms of a smaller set of orthogonal axes that capture the dominant patterns of variation. It is built on a simple geometric idea: in any cloud of points occupying a high-dimensional space—such as our Dermato-ATT dataset, which contains gene expression, microbiome profiles, exposome measurements, and inflammatory biomarkers—some directions contain far more variability than others. PCA identifies these directions and rotates the coordinate system so that the first axis aligns with the greatest possible variance, the next axis captures the largest remaining variance under the constraint of orthogonality, and subsequent axes continue this pattern.\nThe following Figure llustrates this idea using a simple two-dimensional slice of a multivariate dataset. The original coordinate axes are fixed, but the point cloud stretches more strongly along an oblique direction. PCA identifies this dominant direction of spread (PC1) and the largest remaining orthogonal direction (PC2). These new axes are not selected variables but linear combinations of them, and they form the rotated coordinate system that best captures the intrinsic geometry of the dataset.\n\nlibrary(ggplot2)\nlibrary(MASS)\nset.seed(123)\n\n# Generate a correlated cloud (so rotation is visible)\nSigma &lt;- matrix(c(3, 2.7,\n                  2.7, 3), nrow = 2)\n\ndf_2 &lt;- MASS::mvrnorm(n = 300,\n                    mu = c(0, 0),\n                    Sigma = Sigma) %&gt;%\n  as.data.frame() %&gt;%\n  setNames(c(\"x\", \"y\"))\n\n# Fit PCA\npca_fit_2 &lt;- prcomp(df_2, scale. = FALSE)\nrot &lt;- pca_fit_2$rotation\nprop_var &lt;- pca_fit_2$sdev^2 / sum(pca_fit_2$sdev^2)\n\n# Length of arrows\nscale_factor &lt;- 3\n\nPC1 &lt;- rot[,1] * scale_factor\nPC2 &lt;- rot[,2] * scale_factor\n\nggplot(df_2, aes(x, y)) +\n  geom_point(alpha = 0.5, size = 2) +\n  \n  # Original axes\n  geom_segment(aes(x = 0, y = 0, xend = scale_factor, yend = 0),\n               color = \"gray40\", size = 1.2) +\n  geom_segment(aes(x = 0, y = 0, xend = 0, yend = scale_factor),\n               color = \"gray40\", size = 1.2) +\n\n  # PCA axes\n  geom_segment(aes(x = 0, y = 0, xend = PC1[1], yend = PC1[2]),\n               arrow = arrow(length = unit(0.3, \"cm\")),\n               color = \"red\", size = 1.4) +\n  geom_segment(aes(x = 0, y = 0, xend = PC2[1], yend = PC2[2]),\n               arrow = arrow(length = unit(0.3, \"cm\")),\n               color = \"blue\", size = 1.4) +\n  \n  annotate(\"text\", x = PC1[1]*1.1, y = PC1[2]*1.1,\n           label = paste0(\"PC1 (\", round(prop_var[1]*100,1), \"%)\"),\n           color = \"red\", size = 5.2, fontface = \"bold\") +\n  \n  annotate(\"text\", x = PC2[1]*1.1, y = PC2[2]*1.1,\n           label = paste0(\"PC2 (\", round(prop_var[2]*100,1), \"%)\"),\n           color = \"blue\", size = 5.2, fontface = \"bold\") +\n  \n  labs(\n    title = \"Geometric interpretation of PCA as a rotation\",\n    subtitle = \"PC1 aligns with the direction of maximal spread; PC2 is orthogonal and captures the next largest variation\",\n    x = \"Original axis 1\",\n    y = \"Original axis 2\"\n  ) +\n  theme_minimal(base_size = 14)\n\nWarning in geom_segment(aes(x = 0, y = 0, xend = scale_factor, yend = 0), : All aesthetics have length 1, but the data has 300 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0, xend = 0, yend = scale_factor), : All aesthetics have length 1, but the data has 300 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0, xend = PC1[1], yend = PC1[2]), : All aesthetics have length 1, but the data has 300 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0, xend = PC2[1], yend = PC2[2]), : All aesthetics have length 1, but the data has 300 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nFrom a geometric point of view, the operation is a rotation of the data cloud. If each patient is represented as a point in a space where every molecular or environmental variable forms an axis of its own, the cloud of points spreads more extensively along certain directions than others. PCA finds these directions and uses them as new axes. The resulting “principal components’’ are not simply chosen variables; they are linear combinations of the originals and correspond to slanted directions in the original multidimensional space. These combinations are constructed so that the first component captures the broadest overall spread, the second captures the broadest remaining spread orthogonally, and subsequent components continue the pattern.\nAlgebraically, PCA corresponds to computing the eigen decomposition of the covariance (or correlation) matrix of the standardized variables. Each eigenvector gives the coefficients of a principal component, and each eigenvalue gives the amount of variance encoded by that direction. Because the eigenvectors are orthogonal, each component adds genuinely new information not captured by earlier components.\nIn the psoriasis dataset, this geometric rotation often reveals interpretable biological structure. A dominant inflammatory axis typically emerges, blending IL-17 pathway genes, cytokine concentrations, and sometimes specific microbial signatures associated with dysbiosis. A separate axis may isolate environmental gradients, mixing ultraviolet exposure, particulate matter, temperature, and humidity. Although PCA does not know anything about treatment response, these principal directions often correlate with disease activity or therapeutic improvement, making the representation clinically informative.\n\n7.7.1 Building your first PCA model\nBefore fitting a PCA model it is essential to select appropriate variables. PCA can only process quantitative variables because it depends on the computation of variances and covariances in Euclidean space; categorical variables do not have meaningful numerical variance and cannot be incorporated directly. Identifiers, treatment labels, diagnostic categories, and any other non-numeric columns must either be removed or handled using dedicated extension methods such as MCA or FAMD.\nFor the Dermato-ATT dataset, we therefore begin by selecting the quantitative predictors—gene expression markers, microbial profiles, exposome features, and biochemical biomarkers. The 12-week PASI improvement is excluded because PCA is unsupervised. Standardisation is also essential. Without scaling, variables with very large units or natural ranges would dominate the computation of covariance, forcing PCA to align itself with measurement scale rather than with biological structure.\n\nlibrary(dplyr)\n\ndf_pca &lt;- df %&gt;%\n  dplyr::select(-patient_id, -Outcome_PASI12) %&gt;%\n  dplyr::mutate(across(everything(), scale))\n\n\n\n7.7.2 Fitting PCA Using FactoMineR\nWe now fit PCA using the FactoMineR package\n\nlibrary(FactoMineR)\nlibrary(factoextra)\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nres.pca &lt;- PCA(df_pca, graph = FALSE)\n#str(res.pca)\n\nThe code above shows that FactoMineR organises the entire PCA output into a coherent structure that mirrors the geometry of the method. The eigenvalues quantify how much variance each principal component captures, while the associated proportions indicate the relative importance of the axes. The variables are represented through their coordinates on the components, the quality of their representation (cos²), and their contributions to each axis, all of which help reveal which molecular or environmental features shape the underlying structure. Individuals are described in parallel, with their coordinates, cos² values, and contributions providing insight into how patients are positioned in the new latent space. The correlation circle summarises the relationships among variables by projecting them into the component space, and optional supplementary individuals or variables can be projected onto the existing axes without influencing their construction.\nThe variance explained is by each PC is obtained via res.pca$eig of which we show the firt 6 elements below:\n\nhead(res.pca$eig)\n\n        eigenvalue percentage of variance cumulative percentage of variance\ncomp 1 1184.427938             64.0231318                          64.02313\ncomp 2  203.856623             11.0192769                          75.04241\ncomp 3   47.985837              2.5938290                          77.63624\ncomp 4    6.143177              0.3320636                          77.96830\ncomp 5    4.143578              0.2239772                          78.19228\ncomp 6    4.060046              0.2194620                          78.41174\n\n\nTo visualize the variance structure we can use a graphic name scree plot like the one below that represents the percentage of explained variances by each PC.\n\nfviz_screeplot(res.pca, ncp = 10)\n\nWarning in geom_bar(stat = \"identity\", fill = barfill, color = barcolor, :\nIgnoring empty aesthetic: `width`.\n\n\n\n\n\n\n\n\n\n\n\n7.7.3 Scores and Loadings: What They Represent and How to Extract Them\nOnce a PCA model has been fitted, two sets of quantities become central for interpretation: the scores and the loadings. Scores give the coordinates of each individual in the new component space. They show where each patient sits along the latent axes of variation uncovered by PCA, making it possible to visualise biological separation, identify clusters, or explore relationships with clinical outcomes. Loadings describe how each principal component is constructed from the original variables. Each loading is a coefficient in the linear combination that forms the component, and variables with large absolute loadings are those that most strongly influence its orientation.\nFactoMineR stores these quantities within the res.pca$ind and res.pca$var lists. The coordinates of individuals are found in res.pca$ind$coord, while the coordinates of variables—often interpreted as loadings in PCA with scaling—are stored in res.pca$var$coord. These coordinates are essential for building interpretability: they clarify which biological modules dominate each axis and how individuals project onto them.\n\n# Scores (individual coordinates)\nscores &lt;- res.pca$ind$coord\n\n# Loadings (variable coordinates)\nloadings &lt;- res.pca$var$coord\n\nBecause all variables were standardised before PCA, each loading can be interpreted directly as the correlation between the variable and the component. Larger absolute values reflect stronger association with the PC axis.\nTo see the variance structure of the components, a scree plot provides an immediate overview of how much information each axis retains. This is especially useful in high-dimensional multi-omics settings, where only a handful of components represent most of the meaningful covariance.\n\nfviz_screeplot(res.pca, ncp = 10)\n\nWarning in geom_bar(stat = \"identity\", fill = barfill, color = barcolor, :\nIgnoring empty aesthetic: `width`.\n\n\n\n\n\n\n\n\n\n\n\n7.7.4 Identifying the Most Important Variables for Each Component\nUnderstanding which variables drive a principal component requires examining their contributions. FactoMineR directly provides contributions (in %) through res.pca\\(var\\)contrib, which quantify how much each variable participates in forming each axis. Higher contribution values indicate more influential variables.\n\ncontrib &lt;- res.pca$var$contrib\n# Top 10 for PC1, PC2, PC3\n\nFrom the later matrix in res.pca$var$contrib, we can extract the ten most important variables for any component. Below is a general approach to obtain the top contributors for the first three components.\n\n library(ggplot2)\nlibrary(dplyr)\nlibrary(tibble)\nlibrary(patchwork)\n\n\nAttaching package: 'patchwork'\n\n\nThe following object is masked from 'package:MASS':\n\n    area\n\n# -----------------------------------------------------------\n# top10_vars(): Extract the Top 10 contributing variables for a PCA component\n#\n# This helper function retrieves the variables that contribute\n# most strongly to a given principal component.\n#\n# INPUT:\n#   component  -&gt; integer (1 = PC1, 2 = PC2, etc.)\n#\n# INTERNALS:\n#   contrib is a matrix from FactoMineR:\n#     rows   = variables\n#     cols   = principal components\n#\n# STEPS:\n#   1. Select the column corresponding to the chosen PC.\n#   2. Sort contributions from highest to lowest.\n#   3. Keep only the top 10 variables.\n#   4. Convert to a tidy tibble for easy plotting.\n#   5. Reverse factor order so barplots appear in descending order.\n#\n# OUTPUT:\n#   A tibble with two columns:\n#     - variable: variable name\n#     - contribution: percentage contribution to the selected PC\n# -----------------------------------------------------------\ntop10_vars &lt;- function(component) {\n  contrib[, component] |&gt;                       # extract contributions for the given PC\n    sort(decreasing = TRUE) |&gt;                  # order from strongest to weakest\n    head(10) |&gt;                                 # keep the top 10\n    enframe(name = \"variable\",                  # convert vector to tibble\n            value = \"contribution\") |&gt;\n    mutate(variable = factor(variable,          # ensures barplot ordering\n                              levels = rev(variable)))\n}\n\n# -----------------------------------------------------------\n# plot_top10(): Barplot of the top 10 contributors to one PC\n#\n# This function visualises the output of top10_vars().\n#\n# CHOICES:\n#   - Horizontal bars (coord_flip) improve label readability,\n#     especially for gene or ASV names.\n#   - Steelblue bars + minimal theme give a clean textbook look.\n#\n# OUTPUT:\n#   A ggplot object.\n# -----------------------------------------------------------\nplot_top10 &lt;- function(component) {\n  df &lt;- top10_vars(component)\n\n  ggplot(df, aes(x = variable, y = contribution)) +\n    geom_col(fill = \"steelblue\") +              # barplot\n    coord_flip() +                              # horizontal orientation\n    theme_minimal(base_size = 13) +\n    labs(\n      title = paste(\"Top 10 Variable Contributions to PC\", component),\n      x = \"Variable\",\n      y = \"Contribution (%)\"\n    )\n}\n\n# -----------------------------------------------------------\n# Generate barplots for PC1, PC2, and PC3\n#\n# Each plot shows the 10 most influential variables for the\n# corresponding principal component.\n#\n# Patchwork syntax (p1 / p2 / p3) stacks the three plots vertically,\n# producing a coherent combined figure for the textbook.\n# -----------------------------------------------------------\np1 &lt;- plot_top10(1)   # PC1 top contributors\np2 &lt;- plot_top10(2)   # PC2 top contributors\np3 &lt;- plot_top10(3)   # PC3 top contributors\n\np1 / p2 / p3\n\n\n\n\n\n\n\n\n\n# -----------------------------------------------------------\n# Create reusable objects containing the top contributors\n# for the first three principal components.\n#\n# These objects are used later for:\n#   - reduced correlation circles\n#   - reduced biplots\n#   - multi-panel figures combining barplots + loadings\n#\n# By computing them here, we ensure they exist globally\n# throughout the chapter and avoid rendering errors.\n# -----------------------------------------------------------\ntop10_PC1 &lt;- top10_vars(1)\ntop10_PC2 &lt;- top10_vars(2)\ntop10_PC3 &lt;- top10_vars(3)\n\n# Print to confirm structure (optional)\ntop10_PC1\n\n# A tibble: 10 × 2\n   variable  contribution\n   &lt;fct&gt;            &lt;dbl&gt;\n 1 gene_538        0.0779\n 2 gene_364        0.0779\n 3 gene_862        0.0776\n 4 gene_415        0.0775\n 5 gene_404        0.0774\n 6 gene_988        0.0771\n 7 gene_1247       0.0770\n 8 gene_50         0.0770\n 9 gene_97         0.0769\n10 gene_897        0.0769\n\ntop10_PC2\n\n# A tibble: 10 × 2\n   variable contribution\n   &lt;fct&gt;           &lt;dbl&gt;\n 1 ASV_10          0.394\n 2 ASV_235         0.394\n 3 ASV_71          0.390\n 4 ASV_268         0.389\n 5 ASV_292         0.389\n 6 ASV_76          0.387\n 7 ASV_300         0.385\n 8 ASV_264         0.384\n 9 ASV_246         0.384\n10 ASV_225         0.383\n\ntop10_PC3\n\n# A tibble: 10 × 2\n   variable    contribution\n   &lt;fct&gt;              &lt;dbl&gt;\n 1 cytokine_5         0.831\n 2 cytokine_9         0.819\n 3 cytokine_6         0.784\n 4 cytokine_13        0.774\n 5 cytokine_4         0.769\n 6 cytokine_1         0.762\n 7 cytokine_10        0.753\n 8 cytokine_18        0.743\n 9 cytokine_15        0.742\n10 cytokine_8         0.738\n\n\n\n\n7.7.5 Interpreting Positive and Negative Loadings Within Each Component\nFor each principal component, variables may contribute in the same direction or in opposite directions, and this polarity becomes biologically meaningful once the geometry of PCA is understood. A variable with a large positive loading reinforces the orientation of the component, pushing individuals with high values of that variable toward the positive side of the axis. Conversely, a variable with a large negative loading exerts an influence in the opposite direction, pulling individuals toward the negative side. Positive and negative loadings therefore reflect opposing biological tendencies encoded along the same latent dimension.\nIn multi-omics settings, such as the Dermato-ATT dataset, these opposing directions often correspond to antagonistic biological processes. For example, inflammatory gene modules might define the positive direction of PC1, while barrier-restoration transcripts or commensal microbial signatures define the negative direction. Examining both sides of each axis clarifies whether a principal component reflects a true biological gradient rather than a unidirectional cluster of correlated variables.\nThe following code extracts the ten variables with the strongest positive and negative loadings for each of the first three components. Because PCA was performed on standardized variables, the loadings correspond to variable–component correlations, making the sign and magnitude directly interpretable.\n\n# -----------------------------------------------------------\n# Extract strongest positive and negative loadings for a PC\n#\n# INPUT:\n#   component -&gt; integer index (1 = PC1)\n#\n# STEPS:\n#   1. Select loadings for the chosen component.\n#   2. Sort in decreasing order for positive loadings.\n#   3. Sort in increasing order for negative loadings.\n#   4. Keep the 10 strongest on each side.\n#\n# -----------------------------------------------------------\nget_signed_loadings &lt;- function(component) {\n  load_vec &lt;- loadings[, component]\n\n  top_pos &lt;- sort(load_vec, decreasing = TRUE)[1:10]\n  top_neg &lt;- sort(load_vec, decreasing = FALSE)[1:10]\n\n  list(\n    positive = tibble(variable = names(top_pos),\n                      loading  = as.numeric(top_pos)),\n    negative = tibble(variable = names(top_neg),\n                      loading  = as.numeric(top_neg))\n  )\n}\n\n# Build signed-loading summaries for PC1–PC3\nPC1_loads &lt;- get_signed_loadings(1)\nPC2_loads &lt;- get_signed_loadings(2)\nPC3_loads &lt;- get_signed_loadings(3)\n\n\nPC1_loads\n\n$positive\n# A tibble: 10 × 2\n   variable  loading\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 gene_538    0.961\n 2 gene_364    0.961\n 3 gene_862    0.959\n 4 gene_415    0.958\n 5 gene_404    0.957\n 6 gene_988    0.956\n 7 gene_1247   0.955\n 8 gene_50     0.955\n 9 gene_97     0.954\n10 gene_897    0.954\n\n$negative\n# A tibble: 10 × 2\n   variable      loading\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 DLQI          -0.181 \n 2 expo_28       -0.159 \n 3 expo_8        -0.137 \n 4 NO2           -0.115 \n 5 PASI_baseline -0.0982\n 6 expo_29       -0.0935\n 7 expo_27       -0.0890\n 8 PM10          -0.0878\n 9 expo_15       -0.0656\n10 expo_22       -0.0506\n\nPC2_loads\n\n$positive\n# A tibble: 10 × 2\n   variable loading\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 ASV_10     0.896\n 2 ASV_235    0.896\n 3 ASV_71     0.891\n 4 ASV_268    0.891\n 5 ASV_292    0.891\n 6 ASV_76     0.888\n 7 ASV_300    0.886\n 8 ASV_264    0.885\n 9 ASV_246    0.885\n10 ASV_225    0.884\n\n$negative\n# A tibble: 10 × 2\n   variable      loading\n   &lt;chr&gt;           &lt;dbl&gt;\n 1 gene_943       -0.134\n 2 expo_6         -0.133\n 3 gene_191       -0.131\n 4 gene_362       -0.124\n 5 gene_1349      -0.119\n 6 NO2            -0.119\n 7 cytokine_12    -0.105\n 8 PASI_baseline  -0.103\n 9 gene_659       -0.103\n10 gene_1464      -0.102\n\nPC3_loads\n\n$positive\n# A tibble: 10 × 2\n   variable    loading\n   &lt;chr&gt;         &lt;dbl&gt;\n 1 cytokine_5    0.631\n 2 cytokine_9    0.627\n 3 cytokine_6    0.613\n 4 cytokine_13   0.609\n 5 cytokine_4    0.607\n 6 cytokine_1    0.605\n 7 cytokine_10   0.601\n 8 cytokine_18   0.597\n 9 cytokine_15   0.597\n10 cytokine_8    0.595\n\n$negative\n# A tibble: 10 × 2\n   variable  loading\n   &lt;chr&gt;       &lt;dbl&gt;\n 1 gene_158   -0.516\n 2 gene_721   -0.500\n 3 gene_117   -0.487\n 4 gene_917   -0.462\n 5 gene_317   -0.461\n 6 gene_1493  -0.440\n 7 gene_14    -0.424\n 8 gene_388   -0.402\n 9 gene_1157  -0.401\n10 gene_1165  -0.389\n\n\nThese tables help define the biological interpretation of each PC. PC1 may separate patients along an inflammatory gradient, with high-expression inflammatory genes on the positive side and barrier-related transcripts or microbial signatures on the negative side. PC2 may separate alternative microbial communities in opposite directions, while PC3 may distinguish distinct cytokine patterns that represent systemic immune activation. By examining positive and negative contributions jointly, the latent structure of each component becomes clearer and better grounded in biological mechanisms.\n\n\n7.7.6 Interpretation Unifying Contributions and Loadings Signals\nThe contribution profiles and signed loadings provide two complementary perspectives on the PCA structure: contributions show which variables build the component, while loadings reveal how strongly and in which direction they pull the component axis. Interpreting both together allows a precise reconstruction of the biological meaning of each PC.\nPC1 – Local inflammatory transcriptomic axis\nPC1 is dominated almost exclusively by gene-expression features. The top ten contributors all lie between 7.69% and 7.79%, an extremely tight range that indicates a highly coherent molecular module rather than isolated drivers. Their signed loadings are also extraordinarily large, with values around 0.954–0.961, meaning these genes correlate almost perfectly with the PC1 axis.\nBiologically, this is exactly what one expects from a coordinated inflammatory programme. Such modules in real psoriasis data typically represent:\nIL-17 / IL-23 inflammatory pathways\nkeratinocyte activation transcriptional signatures\ngeneric immune up-regulation genes\nThe negative side of PC1 is weak: the strongest negative loadings (DLQI = –0.181, NO₂ = –0.115, PM10 = –0.088) are very small in magnitude compared with the gene-driven positive loadings (~0.96). This asymmetry shows that PC1 is not a balanced contrast but rather a unipolar inflammatory transcriptomic axis. High PC1 scores reflect strong up-regulation of inflammatory genes; low scores correspond to a quieter molecular state.\nPC2 – Microbial community structure\nPC2 presents an entirely different pattern. The top ten contributors are all microbiome ASVs with contributions between 0.383 and 0.394, nearly an order of magnitude larger than the gene contributions in PC1. Their loadings are also very strong and tightly grouped (0.884–0.896), marking a highly cohesive ecological gradient.\nThe negative loadings are again small: gene_943 (–0.134), PASI_baseline (–0.103), cytokine_12 (–0.105). This polarity shows that PC2 is another unipolar axis, now capturing a microbial dysbiosis vs. commensal community structure, independent of the transcriptomic signals of PC1.\nThe near-orthogonality between PC1 and PC2 directions is reflected numerically by the absence of gene or cytokine variables among the top PC2 contributors, reinforcing that PC2 encodes an orthogonal, microbiome-specific layer of variation.\nPC3 – Systemic cytokine activation\nPC3 is overwhelmingly dominated by cytokines. Contributions for the top 10 cytokines lie between 0.738 and 0.831, by far the largest contribution values among all PCs. The associated loadings are also very strong (0.595–0.631). This pattern reveals another tightly coordinated biological module — now representing systemic immune signalling rather than local transcriptional inflammation.\nOn the negative side, PC3 loadings include several genes (e.g., gene_158 = –0.516, gene_721 = –0.500), showing a moderate inverse relationship between certain gene modules and systemic cytokine levels. Unlike PC1 and PC2, PC3 is more balanced, with both strong positive and moderately strong negative loadings. This indicates that PC3 captures a contrast between systemic cytokine activation (positive direction) and specific gene-expression programmes (negative direction).\nBiologically, PC3 therefore reflects systemic inflammatory activation, distinct from:\nlocal/transcriptomic inflammation (PC1)\nmicrobial structure (PC2)\n\n\n\n\n\n\n\n\n\nComponent\nDominant Contributors (numeric evidence)\nLoadings (direction & magnitude)\nBiological Interpretation\n\n\n\n\nPC1\nGenes: 7.69–7.79% contrib\nVery strong positive loadings 0.954–0.961; weak negatives\nLocal/transcriptomic inflammation\n\n\nPC2\nASVs: 0.383–0.394 contrib\nStrong positive loadings 0.884–0.896; small negatives\nMicrobial dysbiosis / ecological gradient\n\n\nPC3\nCytokines: 0.738–0.831 contrib\nStrong positives 0.595–0.631; moderate negative gene loadings –0.516 to –0.389\nSystemic immune activation\n\n\n\n\n\n7.7.7 Geometric Visualisation Through Scores\nOnce the PCA model has been fitted, each patient can be projected into the space defined by the principal components. These new coordinates—called scores—represent the locations of the individuals after the original high-dimensional cloud has been rotated into a smaller set of orthogonal directions.\nThe first two components often provide the most informative low-dimensional view of the data. The plot below displays the scores of all individuals along PC1 and PC2. Points are coloured according to their 12-week PASI change, but this colour gradient is not used by PCA during computation; it is merely overlaid afterward to allow qualitative inspection.\n\nfviz_pca_ind(\n  res.pca,\n  col.ind = df$Outcome_PASI12,\n  gradient.cols = c(\"blue\",\"yellow\",\"red\"),\n  addEllipses = FALSE\n)\n\n\n\n\n\n\n\n\nAt this stage, interpretation is deliberately limited to the geometric structure visible in the projection. PCA is an unsupervised method: it has no awareness of clinical outcomes, diagnostic groups, or biological pathways. The geometry of the scores plot reflects only the directions of maximal variance in the quantitative variables that were provided to the algorithm.\nSeveral points follow from this:\nPC1 represents the direction of greatest variation in the dataset. It is the axis along which individuals differ most strongly in terms of the original standardized variables. No biological meaning should yet be assigned to it; it is simply the mathematically dominant direction.\nPC2 captures the second-largest remaining variation under the constraint of orthogonality. This ensures that PC1 and PC2 describe independent directions of variation, forming a natural coordinate system for exploring individual differences.\nDistances in the scores plot approximate similarity. Individuals close together have similar profiles across all quantitative variables, whereas individuals far apart differ strongly. This geometry comes directly from the rotation and scaling inherent in PCA.\nColour conveys the outcome but does not influence the PCA. The gradient applied to PASI change is merely a visual overlay. Clusters or gradients in colour should not yet be interpreted as causal or mechanistic patterns; they serve only as a visual hint that outcome might align with one of the major dimensions of variation.\n\n\n7.7.8 Visualising Variable Structure: The Correlation Circle (PC1–PC2 Focus)\nWhile the scores plot reveals how patients distribute themselves across the dominant PCA axes, the correlation circle answers a complementary question:\nWhich variables shape PC1 and PC2, and how do these variables relate to each other?\nIn FactoMineR, each standardized variable is projected into the PC1–PC2 plane according to its correlation with each component. This provides a geometric summary of the high-dimensional structure after PCA has reorganised the dataset.\n\n7.7.8.1 The Standard Correlation Circle (PC1–PC2)\n\n# STANDARD FULL CORRELATION CIRCLE (all variables)\nfviz_pca_var(\n  res.pca,\n  col.var = \"contrib\",\n  gradient.cols = c(\"blue\", \"white\", \"red\"),\n  repel = FALSE\n)\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\nℹ The deprecated feature was likely used in the factoextra package.\n  Please report the issue at &lt;https://github.com/kassambara/factoextra/issues&gt;.\n\n\n\n\n\n\n\n\n\nThis plot displays every variable—all genes, ASVs, exposome variables, and cytokines—projected into the PC1–PC2 plane. The colour scale indicates contribution: darker red arrows exert stronger influence on the components.\n\n\n7.7.8.2 How to Read the Correlation Circle\nSeveral principles guide interpretation:\nDistance from the origin Variables far from the centre are well represented by PC1–PC2 (high cos²). Variables near the centre are poorly explained by these two components.\nAngle between vectors:\n\nSmall angle → strongly positively correlated variables\n180° apart → strongly negatively correlated variables\n\n90° apart → uncorrelated variables\nLength of vectors:\nLong arrows indicate strong correlation with the components; short arrows indicate weak correlation.\nClusters of arrows:\nGroups of variables pointing in similar directions indicate biological modules:\n\ninflammatory gene clusters\nmicrobial ASV communities\nexposome gradients\ncytokine signalling\n\nThese clusters reveal the statistical manifestation of latent processes in the dataset.\n\n\n7.7.8.3 Why the Full Correlation Circle Fails in Multi-Omics\nIn high-dimensional datasets (like ours, with ~2000 variables), the full correlation circle becomes:\n\nvisually overloaded\nunreadable due to label collisions\ndominated by density rather than structure\nunhelpful for identifying which specific variables drive a component\nmisleading when poorly represented variables crowd near the origin\n\nIn short: the plot contains the right information but presents it in the wrong way.\nWhat we want is a plot that:\n\nhighlights the key variables\nshows their alignment along PC1 and PC2\nclarifies biological modules\navoids the “hairball” effect\n\nTo fix this, we build reduced correlation circles that show only the top contributing variables.\n\nfviz_pca_var(\n  res.pca,\n  select.var = list(name = top10_PC1$variable),\n  col.var = \"contrib\",\n  gradient.cols = c(\"blue\", \"white\", \"red\"),\n  repel = TRUE,\n  title = \"Reduced Correlation Circle – Top Contributors to PC1\"\n)\n\n\n\n\n\n\n\n\n\nfviz_pca_var(\n  res.pca,\n  select.var = list(name = top10_PC2$variable),\n  col.var = \"contrib\",\n  gradient.cols = c(\"blue\", \"white\", \"red\"),\n  repel = TRUE,\n  title = \"Reduced Correlation Circle – Top Contributors to PC2\"\n)\n\n\n\n\n\n\n\n\nWe can add the circles above with the barplots already created to have an alternative representation like the following:\n\np_PC1_bar &lt;- plot_top10(1)   # Top 10 contributors to PC1\np_PC2_bar &lt;- plot_top10(2)   # Top 10 contributors to PC2\n\np_PC1_circle &lt;- fviz_pca_var(\n  res.pca,\n  select.var = list(name = top10_PC1$variable),\n  col.var = \"contrib\",\n  gradient.cols = c(\"blue\",\"white\",\"red\"),\n  repel = TRUE,\n  title = \"PC1 – Reduced Correlation Circle\"\n)\n\np_PC2_circle &lt;- fviz_pca_var(\n  res.pca,\n  select.var = list(name = top10_PC2$variable),\n  col.var = \"contrib\",\n  gradient.cols = c(\"blue\",\"white\",\"red\"),\n  repel = TRUE,\n  title = \"PC2 – Reduced Correlation Circle\"\n)\n\n# Combine barplots and circles for interpretability\n(p_PC1_bar | p_PC1_circle) /\n(p_PC2_bar | p_PC2_circle)\n\n\n\n\n\n\n\n\nThe top-left barplot lists the ten variables that contribute most to PC1. All are gene-expression features, and their contribution values are highly similar. This pattern indicates that PC1 captures a coherent transcriptomic axis along which many genes vary in a coordinated fashion.\nIn the reduced correlation circle (top-right):\n\nThe top PC1 genes cluster tightly along the positive direction of the PC1 axis.\nTheir vectors share nearly identical angles, reflecting strong positive correlations among these genes.\nTheir distance from the origin shows that they are well represented by the first two components.\n\nThis configuration reafirms our conclusions regarding PCs and their interpretation\nThe four panels together illustrate several important characteristics of the dataset:\n\nPC1 and PC2 reflect different biological layers.\nPC1 is dominated by gene-expression variables.\nPC2 is dominated by microbiome variables.\n\nThe two layers are largely independent.\nThe near-orthogonality between the gene and ASV vectors indicates that transcriptional and microbial variations contribute separate, non-overlapping sources of structure.\nThe PCA axes acquire a clear biological interpretation reaffirming that PC1 can be viewed as a transcriptional variation axis and PC2 can be viewed as a microbiome-composition axis.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "high_dims.html#introducing-another-example-of-interest-for-the-next-methods",
    "href": "high_dims.html#introducing-another-example-of-interest-for-the-next-methods",
    "title": "7  High Dimension Data Strategies",
    "section": "7.8 Introducing another example of interest for the next methods",
    "text": "7.8 Introducing another example of interest for the next methods\nIn this second part of the chapter, we work with a compact dataset drawn from a hypothetical digital clinical trial involving 500 participants. Each patient contributes a combination of inflammatory biomarkers, wearable-sensor metrics and traditional clinical measurements collected across the study period. Although small, the dataset reflects the type of multimodal structure that routinely arises in digitally augmented trials—where physiological, behavioural and biological signals interact in ways that are often nonlinear.\nThe central goal of this dataset is to illustrate how different dimension-reduction methods reveal complementary aspects of the underlying patient landscape. Digital trials frequently involve high-frequency sensor streams, multichannel biomarkers and continuous response measures. Even when the number of features is modest, the relationships among them tend to bend, saturate, cluster locally or interact in complex ways. This makes the dataset particularly well suited for exploring methods such as t-SNE, UMAP, Self-Organizing Maps (SOMs) and Locally Linear Embedding (LLE), which are designed precisely for uncovering hidden geometry in such contexts.\nUnderstanding the variables\nDespite its simplicity, the dataset captures three major pillars typically monitored in digital clinical trials:\nInflammatory biomarkers (cytokine_1–cytokine_4)\nThese four measurements represent circulating cytokines commonly tracked to quantify systemic inflammatory activity and treatment effects. They reflect gradual biological shifts, intermittent rises in inflammatory tone, and coordinated behaviour across related pathways. Because cytokine responses rarely change in a straight line, these variables naturally carry curved or folded patterns—ideal for methods that focus on local biological neighbourhoods.\nWearable-sensor metrics (wearable_1–wearable_4)\nThese features summarise patient-level signals extracted from continuous monitoring devices such as wrist-worn sensors or patches. They capture fluctuations in rest–activity cycles, movement variability, autonomic signatures and broad physiological rhythms. Wearable data are known for their smooth transitions and subtle drifts, often producing clusters or arcs in reduced-dimensional projections.\nClinical assessments (clinical_1–clinical_3)\nThese variables represent conventional clinical endpoints collected during follow-up visits—composite severity scores, symptom indices, or laboratory-based measures. They anchor the digital biomarkers in clinically interpretable space and often evolve along slower trajectories that intersect with cytokine and wearable patterns.\nTreatment response (delta_score)\nThe endpoint of interest, delta_score, reflects the change in each patient’s outcome over time. Although numerical, it encodes a combination of biological, behavioural and clinical processes, making linear separability unlikely and nonlinear shifts common. We also define a binary responder label to distinguish patients who achieved clinically meaningful improvement from those who did not.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "high_dims.html#t-sne-recovering-local-biological-neighbourhoods-in-the-digital-clinical-trial-dataset",
    "href": "high_dims.html#t-sne-recovering-local-biological-neighbourhoods-in-the-digital-clinical-trial-dataset",
    "title": "7  High Dimension Data Strategies",
    "section": "7.9 t-SNE: Recovering Local Biological Neighbourhoods in the Digital Clinical Trial Dataset",
    "text": "7.9 t-SNE: Recovering Local Biological Neighbourhoods in the Digital Clinical Trial Dataset\nIn the second part of the chapter we work with a digital clinical-trial dataset containing multimodal information from several hundred patients—cytokine biomarkers, wearable-sensor patterns and clinical measurements. As in real advanced-therapeutic trials, these features interact in nonlinear ways: inflammatory markers rise along curved trajectories, wearable rhythms drift or bend, and clinical patterns cluster locally rather than aligning along straight axes. This makes the dataset a natural candidate for nonlinear dimension-reduction.\nt-distributed Stochastic Neighbour Embedding (t-SNE) is designed precisely for this type of structure. Instead of searching for global linear directions, as PCA does, t-SNE focuses entirely on preserving local neighbourhoods. The algorithm assumes that what matters biologically is who each patient is similar to, not the overall orientation of the feature space. Patients with comparable cytokine profiles or wearable rhythms should remain close together even if the global geometry is curved or distorted.\nTo achieve this, t-SNE converts pairwise distances into similarity probabilities. Close patients receive high probabilities; distant ones receive values near zero. Each patient uses its own local bandwidth, controlled by the perplexity, which acts as the effective neighbourhood size. For datasets of this scale, perplexity values around 30–50 typically balance fine detail and broader structure.\nWith these similarity relationships defined, t-SNE searches for a two-dimensional layout that reproduces them. Points start in random positions and are iteratively moved so that neighbours in the original space remain neighbours in the embedding. A Student’s t distribution (with heavy tails) is used in 2-D, which forces dissimilar patients apart and prevents the crowding that occurs when collapsing high-dimensional data. The optimisation continues until the mismatch between high- and low-dimensional similarities (quantified by the KL divergence) is minimised.\nThe resulting map preserves local biological structure extremely well. Patients with similar inflammatory and wearable signatures cluster together, and smooth physiological transitions often appear as arcs or curved pathways. However, global distances should not be over-interpreted; the space is faithful locally, not globally.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "high_dims.html#implementing-t-sne",
    "href": "high_dims.html#implementing-t-sne",
    "title": "7  High Dimension Data Strategies",
    "section": "7.10 Implementing t-SNE",
    "text": "7.10 Implementing t-SNE\nTo explore nonlinear structure in the digital clinical-trial dataset, we apply t-distributed Stochastic Neighbour Embedding (t-SNE) to the full set of numeric cytokine, wearable, and clinical measurements. Whereas PCA searches for straight, linear axes summarising the data, t-SNE reconstructs the local neighbourhoods of the original high-dimensional space. Patients who are similar across dozens of biological and digital features remain close together in the two-dimensional embedding, while patients with divergent profiles are pushed apart.\nThe workflow begins by loading the dataset and selecting only numeric variables. Identifier columns and the clinical outcome (delta_score) are removed to ensure that the embedding is driven solely by observed biological and digital signals rather than labels or outcomes. The resulting feature matrix is scaled and passed to Rtsne(), using a perplexity of 40—appropriate for a cohort of a few hundred patients, and effective at balancing fine-grained detail with broader cohort structure.\nAfter t-SNE converges, we merge the two-dimensional coordinates with key variables such as the clinical response and a binary responder label. This allows us to examine how patterns in response, inflammation, or wearable behaviour map onto the learned manifold. The first plot colours each patient by delta_score, revealing the nonlinear organisation of states across the cohort.\nThe full implementation is shown below:\n\nlibrary(dplyr)\nlibrary(Rtsne)\nlibrary(ggplot2)\n\n# ------------------------------------------------------------\n# 1. Load the pedagogical digital trial dataset\n# ------------------------------------------------------------\n df &lt;- readRDS(\"~/att_ai_ml/DigitalTrial_ManifoldPedagogy.rds\")\n# assuming df is already in memory with the columns you showed\n\ncolnames(df)\n\n [1] \"patient_id\"  \"cytokine_1\"  \"cytokine_2\"  \"cytokine_3\"  \"cytokine_4\" \n [6] \"wearable_1\"  \"wearable_2\"  \"wearable_3\"  \"wearable_4\"  \"clinical_1\" \n[11] \"clinical_2\"  \"clinical_3\"  \"delta_score\" \"responder\"  \n\n# ------------------------------------------------------------\n# 2. Build the feature matrix for t-SNE\n#    - keep only numeric variables used as features\n#    - remove identifiers and outcome / labels\n# ------------------------------------------------------------\nnumeric_cols &lt;- names(df)[sapply(df, is.numeric)]\n\ncols_to_remove &lt;- c(\"patient_id\", \"delta_score\") \n\nnumeric_keep &lt;- setdiff(numeric_cols, cols_to_remove)\n\n\nX_tsne &lt;- df[, numeric_keep] |&gt;\n  scale() |&gt;\n  as.matrix()\n\ndim(X_tsne)   # should be 500 × (number of numeric predictors kept)\n\n[1] 500  11\n\n# ------------------------------------------------------------\n# 3. Run t-SNE\n# ------------------------------------------------------------\nset.seed(2026)\n\ntsne_fit &lt;- Rtsne(\n  X_tsne,\n  dims       = 2,\n  perplexity = 40,\n  theta      = 0.5,\n  max_iter   = 1500,\n  verbose    = TRUE\n)\n\nPerforming PCA\nRead the 500 x 11 data matrix successfully!\nUsing no_dims = 2, perplexity = 40.000000, and theta = 0.500000\nComputing input similarities...\nBuilding tree...\nDone in 0.02 seconds (sparsity = 0.294688)!\nLearning embedding...\nIteration 50: error is 59.427653 (50 iterations in 0.02 seconds)\nIteration 100: error is 54.915475 (50 iterations in 0.02 seconds)\nIteration 150: error is 54.720570 (50 iterations in 0.02 seconds)\nIteration 200: error is 54.767592 (50 iterations in 0.02 seconds)\nIteration 250: error is 54.765601 (50 iterations in 0.01 seconds)\nIteration 300: error is 0.426183 (50 iterations in 0.02 seconds)\nIteration 350: error is 0.387879 (50 iterations in 0.01 seconds)\nIteration 400: error is 0.375316 (50 iterations in 0.01 seconds)\nIteration 450: error is 0.370638 (50 iterations in 0.01 seconds)\nIteration 500: error is 0.369975 (50 iterations in 0.01 seconds)\nIteration 550: error is 0.368768 (50 iterations in 0.01 seconds)\nIteration 600: error is 0.367549 (50 iterations in 0.01 seconds)\nIteration 650: error is 0.367283 (50 iterations in 0.01 seconds)\nIteration 700: error is 0.366735 (50 iterations in 0.01 seconds)\nIteration 750: error is 0.367664 (50 iterations in 0.01 seconds)\nIteration 800: error is 0.366811 (50 iterations in 0.01 seconds)\nIteration 850: error is 0.366994 (50 iterations in 0.01 seconds)\nIteration 900: error is 0.366301 (50 iterations in 0.01 seconds)\nIteration 950: error is 0.366323 (50 iterations in 0.01 seconds)\nIteration 1000: error is 0.365508 (50 iterations in 0.01 seconds)\nIteration 1050: error is 0.365973 (50 iterations in 0.01 seconds)\nIteration 1100: error is 0.366026 (50 iterations in 0.01 seconds)\nIteration 1150: error is 0.365795 (50 iterations in 0.01 seconds)\nIteration 1200: error is 0.365993 (50 iterations in 0.01 seconds)\nIteration 1250: error is 0.366089 (50 iterations in 0.01 seconds)\nIteration 1300: error is 0.365688 (50 iterations in 0.01 seconds)\nIteration 1350: error is 0.365264 (50 iterations in 0.01 seconds)\nIteration 1400: error is 0.365237 (50 iterations in 0.01 seconds)\nIteration 1450: error is 0.364811 (50 iterations in 0.01 seconds)\nIteration 1500: error is 0.365480 (50 iterations in 0.01 seconds)\nFitting performed in 0.42 seconds.\n\n# ------------------------------------------------------------\n# 4. Merge embedding with outcome / labels\n# ------------------------------------------------------------\ntsne_df &lt;- data.frame(\n  tSNE1       = tsne_fit$Y[, 1],\n  tSNE2       = tsne_fit$Y[, 2],\n  delta_score = df$delta_score,\n  responder   = df$responder\n)\n\n# ------------------------------------------------------------\n# 5. Plot t-SNE embedding colored by response\n# ------------------------------------------------------------\nggplot(tsne_df, aes(tSNE1, tSNE2, colour = delta_score)) +\n  geom_point(size = 1.5, alpha = 0.8) +\n  scale_colour_viridis_c() +\n  theme_minimal() +\n  labs(\n    title = \"t-SNE embedding of digital clinical trial features\",\n    colour = \"ΔScore\"\n  )\n\n\n\n\n\n\n\n\n\n7.10.1 Interpreting the t-SNE embedding\nThe t-SNE map offers a compact, two-dimensional view of how patients relate to one another across all clinical, cytokine, behavioural, and wearable features collected in the digital trial. Instead of forming neat, linear clusters, the patients occupy several curved, irregular structures—an indication that their profiles vary along nonlinear physiological and behavioural gradients rather than along simple straight axes.\nColours represent the clinical response (ΔScore), and the smooth transitions in colour across the map highlight that treatment response changes gradually along these trajectories. In the upper region of the map, where points cluster more tightly, patients tend to share similar cytokine levels and clinical markers, producing a band of relatively high ΔScore values. In contrast, the lower region contains a more diffuse arrangement of patients with lower response, reflecting greater heterogeneity in their wearable and behavioural patterns.\nAlthough the horizontal and vertical axes have no direct interpretation in t-SNE, the spatial relationships do: patients positioned close together share highly similar multimodal profiles, while those far apart differ across several biological or digital domains. The figure therefore serves as a visual summary of how complex patient data coalesce into meaningful local neighbourhoods—highlighting where similar physiological states appear and where clinically relevant differences emerge within the cohort.\n\n\n7.10.2 Faceted t-SNE: examining individual variable gradients\nTo better understand how specific trial variables vary along the manifold, we create a faceted version of the t-SNE map. Each facet displays the same two-dimensional embedding but colours the points according to a single feature, such as a cytokine, wearable metric, or clinical variable.\nThe code below constructs the faceted plot:\n\nlibrary(dplyr)\nlibrary(tidyr)\n\n\nAttaching package: 'tidyr'\n\n\nThe following object is masked from 'package:reshape2':\n\n    smiths\n\nlibrary(ggplot2)\n\n# ------------------------------------------------------------\n# 1. Select variables to facet (update for new dataset)\n# ------------------------------------------------------------\nvars_to_facet &lt;- c(\n  \"cytokine_1\",\n  \"cytokine_3\",\n  \"wearable_1\",\n  \"wearable_3\",\n  \"clinical_1\",\n  \"clinical_3\"\n)\n\n# ------------------------------------------------------------\n# 2. Merge embedding with the selected variables\n# ------------------------------------------------------------\ntsne_df &lt;- df %&gt;%\n  dplyr::select(all_of(vars_to_facet), delta_score) %&gt;%\n  dplyr::mutate(\n    tSNE1 = tsne_fit$Y[, 1],\n    tSNE2 = tsne_fit$Y[, 2],\n    responder = ifelse(\n      delta_score &gt; median(delta_score),\n      \"High response\",\n      \"Low response\"\n    )\n  )\n\n# ------------------------------------------------------------\n# 3. Reshape for faceting\n# ------------------------------------------------------------\ntsne_long &lt;- tsne_df %&gt;%\n  pivot_longer(\n    cols = all_of(vars_to_facet),\n    names_to = \"Variable\",\n    values_to = \"Value\"\n  )\n\n# ------------------------------------------------------------\n# 4. Faceted t-SNE plot (Figure 14.7 style)\n# ------------------------------------------------------------\nggplot(tsne_long, aes(tSNE1, tSNE2, color = Value, shape = responder)) +\n  geom_point(size = 2, alpha = 0.9) +\n  facet_wrap(~ Variable, ncol = 3) +\n  scale_color_viridis_c() +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"t-SNE1 vs t-SNE2 faceted by trial variables\",\n    subtitle = \"Colour = variable value | Shape = responder type\",\n    x = \"tSNE1\",\n    y = \"tSNE2\",\n    color = \"Value\",\n    shape = \"Responder\"\n  )\n\n\n\n\n\n\n\n\nThe faceted t-SNE figure shows how individual trial variables behave across the same nonlinear patient manifold. Each panel uses identical t-SNE coordinates, but colours the points by a single feature, allowing us to visually compare molecular, wearable, and clinical patterns on a shared 2-D map. Shapes indicate responder status, helping connect biological patterns with clinical improvement.\nAcross variables, consistent structure emerges. The two clinical markers display broad, smooth gradients that follow the main arms of the manifold, suggesting that clinical severity and recovery potential change gradually along patient trajectories. The cytokine variables show more localized shifts: in some regions, cytokine values rise sharply or form pockets with distinct expression levels, hinting at immune-driven sub-phenotypes embedded within the cohort. Wearable features, in contrast, tend to highlight behavioural and physiological rhythms, often producing bands or compact regions where activity or sensor-derived signals deviate from the rest of the manifold.\nResponder shapes overlay this structure and reveal where improvement concentrates. Areas with a higher proportion of high-response patients align with zones where clinical and cytokine gradients shift favorably, whereas regions dominated by low-response patients correspond to less favourable molecular or wearable patterns. Together, these faceted panels show how diverse data modalities vary across the same patient manifold and help clarify which biological or behavioural signatures are most closely linked to treatment response.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "high_dims.html#uniform-manifold-approximation-and-projection-umap",
    "href": "high_dims.html#uniform-manifold-approximation-and-projection-umap",
    "title": "7  High Dimension Data Strategies",
    "section": "7.11 Uniform Manifold Approximation and Projection (UMAP)",
    "text": "7.11 Uniform Manifold Approximation and Projection (UMAP)\nWhere t-SNE concentrates on preserving very local neighbourhoods, UMAP approaches the problem from a geometric perspective. Instead of focusing only on pairwise similarities, UMAP attempts to learn the shape of the space in which the patients live—the manifold formed by their combined cytokine, wearable, and clinical features. Once this manifold is estimated in high dimensions, UMAP builds a lower-dimensional version that preserves both local detail and broader structural relationships.\nThis makes UMAP especially suitable for digital clinical trial data. Multimodal measurements—such as inflammatory cytokines, sensor-derived behaviour, and clinical scores—rarely follow linear directions. Instead, they tend to form curved trajectories, branching patterns, and regions with very different density, all of which UMAP captures naturally. In contrast to t-SNE, UMAP often reveals clearer large-scale organisation, produces more stable embeddings across repeated runs, and scales efficiently to datasets with many patients and features.\nUMAP constructs its representation by assuming that the high-dimensional data lie on an underlying manifold. Around each patient, it defines a “fuzzy” neighbourhood whose width adapts to local density: tight where many similar patients cluster together, broader where data are sparse. These neighbourhoods form the weighted edges of a graph that approximates the dataset’s intrinsic structure. The algorithm then searches for a two-dimensional layout whose relationships mirror this graph as closely as possible.\n\n7.11.1 Three hyperparameters control how UMAP views the data:\nSeveral hyperparameters shape the behaviour of UMAP, and understanding them helps interpret the resulting patient manifold. The first is n_neighbors, which controls how large each neighbourhood is in the high-dimensional space. Small values make UMAP focus on very local structure, highlighting sharp molecular or behavioural transitions. Larger values smooth over these fine details and reveal broader cohort-level organisation, often producing cleaner global trajectories.\nThe min_dist hyperparameter determines how tightly points may cluster in the final embedding. When min_dist is small, UMAP allows compact, sharply defined regions; when it is larger, the map spreads points more evenly across the plane. This helps balance visual clarity with the preservation of meaningful biological variation.\nThe metric argument specifies how distances are computed in the original feature space. Euclidean distance is standard and works well for mixed clinical, cytokine, and wearable features, but alternative metrics can emphasise different relationships if needed.\nA key practical advantage of UMAP is that, unlike t-SNE, it is deterministic when the random seed is fixed and can project new patients onto an existing embedding. This makes it suitable for machine-learning pipelines and for real-time monitoring scenarios in digital or decentralised clinical trials—where new patients may need to be visually integrated into an established patient-state manifold.\n\n\n7.11.2 Implementing UMAP\nThe code follows the same structure: identify the numeric features to use, build the model matrix, compute the UMAP embedding, and visualise the result.\n\nlibrary(dplyr)\nlibrary(umap)\nlibrary(ggplot2)\n\n# ------------------------------------------------------------\n# 1. Load dataset\n# ------------------------------------------------------------\ndf &lt;- readRDS(\"~/att_ai_ml/DigitalTrial_ManifoldPedagogy.rds\")\n\n# ------------------------------------------------------------\n# 2. Identify numeric variables\n# ------------------------------------------------------------\nnumeric_cols &lt;- names(df)[sapply(df, is.numeric)]\n\n# Remove identifiers and the clinical outcome\ncols_to_remove &lt;- c(\"patient_id\", \"delta_score\")\n\nnumeric_keep &lt;- setdiff(numeric_cols, cols_to_remove)\n\n# ------------------------------------------------------------\n# 3. Build UMAP feature matrix\n# ------------------------------------------------------------\nX_umap &lt;- df[, numeric_keep] %&gt;%\n  scale() %&gt;%\n  as.matrix()\n\ndim(X_umap)   # should be 500 × 11 (or similar depending on dataset)\n\n[1] 500  11\n\n# ------------------------------------------------------------\n# 4. Configure and run UMAP\n# ------------------------------------------------------------\nset.seed(2026)\n\numap_config &lt;- umap.defaults\numap_config$n_neighbors &lt;- 20      # size of the neighbourhood\numap_config$min_dist    &lt;- 0.15    # cluster tightness\numap_config$metric      &lt;- \"euclidean\"\n\numap_fit &lt;- umap(X_umap, config = umap_config)\n\n# ------------------------------------------------------------\n# 5. Merge embedding with the dataset\n# ------------------------------------------------------------\numap_df &lt;- df %&gt;%\n  mutate(\n    UMAP1 = umap_fit$layout[, 1],\n    UMAP2 = umap_fit$layout[, 2]\n  )\n\n# ------------------------------------------------------------\n# 6. Plot UMAP embedding\n# ------------------------------------------------------------\nggplot(umap_df, aes(UMAP1, UMAP2, colour = delta_score)) +\n  geom_point(size = 1.8, alpha = 0.9) +\n  scale_colour_viridis_c() +\n  theme_minimal() +\n  labs(\n    title = \"UMAP embedding of digital clinical trial features\",\n    colour = \"ΔScore\"\n  )\n\n\n\n\n\n\n\n\n\n\n7.11.3 Interpreting the UMAP embedding\nThe UMAP embedding provides a compact view of how the digital-trial features organise across the cohort. Unlike t-SNE, which tends to emphasise very local structures, UMAP balances local and global relationships. The result is a map where patients form several smoothly connected regions, each reflecting distinct combinations of cytokine, wearable, and clinical measurements.\nIn this embedding, points are coloured by ΔScore, the trial’s continuous clinical outcome. The colour gradient makes it easy to see where stronger or weaker responses occur across the manifold. In the current figure, these patterns appear as:\nSmooth arcs of increasing ΔScore, where neighbouring patients show gradually higher clinical response. These arcs correspond to coordinated changes across multiple clinical and digital variables.\nCompact pockets of lower ΔScore, typically at the lower part of the manifold. These regions group patients with more limited improvement, reflecting similar biological and behavioural profiles.\nDistinct transitions, where the colour shifts steadily from low to high values. These transitional zones often represent intermediate physiological states rather than sharp subgroup boundaries.\nBecause UMAP preserves more global geometry than t-SNE, the spacing between patient groups is informative: regions that appear close in the 2-D map correspond to patients who were genuinely similar across the high-dimensional feature set. Conversely, areas that are clearly separated reflect meaningful multi-modal differences.\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# ------------------------------------------------------------\n# 1. Select variables to facet (adapted to new dataset)\n# ------------------------------------------------------------\nvars_to_facet &lt;- c(\n  \"cytokine_1\",\n  \"cytokine_3\",\n  \"wearable_1\",\n  \"wearable_3\",\n  \"clinical_1\",\n  \"clinical_3\"\n)\n\n# ------------------------------------------------------------\n# 2. Build UMAP dataframe with responder classification\n# ------------------------------------------------------------\numap_df &lt;- df %&gt;%\n  dplyr::select(all_of(vars_to_facet), delta_score) %&gt;%\n  dplyr::mutate(\n    UMAP1 = umap_fit$layout[, 1],\n    UMAP2 = umap_fit$layout[, 2],\n    responder = ifelse(delta_score &gt; median(delta_score),\n                       \"High response\", \"Low response\")\n  )\n\n# ------------------------------------------------------------\n# 3. Reshape for faceting\n# ------------------------------------------------------------\numap_long &lt;- umap_df %&gt;%\n  pivot_longer(\n    cols      = all_of(vars_to_facet),\n    names_to  = \"Variable\",\n    values_to = \"Value\"\n  )\n\n# ------------------------------------------------------------\n# 4. Faceted UMAP figure (Figure 14.7 style)\n# ------------------------------------------------------------\nggplot(umap_long, aes(UMAP1, UMAP2, color = Value, shape = responder)) +\n  geom_point(size = 2, alpha = 0.9) +\n  facet_wrap(~ Variable, ncol = 3) +\n  scale_color_viridis_c() +\n  theme_minimal(base_size = 13) +\n  labs(\n    title = \"UMAP1 vs UMAP2 faceted by trial variables\",\n    subtitle = \"Color = variable value | Shape = responder type\",\n    x = \"UMAP1\",\n    y = \"UMAP2\",\n    color = \"Value\",\n    shape = \"Responder\"\n  )\n\n\n\n\n\n\n\n\nThe faceted UMAP view allows us to examine how individual trial variables vary across the same two-dimensional embedding. Each panel shows the identical UMAP manifold, but the points are coloured according to a specific cytokine, wearable feature, or clinical marker. This makes it possible to see whether a variable changes smoothly along the manifold, forms local peaks, or concentrates within particular patient regions.\nAcross the panels, several patterns emerge. Some clinical markers (e.g., clinical_1) show broad gradients that sweep across large areas of the embedding, suggesting they capture global physiological trends shared by multiple patient subgroups. Cytokine measurements often display more localised or curved gradients, reflecting the nonlinear biological transitions embedded in the high-dimensional space. Wearable features tend to highlight behaviourally distinct regions, with pockets of higher or lower values appearing in specific segments of the manifold.\nResponder status, represented by point shapes, overlays an additional layer of interpretation. Regions enriched for high-response patients often coincide with specific value ranges of cytokine or wearable variables, whereas low-response patients cluster in areas characterised by different physiological or behavioural patterns. This visual alignment between variable intensity and response category helps connect mechanistic factors to clinical outcomes.\n\n\n7.11.4 Comparing t-SNE and UMAP\nt-SNE and UMAP produce visually different but complementary views of the same high-dimensional patient profiles. Plotting the two embeddings side-by-side makes it easier to understand what each method preserves and how these differences matter for clinical interpretation.\nIn this dataset, t-SNE arranges patients into several compact regions connected by curved paths. This layout emphasises local neighbourhoods: patients who look similar across cytokine, wearable, and clinical measurements cluster tightly together, but the relative spacing between different clusters should not be interpreted literally. t-SNE excels at revealing fine-grained structure and subtle transitions, which appear as smooth colour gradients in ΔScore within each local group.\nUMAP, by contrast, produces an embedding with clearer global organisation. Patient regions that appear distinct under t-SNE often unfold into a broader continuum under UMAP, making large-scale gradients in ΔScore easier to interpret. Neighbourhoods remain coherent, but their arrangement reflects genuine similarities in the original multimodal space, allowing global distances to carry meaning.\nWhen compared side-by-side, the two methods highlight complementary aspects of the trial cohort:\nt-SNE reveals tight micro-structures and local continuity in patient profiles.\nUMAP reveals broader clinical pathways and how patient subpopulations relate to one another.\nViewing both embeddings together provides a richer picture of the cohort, helping identify regions of consistent improvement, areas of mixed response, and potential mechanistic subtypes that may warrant deeper investigation.\nThe code below generates the side-by-side comparison:\n\nlibrary(patchwork)\n\n# t-SNE plot\n\np_tsne &lt;- ggplot(tsne_df, aes(tSNE1, tSNE2, colour = delta_score)) +\ngeom_point(size = 1.6, alpha = 0.85) +\nscale_colour_viridis_c() +\ntheme_minimal() +\nlabs(\ntitle = \"t-SNE embedding\",\ncolour = \"ΔScore\"\n)\n\n# UMAP plot\n\np_umap &lt;- ggplot(umap_df, aes(UMAP1, UMAP2, colour = delta_score)) +\ngeom_point(size = 1.6, alpha = 0.85) +\nscale_colour_viridis_c() +\ntheme_minimal() +\nlabs(\ntitle = \"UMAP embedding\",\ncolour = \"ΔScore\"\n)\n\n# Side-by-side\n\np_tsne + p_umap +\nplot_annotation(\ntitle = \"Comparison of t-SNE and UMAP embeddings\",\nsubtitle = \"t-SNE preserves local neighbourhoods; UMAP captures both local and global structure\"\n)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>High Dimension Data Strategies</span>"
    ]
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "8  Clustering",
    "section": "",
    "text": "8.1 Motivational example\nIn modern healthcare, wearable devices continuously record physiological signals that reflect an individual’s health state far more dynamically than traditional clinical measurements. Our dataset contains observations from hundreds of individuals wearing a smartwatch that records resting heart rate, heart-rate variability, step counts, skin temperature, oxygen saturation, and sleep efficiency. These variables capture multiple dimensions of physiological regulation, including autonomic balance, behavioural activity, metabolic stress, respiratory adequacy, and restorative capacity. Although the dataset was simulated for pedagogical purposes, it mirrors the complexity found in real-world remote-monitoring programs, where clinicians frequently must interpret large amounts of sensor data without explicit labels. The relationships between these variables are often nonlinear and vary across individuals, making manual inspection insufficient for detecting underlying physiological patterns.\nClustering provides a principled way to uncover these latent structures. By grouping individuals with similar physiological profiles, clustering can reveal meaningful digital phenotypes such as “healthy active,” “high stress,” “sedentary stable,” “subclinical fever,” or “post-operative recovery” patterns. These groups may help answer practical questions that arise in digital health: Who appears to be at elevated physiological stress? Which patients may be showing early signs of infection? What subpopulations would benefit from targeted behavioural interventions, such as activity coaching or sleep optimization? Rather than relying on predefined clinical categories, clustering lets the data speak for itself, highlighting naturally occurring physiological states. Throughout this chapter, we will use this wearable dataset to illustrate how different clustering algorithms operate and how they can support decision making in remote healthcare settings.\nWe begin by exploring the raw dataset and understanding the physiological variables it contains, establishing the context in which clustering is applied. The chapter then progresses through four major clustering paradigms—k-means, hierarchical clustering, density-based methods, and model-based mixture clustering—showing how each algorithm interprets the same dataset differently. For every method, we examine how clusters are formed, how to choose key hyperparameters, how to evaluate the resulting partitions, and how to interpret the physiological meaning of the groups identified. By revisiting the same dataset across multiple techniques, the chapter emphasizes the comparative strengths and limitations of each method and illustrates how clustering contributes to real-world questions in digital health monitoring.\n# ------------------------------------------------------------\n# Basic exploration\n# ------------------------------------------------------------\nwear &lt;- readRDS(\"~/att_ai_ml/att_book/data/wearable_cluster.rds\")\n# Preview the first rows of the unscaled dataset\nhead(wear)\n\n   HR_mean HRV_sdnn steps_mean skin_temp     spo2 sleep_eff\n1 62.75530 80.89426   11861.14  33.68899 98.14856 0.9485888\n2 62.56889 94.25405   11991.89  32.24710 98.13686 0.9215097\n3 58.00586 77.88340   11827.13  32.12484 98.65956 1.1353832\n4 55.16962 77.04695   11715.49  31.81894 97.75678 0.9451659\n5 61.48551 72.70625   11917.08  32.25077 97.82389 1.0053722\n6 59.89717 67.43099   12036.52  31.93910 98.10532 0.9887708\n\n# Inspect structure of the dataset\nstr(wear)\n\n'data.frame':   800 obs. of  6 variables:\n $ HR_mean   : num  62.8 62.6 58 55.2 61.5 ...\n $ HRV_sdnn  : num  80.9 94.3 77.9 77 72.7 ...\n $ steps_mean: num  11861 11992 11827 11715 11917 ...\n $ skin_temp : num  33.7 32.2 32.1 31.8 32.3 ...\n $ spo2      : num  98.1 98.1 98.7 97.8 97.8 ...\n $ sleep_eff : num  0.949 0.922 1.135 0.945 1.005 ...",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#clustering-with-k-means",
    "href": "clustering.html#clustering-with-k-means",
    "title": "8  Clustering",
    "section": "8.2 Clustering with k-means",
    "text": "8.2 Clustering with k-means\nk-means is one of the oldest and most influential methods for discovering structure in multivariate data. Its central idea is remarkably direct: if groups exist in the data, then there should be representative “centers” around which observations naturally cluster. In the context of our wearable-health dataset, these centers correspond to typical physiological patterns—for example, a pattern representing highly active and well-regulated individuals, another capturing people experiencing sustained sympathetic stress, or a pattern consistent with early inflammatory responses. By learning these centers directly from the data, k-means helps us transform a cloud of sensor measurements into meaningful physiological phenotypes.\nk-means clustering attempts to partition the data into a fixed number of groups, denoted by k such that individuals within the same group are more similar to one another than to members of other groups. It accomplishes this by iteratively performing two steps. First, it assigns each observation to the center it is closest to in the multivariate space defined by our wearable features. Second, it recalculates each center as the average of the observations assigned to it. Repeating this assignment–update cycle gradually stabilizes the centers until the clusters stop changing significantly. When applied to our physiological measurements, this process naturally separates individuals with low resting heart rate and high activity from those with high temperature and reduced oxygen saturation or from those with extremely low activity and increased heart rate.\nSeveral strategies exist for carrying out these updates, each offering different computational and statistical trade-offs. A classic formulation alternates between recomputing centers and reassigning points until convergence, providing a straightforward and intuitive procedure. Online variants update the centers incrementally as each observation is processed, which can be useful for streaming data from wearable devices. More refined approaches examine whether reassigning individual points between clusters would reduce overall within-cluster variation, often producing cleaner and more compact partitions. Regardless of the variant, all approaches pursue the same objective: to find a configuration of k centers that best summarizes the structure present in the data.\n\n8.2.1 Lloyds algorithm\nLloyd’s algorithm is perhaps the most intuitive formulation of k-means clustering and is commonly used to introduce the method. To illustrate how it works, consider a simple two-dimensional toy dataset with three roughly separable groups. The algorithm begins by selecting a value for k and placing k temporary centroids at random positions in the feature space. Each observation is then assigned to the centroid it is closest to, and the centroids are updated to the mean of the observations assigned to them.\nThis iterative procedure can be understood geometrically through the notion of Voronoi regions. Given a set of centroids, the feature space can be partitioned into polygonal areas—one for each centroid—such that every point inside a region is closer to that region’s centroid than to any of the others. These regions form what is known as a Voronoi diagram. In the context of k-means, each region corresponds to the set of observations that would currently be assigned to a particular centroid. During the assignment step, each observation effectively “chooses” the Voronoi region it falls into. During the update step, the centroid of each region moves to the average location of the points assigned to it, which in turn reshapes the Voronoi boundaries. Iteration after iteration, the diagram adjusts as the centroids drift toward more stable positions, and convergence occurs precisely when the Voronoi regions stop changing from one iteration to the next.\nConceptually, Lloyd’s algorithm follows these steps:\n1. Choose the number of clusters \\(\\mathbf{k}\\).\n2. Randomly initialize \\(\\mathbf{k}\\) centroids in the space.\n3. Assignment step: for each observation a. compute its distance to each centroid, b. assign it to the nearest centroid.\n4. Update step: move each centroid to the mean of the observations assigned to it.\n5. Repeat the assignment and update steps until no observations change cluster or a specified iteration limit is reached.\nDuring the early iterations the centroids often move substantially because the initial configuration is arbitrary. As the Voronoi cells begin to align with the natural structure in the data, the updates become progressively smaller. Eventually, a point is no longer reassigned to a different cell, and the algorithm converges. Because the initial centroids are chosen at random, different runs may converge to different solutions; it is therefore common practice to repeat the algorithm several times and select the configuration with the lowest within-cluster variation.\nThe progression of Lloyd’s algorithm can be seen clearly in the sequence of plots in ?fig-lloyd. Because the initial centroids are placed near the center of the feature space, the first iteration produces assignments that bear little resemblance to the true structure of the data. As the algorithm alternates between reassigning points and recomputing the centroids, the centers begin to migrate rapidly toward regions of high data density. The early iterations display large corrective movements, reflecting the substantial changes in cluster membership. By the third iteration, the centroids have almost reached the natural cluster cores, and subsequent adjustments become minor. This gradual stabilization exemplifies the convergence behavior of Lloyd’s method: rapid shifts at the start, followed by increasingly smaller refinements as the clusters crystallize around their final positions.\nThe following code illustrates the behaviour of the clustering algorithm.\n\n# ============================================================\n# FULL SCRIPT: Lloyd’s Algorithm with Voronoi Visualization\n# Harder clustering example: clusters not well separated\n# ============================================================\n\nlibrary(tidyverse)\nlibrary(deldir)\nlibrary(ggplot2)\nlibrary(patchwork)\n\nset.seed(2025)\n\n# ------------------------------------------------------------\n# 1. HARDER TOY DATASET (Clusters overlap on purpose)\n# ------------------------------------------------------------\ntoy &lt;- tibble(\n  x = c(rnorm(20, 50, 6),   # cluster A\n        rnorm(20, 58, 6),   # cluster B\n        rnorm(20, 54, 6)),  # cluster C\n  y = c(rnorm(20, 55, 6),\n        rnorm(20, 50, 6),\n        rnorm(20, 60, 6)),\n  cluster_true = rep(c(\"A\",\"B\",\"C\"), each = 20)\n)\n\n# Initial centroids near each other and poorly placed\ninit_centers &lt;- tibble(\n  x = c(52, 56, 54),\n  y = c(58, 52, 48)\n)\n\n\n# ------------------------------------------------------------\n# 2. Voronoi polygon helper\n# ------------------------------------------------------------\nvoronoi_polygons &lt;- function(centers) {\n  v &lt;- deldir(centers$x, centers$y)\n  tiles &lt;- tile.list(v)\n  \n  map_df(seq_along(tiles), function(i){\n    tibble(\n      x = tiles[[i]]$x,\n      y = tiles[[i]]$y,\n      id = i\n    )\n  })\n}\n\n\n# ------------------------------------------------------------\n# 3. Lloyd iteration tracking (assignment + update)\n# ------------------------------------------------------------\nlloyd_track_voronoi &lt;- function(data, centers, iters = 5){\n  centers_list &lt;- list(centers)\n  \n  for(i in 1:iters){\n    \n    # Distances from each observation to each centroid\n    dist_mat &lt;- as.matrix(dist(rbind(data[,1:2], centers)))\n    dist_mat &lt;- dist_mat[\n      1:nrow(data),\n      (nrow(data)+1):(nrow(data)+nrow(centers))\n    ]\n    \n    # Assign observations\n    cluster &lt;- max.col(-dist_mat)\n    \n    # Update centroids\n    new_centers &lt;- map_df(1:nrow(centers), ~{\n      pts &lt;- data[cluster == .x, 1:2]\n      tibble(x = mean(pts$x), y = mean(pts$y))\n    })\n    \n    centers_list[[i+1]] &lt;- new_centers\n    centers &lt;- new_centers\n  }\n  \n  centers_list\n}\n\n\n# ------------------------------------------------------------\n# 4. Plot a single iteration (Voronoi + centroid movement)\n# ------------------------------------------------------------\nplot_lloyd_iter &lt;- function(data, centers_old, centers_new, title){\n  \n  polys &lt;- voronoi_polygons(centers_old)\n  \n  ggplot() +\n    \n    # Voronoi regions (new colors)\n    geom_polygon(\n      data = polys,\n      aes(x, y, group = id, fill = factor(id)),\n      alpha = 0.32,\n      color = \"grey30\"\n    ) +\n    \n    # Observations\n    geom_point(\n      data = data,\n      aes(x, y),\n      size = 3,\n      alpha = 0.8,\n      color = \"black\"\n    ) +\n    \n    # Old centroid = red cross\n    geom_point(\n      data = centers_old,\n      aes(x, y),\n      shape = 4,\n      size = 6,\n      stroke = 2,\n      color = \"red\"\n    ) +\n    \n    # New centroid = filled red point\n    geom_point(\n      data = centers_new,\n      aes(x, y),\n      size = 4,\n      color = \"red\"\n    ) +\n    \n    # Movement arrows\n    geom_segment(\n      data = centers_old %&gt;% mutate(\n        xend = centers_new$x,\n        yend = centers_new$y\n      ),\n      aes(x = x, y = y, xend = xend, yend = yend),\n      arrow = arrow(length = unit(0.25, \"cm\")),\n      linewidth = 0.8,\n      color = \"red\"\n    ) +\n    \n    # New Voronoi color palette\n    scale_fill_manual(values = c(\"#A6CEE3\", \"#FDBF6F\", \"#B2DF8A\")) +\n    \n    coord_equal() +\n    theme_minimal(base_size = 12) +\n    theme(legend.position = \"none\") +\n    ggtitle(title)\n}\n\n\n# ------------------------------------------------------------\n# 5. Run the algorithm for 5 iterations\n# ------------------------------------------------------------\ncenters_list &lt;- lloyd_track_voronoi(toy, init_centers, iters = 5)\n\nplots &lt;- list()\n\n# First plot: initial → iteration 1\nplots[[1]] &lt;- plot_lloyd_iter(\n  toy,\n  init_centers,\n  centers_list[[2]],\n  \"Initial centroids\"\n)\n\n# Iteration 1 → 4\nfor(i in 2:5){\n  plots[[i]] &lt;- plot_lloyd_iter(\n    toy,\n    centers_list[[i]],\n    centers_list[[i+1]],\n    paste(\"Iteration\", i-1)\n  )\n}\n\n\n# ------------------------------------------------------------\n# 6. Patchwork layout\n# ------------------------------------------------------------\nwrap_plots(plots[], ncol = 3)\n\n\n\n\n\n\n\nFigure 8.1: Loyds algorithm\n\n\n\n\n\nThe sequence of panels in Figure 8.1 illustrates how Lloyd’s algorithm gradually restructures the cluster partition as the centroids migrate toward areas of higher data density. Because the three initial centroids are placed poorly—close together and away from the true latent groups—the first Voronoi diagram (top left) assigns points to regions that bear little resemblance to the underlying structure. This produces large corrective movements in the first iteration: each centroid jumps toward the cloud of points that currently dominates its assigned region.\n\n\n8.2.2 MacQueen’s algorithm\nWhile Lloyd’s algorithm updates its cluster centers only after processing all observations in a full batch, MacQueen’s algorithm adopts a more responsive strategy. It begins in a similar way—by selecting the number of clusters kand placing kinitial centroids in the feature space—but the refinement of these centroids happens in a markedly different manner. After the first round of assignments and centroid updates, MacQueen’s method scans through the dataset one observation at a time. For each case, it recomputes the distances to all current centroids and checks whether the case should be moved to a different cluster. If a reassignment occurs, the algorithm immediately updates the centroids involved: the centroid of the old cluster is adjusted to reflect the removal of the case, and the centroid of the new cluster is updated to incorporate it. This incremental update mechanism contrasts with the batch-style adjustment in Lloyd’s algorithm and makes the method sensitive to local changes as soon as they arise.\nThe general structure of the algorithm can be expressed as:\n\nSelect \\(k\\).\n\n2. Randomly initialize \\(k\\) centers in the feature space.\n3. Assign each observation to its nearest center.\n4. Update each center to the mean of the observations currently assigned to it.\n5. For each observation, sequentially:\na. Compute distances to all current centroids.\nb. Reassign the case if another centroid is now closer. c. If reassigned, immediately update both the old and new centroids.\n6. After all observations have been processed, update centroids again if needed. 7. If no assignments have changed, stop; otherwise repeat step 5.\nBecause each update uses a single observation at a time, MacQueen’s method is often referred to as online k-means. This design makes the algorithm particularly well suited to streaming or continuously collected data, where new measurements arrive gradually rather than in large batches. In such settings, incremental updates can adapt the cluster structur ↓ ster than Lloyd’s more global adjustments.\nIn practice, MacQueen’s algorithm may converge in fewer global iterations than Lloyd’s method, since the centroids are continuously “nudged” toward their final positions. However, this comes at the cost of more computations within each iteration, and the algorithm may exhibit more variability early on due to its sensitivity to the order of the observations.\n\n\n8.2.3 Hartigan–Wong algorithm\nAmong the classical variants of k-means, the Hartigan-Wong algorithm is the one that most aggressively searches for an arrangement of clusters that minimizes the overall within-cluster variance. Like the previous methods, it begins with \\(k\\) randomly chosen centroids and assigns each observation to whichever center is initially closest. What distinguishes this algorithm is what happens next: instead of simply recomputing the centroids from these assignments, it examines every observation one by one and evaluates the effect of hypothetically moving that observation into each of the other clusters.\nThis evaluation uses the within-cluster sum of squared deviations, a quantity that measures how tightly a cluster is grouped around its centroid. For a given cluster \\(k\\), this quantity is\n\\[\nS S_k=\\sum_{i \\in k}\\left(x_i-c_k\\right)^2,\n\\]\nwhere \\(x_i\\) is an observation in cluster \\(k\\), and \\(c_k\\) is the centroid of that cluster. During the algorithm’s search, the contribution of the observation currently under consideration is subtracted from the sum of squares of its present cluster and added to the sum of squares of each candidate cluster. If reassigning that observation reduces the total error, the move is accepted-even if the new centroid is not the closest in Euclidean distance. After a move, both affected centroids are immediately updated to reflect their new memberships, and the algorithm proceeds to evaluate the next observation.\nThis iterative “error reduction” strategy continues until a full pass through the dataset produces no reassignments. Because it evaluates more possibilities than Lloyd’s or MacQueen’s methods, Hartigan–Wong often discovers cluster configurations with smaller within-cluster variance. In applied settings—such as physiological signals, where clusters may subtly differ in heart rate variability, skin temperature, or activity patterns—this approach can yield cleaner boundaries and more coherent physiological groups. The cost is computational: evaluating the effect of all possible moves is more expensive, particularly for large datasets. Nonetheless, for data of moderate size, the additional precision often makes the method an appealing default.\nA concise outline of the algorithm is:\n\nChoose \\(k\\).\nInitialize \\(k\\) centers at random.\nAssign each observation to the nearest center.\nRecompute all centroids.\nFor each observation:\n\n\n\nCompute how the sum of squared error would change if the observation were removed from its current cluster.\nCompute how the sum of squared error would change if it were added to each of the other clusters.\nMove the observation to the cluster that yields the largest decrease (or smallest increase) in total error.\nUpdate both affected centroids.\n\n\n\nRepeat step 5 until a complete pass through the data produces no changes.\n\n\n\n8.2.4 Chosing algorithms for clustering\nAll three algorithms optimize the same objective function, but they reach solutions in different ways. Lloyd’s method is simple and fast, MacQueen’s incremental updates can be useful for streaming contexts, and Hartigan–Wong tends to produce the most compact clusters. In practice, one can treat the algorithm choice as a discrete hyperparameter and evaluate which variant provides the most interpretable or stable results for the dataset at hand. Later in this chapter, we will compare these options empirically using the wearable dataset.\n\n\n8.2.5 Defining a clustering workflow\nBefore fitting any k-means model, it is important to set up a clear workflow for how the clustering analysis will proceed. Unlike supervised learning, clustering problems do not include a target variable to predict. All that the algorithm receives is a matrix of features—in our case, the scaled physiological measurements collected from wearable devices. The goal is to let the algorithm uncover the underlying structure of the data without external guidance.\nA typical clustering workflow involves four steps:\n\nPrepare the feature matrix\n\nWe select the variables of interest and standardize them so that all measurements-heart rate, steps, temperature, oxygen saturation, sleep efficiency-contribute equally to the distance calculations.\n2. Choose a value for \\(k\\)\nBecause k-means requires specifying the number of clusters ahead of time, we evaluate several values of \\(k\\) and compare their quality using internal metrics such as the Davies-Bouldin index, Dunn index, and pseudo-F statistic. These indices quantify how compact and well separated the resulting clusters are.\n3. Fit the k-means algorithm\nOnce a candidate value of \\(k\\) has been selected, we run the k -means algorithm using different initialization strategies and algorithmic variants (Lloyd, MacQueen, or Hartigan-Wong). Each variant follows the same conceptual objective but differs in how centroids are updated at each iteration.\n4. Validate the resulting clusters\nBecause we do not have ground-truth labels, model evaluation relies on visual inspection and internal diagnostics. Scatterplots, pairwise density views, and silhouette-like measures help determine whether the clusters represent meaningful physiological groupings.\n\n\n8.2.6 Preparing data for k-means\nBefore applying any clustering method, it is essential to consider how the numerical scale of each variable influences the algorithm. In our wearable dataset, heart rate is measured in beats per minute, step counts reach the thousands, skin temperature varies by only a few degrees, and sleep efficiency ranges between zero and one. Because k-means relies on Euclidean distance, variables with large numerical ranges dominate the distance computation; as a result, the algorithm may cluster individuals almost entirely based on step counts while virtually ignoring temperature or oxygen saturation. To prevent this distortion and ensure that all physiological measurements contribute equally, we standardize each variable to have mean zero and unit variance. This step does not change the structure of the dataset—it simply places all variables on a comparable footing.\n\n# Scale all physiological variables\nwear_scaled &lt;- wear %&gt;%\n  dplyr::mutate(across(HR_mean:sleep_eff, scale))\n\n# Inspect first rows\nhead(wear_scaled)\n\n     HR_mean  HRV_sdnn steps_mean  skin_temp     spo2 sleep_eff\n1 -0.7836764 1.5432604   1.771569  0.3185353 1.382956 0.6921512\n2 -0.7921270 2.2830676   1.804396 -0.9604974 1.370759 0.5211669\n3 -0.9989816 1.3765321   1.763030 -1.0689523 1.915855 1.8716154\n4 -1.1275560 1.3302136   1.735003 -1.3403055 0.974393 0.6705376\n5 -0.8412392 1.0898444   1.785613 -0.9572478 1.044375 1.0506947\n6 -0.9132430 0.7977232   1.815601 -1.2337157 1.337863 0.9458695\n\n\nBefore introducing k-means, it is good practice to look at the raw data without any color coding or cluster labels. This allows us to build intuition about the geometry of the dataset and appreciate what the algorithm will try to uncover. If groups exist, we might already see hints of separation by projecting the data onto two dimensions. Heart rate and step count are a natural first choice, as they capture two major behavioural–physiological axes: autonomic state and activity level.\n\nlibrary(ggplot2)\n\nggplot(wear_scaled, aes(HR_mean, steps_mean)) +\n  geom_point(alpha = 0.5) +\n  theme_bw() +\n  labs(\n    title = \"Wearable physiological data (scaled)\",\n    subtitle = \"Initial visualization without cluster labels\",\n    x = \"Scaled resting heart rate\",\n    y = \"Scaled daily steps\"\n  )\n\n\n\n\n\n\n\n\nAlthough no cluster assignments have been provided, the scatterplot already suggests that the individuals do not form a homogeneous cloud. Instead, regions of higher density and subtle geometric shapes appear, hinting that certain physiological profiles may recur across the population. This visual cue is not a formal clustering result, but it encourages us to proceed: if structure is present, a clustering method such as k-means should be able to articulate it.\nWe can also use the pairs plot to explore the dataset and detect some useful pattenrs.\n\npairs(wear_scaled) \n\n\n\n\n\n\n\n\nThis view often reveals early hints of clusters: individuals with very low steps and high heart rate, for example, form dense subregions distinct from those with high HRV and high steps.\n\n\n8.2.7 Choosing the number of clusters\nSelecting the number of clusters is one of the central modelling decisions in k-means, and the bias–variance intuition from supervised learning provides a helpful guide. When k is too small, the algorithm underfits: distinct physiological states collapse into a coarse, overly broad grouping (high bias, low flexibility). When k is too large, the solution overfits, fragmenting a genuine state into many small, unstable micro-clusters driven by noise (high variance).\nIn practice, neither extreme is desirable. Underfitting hides meaningful structure—such as differences between high-stress, febrile, or well-regulated physiological signatures—while overfitting invents structure that does not generalize. The goal is to choose a value of k that captures stable, interpretable patterns without injecting artificial complexity.\nBecause clustering has no labels, we rely on two complementary tools:\n- Internal validation metrics (e.g., Davies-Bouldin, Dunn, pseudo- \\(F\\) ) to quantify compactness and separation.\n- Visual diagnostics (scatterplots, PCA/UMAP projections, silhouette profiles) to judge whether the clusters align with domain intuition.\nTogether, these help navigate the same trade-off seen in supervised learning: enough flexibility to reveal the underlying structure, but not so much that the algorithm begins to model noise.\nThe figure below illustrates the idea of overfitting in clustering using a simple 2-D toy example.\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(RColorBrewer)\n\nset.seed(2025)\n\n# ------------------------------------------------------------\n# Helper: generate three natural clusters\n# ------------------------------------------------------------\nmake_true_clusters &lt;- function(n = 20, sep = 2){\n  tibble(\n    x = c(rnorm(n, -sep, 0.6),\n          rnorm(n,  sep, 0.6),\n          rnorm(n,  0,   0.6)),\n    y = c(rnorm(n,  sep, 0.6),\n          rnorm(n,  sep, 0.6),\n          rnorm(n, -sep, 0.6)),\n    true = rep(c(\"C1\",\"C2\",\"C3\"), each = n)\n  )\n}\n\ndata_true &lt;- make_true_clusters()\n\n# consistent color palette\npal3 &lt;- c(\"C1\" = \"#4B0082\", \"C2\" = \"#74C69D\", \"C3\" = \"#F9C74F\")\npal6 &lt;- brewer.pal(6, \"Set2\")\nnames(pal6) &lt;- paste0(\"C\", 1:6)\n\n# ------------------------------------------------------------\n# Underfit: force only 2 clusters\n# ------------------------------------------------------------\ndata_under &lt;- data_true %&gt;%\n  mutate(cluster = ifelse(x &lt; 0, \"C1\", \"C2\"))\n\nplot_under &lt;- ggplot(data_under, aes(x, y, color = cluster)) +\n  geom_point(size = 3) +\n  scale_color_manual(values = pal3[c(\"C1\",\"C2\")]) +\n  coord_equal() +\n  theme_bw(base_size = 11) +\n  theme(\n    panel.border = element_rect(color = \"black\", fill = NA, linewidth = 0.8),\n    legend.position = \"none\"\n  ) +\n  ggtitle(\"Underfit\")\n\n# ------------------------------------------------------------\n# Optimal: true 3 clusters\n# ------------------------------------------------------------\ndata_optimal &lt;- data_true %&gt;%\n  mutate(cluster = true)\n\nplot_optimal &lt;- ggplot(data_optimal, aes(x, y, color = cluster)) +\n  geom_point(size = 3) +\n  scale_color_manual(values = pal3) +\n  coord_equal() +\n  theme_bw(base_size = 11) +\n  theme(\n    panel.border = element_rect(color = \"black\", fill = NA, linewidth = 0.8),\n    legend.position = \"none\"\n  ) +\n  ggtitle(\"Optimal\")\n\n# ------------------------------------------------------------\n# Overfit: force 6 clusters\n# ------------------------------------------------------------\nset.seed(2025)\nkm &lt;- kmeans(data_true[,1:2], centers = 6, nstart = 20)\n\ndata_over &lt;- data_true %&gt;%\n  mutate(cluster = paste0(\"C\", km$cluster))\n\nplot_over &lt;- ggplot(data_over, aes(x, y, color = cluster)) +\n  geom_point(size = 3) +\n  scale_color_manual(values = pal6) +\n  coord_equal() +\n  theme_bw(base_size = 11) +\n  theme(\n    panel.border = element_rect(color = \"black\", fill = NA, linewidth = 0.8),\n    legend.position = \"none\"\n  ) +\n  ggtitle(\"Overfit\")\n\n# ------------------------------------------------------------\n# Combine panels\n# ------------------------------------------------------------\nplot_under | plot_optimal | plot_over\n\n\n\n\n\n\n\nFigure 8.2: Illustration of underfitting, optimal clustering, and overfitting with consistent labels.\n\n\n\n\n\n\n\n8.2.8 Using the Davies Bouldin Index\nThe Davies-Bouldin (DB) index provides a quantitative way to assess how well a clustering algorithm has separated the data. It is built on two intuitive ideas:\n1. Intracluster variance (scatter):\nEach cluster should be compact - the observations inside the cluster should lie close to their centroid.\n2. Distance between centroids (separation):\nDifferent clusters should be far apart - the centroids of distinct clusters should not be too close.\nFor every cluster, the DB index compares its internal scatter with the distance to its most similar neighboring cluster. More formally, for clusters \\(i\\) and \\(j\\) :\n\\[\nR_{i j}=\\frac{S_i+S_j}{M_{i j}},\n\\]\nwhere\n- \\(S_i\\) is the average distance of points in cluster \\(i\\) to its centroid (its scatter), and\n- \\(M_{i j}\\) is the distance between the centroids of clusters \\(i\\) and \\(j\\).\nFor each cluster \\(i\\), the algorithm finds the cluster \\(j\\) that gives the largest ratio \\(R_{i j}\\).\nFor each cluster \\(i\\), the algorithm finds the cluster \\(j\\) that gives the largest ratio \\(R_{i j}\\). The Davies-Bouldin index is the average of these maxima across all clusters:\n\\[\nD B=\\frac{1}{k} \\sum_{i=1}^k \\max _{j \\neq i} R_{i j} .\n\\]\nA lower DB value indicates:\n- tighter clusters (low scatter), and\n- clearer separation between centroids.\nThe figure below illustrates the two components behind the index: intracluster variance and centroid separation.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nset.seed(2025)\n\n# ------------------------------------------------------------\n# Helper: create two clean, well-separated clusters\n# ------------------------------------------------------------\nmake_two_clusters &lt;- function(n = 12, sep = 4){\n  tibble(\n    x = c(rnorm(n, -sep, 0.7),\n          rnorm(n,  sep, 0.7)),\n    y = c(rnorm(n,  sep, 0.7),\n          rnorm(n, -sep, 0.7)),\n    cluster = rep(c(\"C1\",\"C2\"), each = n)\n  )\n}\n\ndf &lt;- make_two_clusters()\n\ncenters &lt;- df %&gt;%\n  group_by(cluster) %&gt;%\n  summarise(cx = mean(x), cy = mean(y))\n\n# consistent palette\npal &lt;- c(\"C1\" = \"#1B9E77\",   # green\n         \"C2\" = \"#0072B2\")   # blue\n\n# ------------------------------------------------------------\n# Panel 1: Intracluster variance (scatter from centroid)\n# ------------------------------------------------------------\nlines_df &lt;- df %&gt;%\n  left_join(centers, by = \"cluster\") %&gt;%\n  mutate(xend = cx, yend = cy)\n\np1 &lt;- ggplot() +\n  geom_segment(data = lines_df,\n               aes(x, y, xend = xend, yend = yend, color = cluster),\n               alpha = 0.6, linewidth = 0.6) +\n  geom_point(data = df, aes(x, y, color = cluster), size = 3) +\n  geom_point(data = centers, aes(cx, cy), size = 5, shape = 4, stroke = 1.5) +\n  scale_color_manual(values = pal) +\n  coord_equal() +\n  theme_bw(base_size = 12) +\n  theme(\n    legend.position = \"none\",\n    panel.border = element_rect(color = \"black\", fill = NA, linewidth = 0.8)\n  ) +\n  ggtitle(\"Intracluster variance\")\n\n# ------------------------------------------------------------\n# Panel 2: Distance between centroids\n# ------------------------------------------------------------\np2 &lt;- ggplot(df, aes(x, y, color = cluster)) +\n  geom_point(size = 3) +\n  geom_point(data = centers, aes(cx, cy), size = 5, shape = 4, stroke = 1.5) +\n  geom_segment(data = centers,\n               aes(x = centers$cx[1], y = centers$cy[1],\n                   xend = centers$cx[2], yend = centers$cy[2]),\n               linetype = \"dashed\", linewidth = 1) +\n  scale_color_manual(values = pal) +\n  coord_equal() +\n  theme_bw(base_size = 12) +\n  theme(\n    legend.position = \"none\",\n    panel.border = element_rect(color = \"black\", fill = NA, linewidth = 0.8)\n  ) +\n  ggtitle(\"Distance between centroids\")\n\n# ------------------------------------------------------------\n# Combine panels\n# ------------------------------------------------------------\np1 | p2\n\n\n\n\n\n\n\nFigure 8.3: Intracluster variance and centroid separation used in the Davies–Bouldin index.\n\n\n\n\n\n\n\n8.2.9 Dunn Index\nThe Dunn index is one of the oldest internal validation measures for clustering and is designed to reward solutions in which clusters are compact and well separated. Whereas the Davies-Bouldin index focuses on average behavior across clusters, the Dunn index emphasizes the worst-case geometry: the smallest separation between any two clusters, and the largest dispersion within any cluster. Because of this, the Dunn index is stricter and is maximized-not minimized-when the clustering arrangement is good.\nMathematical definition Given clusters \\(C_1, C_2, \\ldots, C_k\\), the Dunn index is defined as\n\\[\nD=\\frac{\\min _{i \\neq j} \\delta\\left(C_i, C_j\\right)}{\\max _{1 \\leq \\ell \\leq k} \\Delta\\left(C_{\\ell}\\right)},\n\\]\nwhere: - \\(\\delta\\left(C_i, C_j\\right)=\\) separation between clusters \\(i\\) and \\(j\\), usually\n\\[\n\\delta\\left(C_i, C_j\\right)=\\min _{x \\in C_i, y \\in C_j}\\|x-y\\| .\n\\]\n\n\\(\\Delta\\left(C_{\\ell}\\right)=\\) diameter (maximum internal spread) of cluster \\(\\ell\\),\n\n\\[\n\\Delta\\left(C_{\\ell}\\right)=\\max _{x, y \\in C_{\\ell}}\\|x-y\\|\n\\]\nSo the Dunn index compares:\n- numerator: the smallest distance between clusters (best-case separation),\n- denominator: the largest distance inside any cluster (worst-case compactness).\nInterpretation\n- Large Dunn index → clusters are tight and far apart\n- Small Dunn index → at least one of the following is true:\n- two clusters are very close together (poor separation), - one cluster is very spread out (poor compactness).\nBecause it uses ” \\(m \\min\\) ” in the numerator and ” \\(m a x\\) ” in the denominator, the Dunn index is highly sensitive to outliers and is generally conservative. It is nonetheless useful as a principle-based measure of cluster quality.\nThe code below builds a figure that provide an explanation of the Dunn Index.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nset.seed(2025)\n\n# ------------------------------------------------------------\n# Helper: two clusters\n# ------------------------------------------------------------\nmake_two_clusters &lt;- function(n = 12, sep = 4){\n  tibble(\n    x = c(rnorm(n, -sep, 0.7),\n          rnorm(n,  sep, 0.7)),\n    y = c(rnorm(n,  sep, 0.7),\n          rnorm(n, -sep, 0.7)),\n    cluster = rep(c(\"C1\",\"C2\"), each = n)\n  )\n}\n\ndf &lt;- make_two_clusters()\n\n# Use green + blue\npal &lt;- c(\"C1\" = \"#1B9E77\", \"C2\" = \"#0072B2\")\n\n\n# ------------------------------------------------------------\n# SAFE CROSSING STRATEGY:\n# Rename ALL columns of second df before crossing\n# ------------------------------------------------------------\n\n# For between-cluster distance\ndf_c1 &lt;- df %&gt;% filter(cluster == \"C1\")\ndf_c2 &lt;- df %&gt;%\n  filter(cluster == \"C2\") %&gt;%\n  rename(\n    x2 = x,\n    y2 = y,\n    cluster2 = cluster   # avoids duplicate name\n  )\n\n# For within-cluster distance\ndf2a &lt;- df %&gt;% filter(cluster == \"C2\")\ndf2b &lt;- df2a %&gt;%\n  rename(\n    x2 = x,\n    y2 = y,\n    cluster2 = cluster   # avoids duplicate name\n  )\n\n\n# ------------------------------------------------------------\n# Smallest distance BETWEEN clusters\n# ------------------------------------------------------------\ndist_pairs &lt;- crossing(df_c1, df_c2) %&gt;%\n  mutate(d = sqrt((x - x2)^2 + (y - y2)^2))\n\nmin_pair &lt;- dist_pairs %&gt;% slice_min(d, n = 1)\n\np1 &lt;- ggplot(df, aes(x, y, color = cluster)) +\n  geom_point(size = 3) +\n  geom_segment(data = min_pair,\n               aes(x = x, y = y, xend = x2, yend = y2),\n               linewidth = 1) +\n  scale_color_manual(values = pal) +\n  coord_equal() +\n  theme_bw(base_size = 12) +\n  theme(\n    legend.position = \"none\",\n    panel.border = element_rect(color = \"black\", fill = NA)\n  ) +\n  ggtitle(\"Smallest distance\\nbetween clusters\")\n\n\n# ------------------------------------------------------------\n# Largest distance WITHIN a cluster (C2) — SAFE VERSION\n# ------------------------------------------------------------\nwithin_d &lt;- crossing(df2a, df2b) %&gt;%\n  filter(!(x == x2 & y == y2)) %&gt;%  # remove self-pairs\n  mutate(d = sqrt((x - x2)^2 + (y - y2)^2))\n\nmax_pair &lt;- within_d %&gt;% slice_max(d, n = 1)\n\np2 &lt;- ggplot(df, aes(x, y, color = cluster)) +\n  geom_point(size = 3) +\n  geom_segment(data = max_pair,\n               aes(x = x, y = y, xend = x2, yend = y2),\n               linewidth = 1) +\n  scale_color_manual(values = pal) +\n  coord_equal() +\n  theme_bw(base_size = 12) +\n  theme(\n    legend.position = \"none\",\n    panel.border = element_rect(color = \"black\", fill = NA)\n  ) +\n  ggtitle(\"Largest distance\\nwithin a cluster\")\n\n\n# ------------------------------------------------------------\n# Final side-by-side Dunn index figure\n# ------------------------------------------------------------\np1 | p2\n\n\n\n\n\n\n\nFigure 8.4: Conceptual illustration of the Dunn index. The index measueres the ration between smallest distance between cases in different clusters and the largest within cluster distance\n\n\n\n\n\n\n\n8.2.10 Pseudo-F Statistic (Calinski-Harabasz Index)\nThe pseudo-F statistic-also known as the Calinski-Harabasz index-evaluates clustering quality by comparing two sources of variation:\n1. Between-cluster dispersion:\nHow far the cluster centroids are from the grand centroid.\n2. Within-cluster dispersion:\nHow tightly grouped the observations are around their own cluster centroid. These two components mirror the structure of an ANOVA decomposition, even though no response variable exists. The pseudo-F index formalizes this idea:\n\\[\nF^*(k)=\\frac{\\text { Between-cluster } \\mathrm{SS} /(k-1)}{\\text { Within-cluster } \\mathrm{SS} /(n-k)} .\n\\]\nWhere: - \\(k=\\) number of clusters - \\(n=\\) total number of observations - Within-cluster SS (WSS):\n\\[\nW S S=\\sum_{j=1}^k \\sum_{i \\in C_j}\\left\\|x_i-c_j\\right\\|^2\n\\]\n\nBetween-cluster SS (BSS):\n\n\\[\nB S S=\\sum_{j=1}^k n_j\\left\\|c_j-c_{\\text {grand }}\\right\\|^2\n\\]\nwith \\(c_j\\) the centroid of cluster \\(j\\), and \\(c_{\\text {grand }}\\) the centroid of the full dataset. Intuition\n\nGood clustering has:\n\n\n\nsmall WSS (clusters are compact)\n\n\n\nlarge BSS (clusters are far from each other)\n\nThe pseudo-F increases when:\n\ncentroids are well separated\nobservations are tight around each centroid\n\nThus, higher values indicate clearer, more meaningful clusters.\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\nset.seed(2025)\n\n# ------------------------------------------------------------\n# Helper: make two clusters\n# ------------------------------------------------------------\nmake_two_clusters &lt;- function(n = 12, sep = 4){\n  tibble(\n    x = c(rnorm(n, -sep, 0.7),\n          rnorm(n,  sep, 0.7)),\n    y = c(rnorm(n,  sep, 0.7),\n          rnorm(n, -sep, 0.7)),\n    cluster = rep(c(\"C1\",\"C2\"), each = n)\n  )\n}\n\ndf &lt;- make_two_clusters()\n\npal &lt;- c(\"C1\" = \"#1B9E77\", \"C2\" = \"#0072B2\")   # green and blue\n\n\n# ------------------------------------------------------------\n# Compute centroids and grand centroid\n# ------------------------------------------------------------\ncentroids &lt;- df %&gt;%\n  group_by(cluster) %&gt;%\n  summarise(cx = mean(x), cy = mean(y))\n\ngrand_centroid &lt;- tibble(\n  gx = mean(df$x),\n  gy = mean(df$y)\n)\n\n\n# ============================================================\n# Panel 1: Within-cluster sum of squares\n# ============================================================\n\n# create segments from each point to its centroid\nwithin_segments &lt;- df %&gt;%\n  left_join(centroids, by = \"cluster\") %&gt;%\n  mutate(xend = cx, yend = cy)\n\np_within &lt;- ggplot() +\n\n  geom_point(data = df, aes(x, y, color = cluster), size = 3) +\n  geom_segment(\n    data = within_segments,\n    aes(x = x, y = y, xend = xend, yend = yend, color = cluster),\n    linewidth = 0.7\n  ) +\n\n  scale_color_manual(values = pal) +\n  coord_equal() +\n  theme_bw(base_size = 12) +\n  theme(\n    legend.position = \"none\",\n    panel.border = element_rect(color = \"black\", fill = NA)\n  ) +\n  ggtitle(\"Within-cluster\\nsum of squares\")\n\n\n# ============================================================\n# Panel 2: Between-cluster sum of squares\n# ============================================================\n\n# create segments from each cluster centroid to the grand centroid\nbetween_segments &lt;- centroids %&gt;%\n  mutate(xend = grand_centroid$gx, yend = grand_centroid$gy)\n\np_between &lt;- ggplot() +\n\n  geom_point(data = df, aes(x, y, color = cluster), size = 3) +\n\n  # cluster centroids\n  geom_point(data = centroids,\n             aes(cx, cy, fill = cluster),\n             color = \"black\",\n             shape = 21, size = 5) +\n\n  # grand centroid (square)\n  geom_point(data = grand_centroid,\n             aes(gx, gy),\n             shape = 22, size = 5, fill = \"grey20\") +\n\n  # lines from centroids to grand centroid\n  geom_segment(\n    data = between_segments,\n    aes(x = cx, y = cy, xend = xend, yend = yend, color = cluster),\n    linewidth = 1\n  ) +\n\n  scale_color_manual(values = pal) +\n  scale_fill_manual(values = pal) +\n  coord_equal() +\n  theme_bw(base_size = 12) +\n  theme(\n    legend.position = \"none\",\n    panel.border = element_rect(color = \"black\", fill = NA)\n  ) +\n  ggtitle(\"Between-cluster\\nsum of squares\")\n\n\n# ------------------------------------------------------------\n# Combine panels\n# ------------------------------------------------------------\np_within | p_between\n\n\n\n\n\n\n\nFigure 8.5: Conceptual illustration of the pseudo-F statistic showing within-cluster and between-cluster sums of squares.\n\n\n\n\n\n\n\n8.2.11 Tunning k\nSelecting the number of clusters ( \\(k\\) ) is one of the most consequential decisions in k-means analysis. Unfortunately, there is no single internal metric that universally identifies the “right” clustering structure. Different indices focus on different aspects of separation: compactness within clusters, distances between clusters, or variance decomposition. These metrics tend to agree when the data contain strong, well-separated structure, but they diverge once the solution becomes ambiguous. For example, metrics based on sums of squares often prefer solutions in which all clusters have similar diameters, even when the underlying structure is naturally unbalanced.\nFor this reason, it is good practice to evaluate multiple internal cluster metrics and treat them as complementary pieces of evidence. This helps guard against misleading conclusions driven by the quirks of any single index.\n\n\n8.2.12 Avoiding overclustering\nEven when using internal metrics, it is still easy to overfit by selecting too many clusters. One way to assess stability is through bootstrapping: repeatedly sampling individuals with replacement, reclustering each bootstrap sample, and quantifying how often the same individuals are assigned to the same cluster. If the clustering is stable across bootstrap replicates, the solution is more likely to reflect real structure rather than noise.\nAnother practical approach—especially for algorithms that can assign cluster labels to new data—is to use a cross-validation–like strategy. We split the dataset into folds, fit k-means on the training portion, assign test-set individuals to the nearest learned centroid, and then compute internal metrics on the predicted clusters. This provides a measure of how well the clustering generalizes to unseen data. It also simultaneously evaluates stability (does the clustering generalize?) and separation quality (how do the internal metrics score?).\nThis is the approach we use in this chapter to tune both k (the number of clusters) and the k-means algorithm variant (Hartigan–Wong, Lloyd, or MacQueen). Here we build the full tuning workflow directly in base R plus tidyverse tools.\nHyperparameter Search Setup We begin by defining a simple grid over the hyperparameters: - centers: the number of clusters ( \\(k\\) ), from 3 to 8 - algorithm: one of “Hartigan-Wong”, “Lloyd”, or “MacQueen”\nFor each combination of values, we run a 10-fold cross-validation procedure. In each fold, we: 1. Fit the model on the training fold. 2. Assign test-set individuals to their nearest centroid. 3. Compute the three internal metrics on the test set: - Davies-Bouldin (DB) - lower is better - Dunn index - higher is better - Pseudo-F (G1) - higher is better\nWe also record execution time for each combination to compare computational cost. The complete tuning scrint is shown below.\n\n# ===============================================================\n# K-means hyperparameter tuning: centers (k) × algorithm\n# Cross-validation-like evaluation using internal cluster metrics\n# ===============================================================\n\nlibrary(tidyverse)\nlibrary(clusterSim)     # DB, pseudo-F (G1)\nlibrary(clusterCrit)    # Dunn index\nlibrary(patchwork)\n\nset.seed(2025)\n\n\n# Feature matrix only\nX &lt;- wear_scaled %&gt;% dplyr::select(HR_mean:sleep_eff)\nX_mat &lt;- as.matrix(X)\n\n# ---------------------------------------------------------------\n# Hyperparameter grid\n# ---------------------------------------------------------------\nk_values &lt;- 3:8\nalgorithms &lt;- c(\"Hartigan-Wong\", \"Lloyd\", \"MacQueen\")\n\ngrid &lt;- expand_grid(\n  centers   = k_values,\n  algorithm = algorithms\n)\n\n# ---------------------------------------------------------------\n# Cross-validation folds\n# ---------------------------------------------------------------\nK &lt;- 10\nn &lt;- nrow(X_mat)\nfold_id &lt;- sample(rep(1:K, length.out = n))\n\n\n# ---------------------------------------------------------------\n# Function to compute internal metrics on a test fold\n# ---------------------------------------------------------------\nevaluate_fold &lt;- function(X_train, X_test, centers, algorithm){\n\n  t0 &lt;- Sys.time()\n\n  km &lt;- kmeans(\n    X_train,\n    centers  = centers,\n    algorithm = algorithm,\n    iter.max = 100,\n    nstart = 10\n  )\n\n  # Assign test to nearest centroid\n  dist_test &lt;- as.matrix(dist(rbind(km$centers, X_test)))\n  D &lt;- dist_test[(1:centers), (centers + 1):(centers + nrow(X_test))]\n  pred &lt;- apply(D, 2, which.min) |&gt; as.integer()\n\n  # Internal metrics\n  db_val   &lt;- index.DB(X_test, pred)$DB\n  dunn_val &lt;- intCriteria(X_test, pred, c(\"Dunn\"))$dunn\n  f_val    &lt;- index.G1(X_test, pred)\n\n  exec     &lt;- as.numeric(Sys.time() - t0, units = \"secs\")\n\n  tibble(\n    db   = db_val,\n    dunn = dunn_val,\n    G1   = f_val,\n    exec = exec\n  )\n}\n\n\n# ---------------------------------------------------------------\n# FIXED: Predefine results tibble with correct columns\n# ---------------------------------------------------------------\nresults &lt;- tibble(\n  centers = numeric(),\n  algorithm = character(),\n  db.test.mean = numeric(),\n  dunn.test.mean = numeric(),\n  G1.test.mean = numeric(),\n  exec.time = numeric()\n)\n\n\n# ---------------------------------------------------------------\n# Run tuning: grid × CV folds\n# ---------------------------------------------------------------\nfor(i in 1:nrow(grid)){\n\n  k  &lt;- grid$centers[i]\n  alg &lt;- grid$algorithm[i]\n\n  metric_db   &lt;- c()\n  metric_dunn &lt;- c()\n  metric_f    &lt;- c()\n  times       &lt;- c()\n\n  for(f in 1:K){\n\n    X_train &lt;- X_mat[fold_id != f, ]\n    X_test  &lt;- X_mat[fold_id == f, ]\n\n    tmp &lt;- evaluate_fold(X_train, X_test, k, alg)\n\n    metric_db   &lt;- c(metric_db,   tmp$db)\n    metric_dunn &lt;- c(metric_dunn, tmp$dunn)\n    metric_f    &lt;- c(metric_f,    tmp$G1)\n    times       &lt;- c(times,       tmp$exec)\n  }\n\n  results &lt;- results %&gt;% add_row(\n    centers        = k,\n    algorithm      = alg,\n    db.test.mean   = mean(metric_db, na.rm = TRUE),\n    dunn.test.mean = mean(metric_dunn, na.rm = TRUE),\n    G1.test.mean   = mean(metric_f, na.rm = TRUE),\n    exec.time      = mean(times)\n  )\n}\n\nresults\n\n# A tibble: 18 × 6\n   centers algorithm     db.test.mean dunn.test.mean G1.test.mean exec.time\n     &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n 1       3 Hartigan-Wong         1.04          0.286         55.0   0.00164\n 2       3 Lloyd                 1.04          0.286         55.0   0.00129\n 3       3 MacQueen              1.04          0.286         55.0   0.00108\n 4       4 Hartigan-Wong         1.01          0.344         61.4   0.00216\n 5       4 Lloyd                 1.01          0.344         61.4   0.00154\n 6       4 MacQueen              1.01          0.344         61.4   0.00124\n 7       5 Hartigan-Wong         1.15          0.153         55.9   0.00260\n 8       5 Lloyd                 1.16          0.147         55.8   0.00197\n 9       5 MacQueen              1.16          0.157         55.0   0.00142\n10       6 Hartigan-Wong         1.22          0.150         48.8   0.00268\n11       6 Lloyd                 1.22          0.149         49.0   0.00267\n12       6 MacQueen              1.23          0.155         49.4   0.00163\n13       7 Hartigan-Wong         1.29          0.150         44.2   0.00300\n14       7 Lloyd                 1.29          0.158         45.2   0.00300\n15       7 MacQueen              1.31          0.150         44.5   0.00186\n16       8 Hartigan-Wong         1.39          0.147         41.2   0.00342\n17       8 Lloyd                 1.31          0.152         41.6   0.00362\n18       8 MacQueen              1.34          0.151         42.1   0.00211\n\n\nIn our wearable sensor dataset, all three algorithms identify \\(\\mathbf{k} \\boldsymbol{=} \\mathbf{4}\\) as the configuration with: - the lowest Davies-Bouldin index, - the highest Dunn index, and □ - the highest pseudo-F statistic.\nThis agreement across three distinct internal metrics gives strong evidence that four clusters capture the main physiological structure in the data without overfragmentation.\nTo better understand how the internal metrics behave across k and across algorithms, we reshape the results table and generate a panel of diagnostic plots (figure below). Each facet corresponds to one metric, and each line corresponds to an algorithm.\n\n# ===============================================================\n# Plot hyperparameter tuning results — clean facet labels\n# ===============================================================\n\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Ensure algorithms have nice display order\nresults &lt;- results %&gt;%\n  mutate(\n    algorithm = factor(\n      algorithm,\n      levels = c(\"Hartigan-Wong\", \"Lloyd\", \"MacQueen\")\n    )\n  )\n\n# ---------------------------------------------------------------\n# Convert results to long format for facet plotting (4 metrics)\n# ---------------------------------------------------------------\nresults_long &lt;- results %&gt;%\n  pivot_longer(\n    cols = c(db.test.mean, dunn.test.mean, exec.time, G1.test.mean),\n    names_to = \"Metric\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(\n    Metric = factor(\n      case_when(\n        Metric == \"db.test.mean\"     ~ \"db test mean\",\n        Metric == \"dunn.test.mean\"   ~ \"dunn test mean\",\n        Metric == \"exec.time\"        ~ \"exec time\",\n        Metric == \"G1.test.mean\"     ~ \"g1 test mean\"\n      ),\n      levels = c(\n        \"db test mean\",\n        \"dunn test mean\",\n        \"exec time\",\n        \"g1 test mean\"\n      )\n    )\n  )\n\n# ---------------------------------------------------------------\n# The Plot\n# ---------------------------------------------------------------\np_tuning &lt;- ggplot(\n  results_long,\n  aes(x = centers, y = Value, color = algorithm)\n) +\n  geom_line(linewidth = 0.9) +\n  geom_point(size = 2) +\n  facet_wrap(~ Metric, scales = \"free_y\") +\n  scale_color_manual(\n    values = c(\n      \"Hartigan-Wong\" = \"#E69F00\",\n      \"Lloyd\"         = \"#1B9E77\",\n      \"MacQueen\"      = \"#0072B2\"\n    )\n  ) +\n  theme_bw(base_size = 13) +\n  theme(\n    strip.background = element_rect(fill = \"grey85\"),\n    strip.text = element_text(face = \"bold\"),\n    legend.position = \"right\",\n    legend.title = element_blank(),\n    panel.border = element_rect(color = \"black\", fill = NA)\n  ) +\n  labs(\n    x = \"Centers\",\n    y = \"Value\"\n  )\n\np_tuning\n\n\n\n\n\n\n\n\n\n\n8.2.13 Interpreting the Tuning Results\nThe four panels summarize how each internal metric behaves across values of k and across the three k-means algorithms (Hartigan–Wong, Lloyd, and MacQueen). Because the metrics capture different aspects of cluster quality, agreement among them is particularly informative.\nDavies–Bouldin (DB)\n\nLower values indicate better separation between clusters relative to their internal scatter.\n\nAcross all algorithms, DB is minimized at k = 4, with a noticeable jump in DB for larger values of k. This strongly suggests that increasing the number of clusters beyond four leads to overpartitioning of the same underlying physiological patterns. All three algorithms give nearly identical DB curves, showing that algorithm choice matters very little for this dataset.\nDunn Index\n\nHigher values indicate compact clusters that are well separated.\n\nThe Dunn index clearly peaks at k = 4, matching the DB result. For k ≥ 5, the score collapses toward a low plateau, reflecting the fact that additional clusters produce smaller, more fragile partitions that are close to one another. Again, the three algorithms behave almost identically, with only minor jitter.\nPseudo-F (G1)\n\nHigher values indicate that between-cluster variation dominates within-cluster variation.\n\nThe Pseudo-F statistic also reaches its maximum at k = 4, with a sharp drop afterward. This pattern mirrors the intuition from DB and Dunn: with more clusters, the between-cluster separation shrinks because we start splitting coherent groups into smaller fragments.\nExecution Time\n\nComputation increases moderately with k, as expected: more clusters require more centroid updates and more distance calculations.\nMacQueen tends to be the fastest, Lloyd sits in the middle, and Hartigan–Wong is consistently slower—though still extremely fast in absolute terms (all under a few milliseconds per fold).\nAll three internal metrics—DB, Dunn, and Pseudo-F—agree that four clusters (k = 4) is the most stable and well-separated solution for this wearable-sensor dataset. The convergence of the three algorithms (Hartigan–Wong, Lloyd, MacQueen) on the same pattern gives us confidence that this choice of k reflects genuine underlying structure rather than algorithmic quirks.\n\n\n8.2.14 Training the final tuned k-means\nOnce we have identified the optimal number of clusters and the most appropriate k -means algorithm, the next step is to train the final model using all available data. Unlike supervised learning models, k -means is generally not used as a predictive model in the traditional sense. Instead, it is primarily a tool for discovering structure in the data-clusters that may correspond to physiological states, behaviour patterns, or other latent subgroups.\nBecause of this, we do not perform nested cross-validation to re-estimate generalization error. The goal here is simply to fit the tuned clustering solution on the full dataset, so that the identified clusters can later be: - analysed in terms of their clinical or physiological meaning, - used as features in downstream classification tasks, - or compared with external labels (e.g., patient subtypes) if available.\nBased on the tuning results obtained earlier, the best-performing configuration was: - \\(\\mathrm{k}=4\\) clusters, - algorithm = “Hartigan-Wong” (though note that all algorithms performed similarly).\nWe now fit this final model.\n\n# ===============================================================\n# Final tuned k-means model\n# ===============================================================\n\nset.seed(2025)\n\nbest_k   &lt;- 4\nbest_alg &lt;- \"Hartigan-Wong\"\n\nfinal_km &lt;- kmeans(\n  X_mat,\n  centers  = best_k,\n  algorithm = best_alg,\n  iter.max  = 100,\n  nstart    = 20\n)\n\nfinal_km\n\nK-means clustering with 4 clusters of sizes 318, 162, 160, 160\n\nCluster means:\n     HR_mean   HRV_sdnn steps_mean   skin_temp       spo2  sleep_eff\n1  1.1301127 -0.8640422 -0.8290313 -0.05404147 -0.4673542 -0.4697598\n2 -1.1072718  0.3590102 -0.4574964 -0.45757327  0.7022599  0.3145112\n3 -0.2242597 -0.1695398  0.3006915  1.39929849 -1.0035252 -0.1131580\n4 -0.9007265  1.5233259  1.8102234 -0.82859813  1.2213535  0.7283630\n\nClustering vector:\n  [1] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n [38] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n [75] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n[112] 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n[149] 4 4 4 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[186] 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[223] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n[260] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[297] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2\n[334] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[371] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[408] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[445] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3\n[482] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[519] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[556] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[593] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[630] 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[667] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[741] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[778] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\nWithin cluster sum of squares by cluster:\n[1] 621.6970 299.1251 258.0209 266.8678\n (between_SS / total_SS =  69.8 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nOnce the final k-means model has been trained, the returned object contains several pieces of information that are useful for understanding the clustering solution. The most important components are:\n\ncluster — a vector assigning each observation to one of the k clusters.\ncenters — the k cluster centroids in the original scaled feature space.\nwithinss — the within-cluster sum of squares for each cluster, indicating how tightly grouped its members are.\ntot.withinss — the overall within-cluster variation.\niter — the number of iterations required until convergence.\n\nIn our dataset, the algorithm converged quickly (typically within a few iterations), which is expected given the clear physiological structure observed during exploratory analysis. This final model encodes the latent physiological states present in the wearable signals; the next step is to visualise and interpret these states in ways that relate back to domain knowledge\nUsing the outputs of the model we can create a plot\n\n# ===============================================================\n# Plot clusters in the original scaled feature space\n# (using two representative variables)\n# ===============================================================\n\nlibrary(tidyverse)\n\n# Attach cluster labels to scaled data\ndf_clusters &lt;- wear_scaled %&gt;%\n  mutate(cluster = factor(final_km$cluster))\n\n# Choose two variables to visualise (example)\nvar_x &lt;- \"HR_mean\"\nvar_y &lt;- \"steps_mean\"\n\nggplot(df_clusters, aes_string(var_x, var_y, color = \"cluster\")) +\n  geom_point(alpha = 0.7, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme_bw(base_size = 12) +\n  labs(\n    title = \"Final Tuned k-Means Clusters in the Original Scaled Space\",\n    subtitle = paste(var_x, \"vs\", var_y),\n    x = var_x,\n    y = var_y,\n    color = \"Cluster\"\n  )\n\n\n\n\n\n\n\n\nInternal metrics help select the number of clusters, but visualisation is essential for understanding what the algorithm has actually discovered. A natural first step is to explore the learned clusters in the original scaled feature space. By plotting two representative physiological variables—such as resting heart rate and daily steps—we obtain a transparent, low-dimensional view of how the algorithm partitions individuals.\nThis kind of scatterplot does not summarise the full six-dimensional structure of the dataset, but it does highlight broad behavioural–physiological differences between groups, such as active vs. sedentary patterns or high-stress vs. well-regulated profiles. It serves as a simple sanity check before moving on to more comprehensive multivariate visualisations.\nUsing ggpairs\n\n# ===============================================================\n# GGpairs visualization of the final k-means clusters\n# Robust version (no geom_text errors, no internal failures)\n# ===============================================================\n\nlibrary(tidyverse)\nlibrary(GGally)\n\n# ---------------------------------------------------------------\n# 1. Clean the scaled dataset\n# ---------------------------------------------------------------\n# GGpairs sometimes fails when variables carry scale() attributes\n# or are stored as tibble columns with attributes. Converting all\n# variables to numeric and dropping attributes ensures stability.\n\nclean_scaled &lt;- wear_scaled %&gt;%\n  mutate(across(everything(), as.numeric)) %&gt;%  # remove attributes\n  as.data.frame()\n\n# Add final k-means cluster assignments\ndf_clusters &lt;- clean_scaled %&gt;%\n  mutate(cluster = factor(final_km$cluster))\n\n\n# ---------------------------------------------------------------\n# 2. Custom lower-panel function (colored scatterplots)\n# ---------------------------------------------------------------\n# GGpairs allows overriding each panel. This function produces\n# a scatterplot for each pair of variables, colored by cluster.\n\nmy_points &lt;- function(data, mapping, ...){\n  ggplot(data, mapping) +\n    geom_point(aes(color = cluster), alpha = 0.6, size = 1.6) +\n    scale_color_brewer(palette = \"Set2\") +\n    theme_bw(base_size = 10)\n}\n\n\n# ---------------------------------------------------------------\n# 3. Custom diagonal function (density curves)\n# ---------------------------------------------------------------\n# Each diagonal element shows a density plot of a single variable.\n# This replaces the default behaviour and avoids problematic geoms.\n\nmy_diag &lt;- function(data, mapping, ...){\n  ggplot(data, mapping) +\n    geom_density(fill = \"grey80\", alpha = 0.7) +\n    theme_bw(base_size = 10)\n}\n\n\n# ---------------------------------------------------------------\n# 4. Custom upper panel (blank)\n# ---------------------------------------------------------------\n# Upper-panel correlation plots in GGpairs often use geom_text(),\n# which can trigger the “object 'Y' not found” error depending on\n# the internal data structure. To avoid all issues, we set them blank.\n\nmy_blank &lt;- function(data, mapping, ...){\n  ggplot() + theme_void()\n}\n\n\n# ---------------------------------------------------------------\n# 5. Construct the GGpairs object\n# ---------------------------------------------------------------\n# We explicitly define:\n#   - lower panels: scatterplots\n#   - diagonal: densities\n#   - upper panels: blank (safe)\n# This layout mirrors many applied ML textbooks.\n\np_pairs &lt;- ggpairs(\n  df_clusters,\n  columns = 1:6,                 # HR_mean, HRV_sdnn, steps_mean, skin_temp, spo2, sleep_eff\n  lower = list(continuous = my_points),\n  diag  = list(continuous = my_diag),\n  upper = list(continuous = my_blank),\n  progress = FALSE\n)\n\n\n# ---------------------------------------------------------------\n# 6. Final aesthetics\n# ---------------------------------------------------------------\np_pairs +\n  ggtitle(\"Final Tuned k-Means Clusters in the Original Scaled Feature Space\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme_bw(base_size = 11) +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nAlthough two-dimensional scatterplots reveal partial structure, they inevitably ignore interactions among the remaining physiological variables. To obtain a richer multivariate overview, we use a GGpairs plot, which displays:\n\npairwise scatterplots (lower panels),\ndensity curves for each variable (diagonal),\nand optional correlation summaries (upper panels).\n\nIn this chapter we use a simplified, robust version of GGpairs, replacing the upper panels with blanks to avoid text-drawing issues and emphasise the visual geometry of the clusters. This matrix of plots allows us to assess:\n\nwhether clusters are separated across several physiological axes,\nwhether two variables jointly reveal structure not seen in isolation,\nand whether particular clusters occupy well-defined subregions of the space.\n\nThis visual diagnostic is particularly helpful in wearable-health data, where physiological variables interact nonlinearly and cluster boundaries may depend on several dimensions simultaneously.\nThe GGpairs matrix provides detailed pairwise relationships, but it remains difficult to visualise the entire six-dimensional structure at once. To obtain a global, low-dimensional summary of the clusters, we project the scaled data onto the first two principal components (PCs). PCA constructs a new coordinate system that captures the greatest possible variance in the data, subject to orthogonality constraints. Although PCA is not a clustering method, it is extremely useful for:\n\ndisplaying high-dimensional clusters in two dimensions,\nidentifying major physiological axes of variation (e.g., activity vs. autonomic balance),\nand confirming whether clusters that appear distinct in the original space remain well-separated under a linear projection.\n\nIf the trained clusters maintain separation in the PCA plane, this suggests that the underlying physiological states are robust and not artefacts of particular variable combinations.\n\n# ---------------------------------------------------------------\n# PCA projection for visualization\n# ---------------------------------------------------------------\npca_res &lt;- prcomp(X_mat, scale. = FALSE)\npca_df &lt;- data.frame(\n  PC1 = pca_res$x[,1],\n  PC2 = pca_res$x[,2],\n  cluster = factor(final_km$cluster)\n)\n\nggplot(pca_df, aes(PC1, PC2, color = cluster)) +\n  geom_point(alpha = 0.7) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme_bw(base_size = 12) +\n  labs(\n    title = \"Final Tuned k-Means Model (k = 4)\",\n    subtitle = \"Visualized on first two principal components\",\n    color = \"Cluster\"\n  )\n\n\n\n\n\n\n\n\n\n\n8.2.15 Using final model to assign new observations\nAlthough clustering methods are not designed for prediction in the same way as supervised classifiers, a fitted k-means model can assign cluster labels to new observations. This is often useful during exploratory analysis: once the cluster structure has been established, we may want to understand where a new patient, recording, or time window falls relative to the existing physiological groups.\nThe tuned model we trained earlier contains all the necessary components for this task, particularly the cluster centroids. Because k-means assigns each case to the nearest centroid in feature space, predicting a cluster membership for new data amounts to computing distances from the new case to each centroid and selecting the closest one.\n\n\n8.2.16 Predicting the cluster of a new observation\nOnce the new wearable record is prepared and scaled using the same means and standard deviations as the training data, we can pass it through our tuned k-means model. Unlike supervised learning, k-means does not have a formal prediction function that re-estimates centroids; prediction simply assigns the new point to the nearest centroid in Euclidean space.\n\n# ===============================================================\n# Predicting cluster membership for a new case using the tuned\n# k-means model (final_km)\n# ===============================================================\n\nlibrary(tidyverse)\n\n# ---------------------------------------------------------------\n# 1. Ensure we have the same feature matrix used for training\n# ---------------------------------------------------------------\n# X_mat must be the matrix used to train k-means:\n# Example in your code: X_mat &lt;- as.matrix(wear_scaled %&gt;% select(HR_mean:sleep_eff))\n\n# Reconstruct training center and scale (attributes were lost)\ntrain_center &lt;- apply(X_mat, 2, mean)\ntrain_scale  &lt;- apply(X_mat, 2, sd)\n\n# Inspect (should show HR_mean, HRV_sdnn, steps_mean, skin_temp, spo2, sleep_eff)\nprint(train_center)\n\n      HR_mean      HRV_sdnn    steps_mean     skin_temp          spo2 \n 9.181544e-16 -1.287304e-15 -1.498524e-15  3.752436e-14 -4.568068e-14 \n    sleep_eff \n 1.104949e-15 \n\nprint(train_scale)\n\n   HR_mean   HRV_sdnn steps_mean  skin_temp       spo2  sleep_eff \n         1          1          1          1          1          1 \n\n# ---------------------------------------------------------------\n# 2. New case: raw, unscaled values\n# ---------------------------------------------------------------\nnew_raw &lt;- tibble(\n  HR_mean    = 72,\n  HRV_sdnn   = 40,\n  steps_mean = 5800,\n  skin_temp  = 33.2,\n  spo2       = 97.1,\n  sleep_eff  = 0.88\n)\n\n# ---------------------------------------------------------------\n# 3. Match the column order from training\n# ---------------------------------------------------------------\nvars &lt;- colnames(X_mat)\n\nnew_raw &lt;- new_raw %&gt;%\n  dplyr::select(all_of(vars))  # ensures correct variable order\n\n# ---------------------------------------------------------------\n# 4. Scale using training mean & sd\n# ---------------------------------------------------------------\nnew_scaled &lt;- new_raw %&gt;%\n  mutate(\n    across(\n      everything(),\n      ~ (.x - train_center[cur_column()]) / train_scale[cur_column()]\n    )\n  ) %&gt;%\n  as.matrix()\n\n# ---------------------------------------------------------------\n# 5. Predict cluster: nearest centroid\n# ---------------------------------------------------------------\nbest_k &lt;- nrow(final_km$centers)\n\n# Compute distances from new point to centroids\nd_new &lt;- as.matrix(dist(rbind(final_km$centers, new_scaled)))[\n  1:best_k, best_k + 1\n]\n\nassigned_cluster &lt;- which.min(d_new)\n\ncat(\"\\nThe new case is assigned to cluster:\", assigned_cluster, \"\\n\")\n\n\nThe new case is assigned to cluster: 4 \n\n\nThe code above performs this assignment in three steps. First, we reconstruct the feature-wise means and standard deviations from the original training matrix, because these attributes are no longer attached after converting the scaled data into a bare matrix. Second, we scale the new observation using those training parameters, ensuring that it is placed in the same standardized feature space in which the model was learned. Finally, we compute the Euclidean distance from the new point to each centroid, and select the cluster whose centroid is closest. The assignment step is:\nassigned_cluster &lt;- which.min(d_new)\nBecause the tuned model selected four clusters, the distances from the new observation to each of the four centroids are evaluated. The output\nThe new case is assigned to cluster: 4\nindicates that the new wearable profile lies closest to centroid 4 in the standardized feature space. K-means does not provide uncertainty measures or probabilistic cluster membership; the prediction relies solely on geometric proximity. This makes cluster prediction useful for exploratory tagging of new wearable records, but it should not be interpreted as a substitute for supervised classification.\n\n\n8.2.17 Strengths and limitations of k-means clustering\nAlthough k -means is one of the most widely used clustering methods, it is far from universally applicable. Understanding where the algorithm shines-and where it struggles-helps determine whether it is appropriate for a given dataset, especially in complex physiological monitoring contexts.\nStrengths k-means offers several practical advantages:\n\nFlexible reassignment: During the iterative process, observations can switch clusters whenever doing so reduces the objective function. This allows the algorithm to refine boundaries gradually until it reaches a stable configuration.\nComputational efficiency: Because k -means relies mainly on distance calculations and centroid updates, it scales well to high-dimensional datasets and large sample sizes. This makes it attractive for streaming or continuously collected wearable data.\nConceptual simplicity: The algorithm’s mechanics-assign points to the nearest centroid, recompute the centroids, and repeat-are easy to understand and explain, which contributes to its popularity in applied settings.\nCentroid interpretability: The learned centers often provide intuitive summaries of the “typical profile” for each physiological state (e.g., low HR + high steps = active, healthy behaviour).\n\nLimitations:\nDespite its usefulness, k-means has structural assumptions and sensitivities that must be acknowledged:\n\nInability to model non-numeric or categorical variables: Because the algorithm depends on Euclidean distance, it cannot naturally incorporate nominal features (such as device type, medication class, or diagnosis labels) without inappropriate numerical encoding.\nThe number of clusters must be chosen beforehand: k -means does not infer how many groups are present. The analyst must supply a value of \\(k\\), which requires external validation tools such as internal metrics or stability measures.\nSensitivity to variable scaling: Since features with larger numerical ranges dominate the distance metric, proper standardization is essential. Failure to scale variables can cause clusters to form along arbitrary axes driven by units of measurement.\nInitialization variability: Different random initial centroids can yield different clustering solutions, especially when the underlying structure is noisy or weak. Multiple starts (e.g., nstart = 20 ) help mitigate this issue.\nVulnerability to outliers: A single extreme observation can pull a centroid toward it, distorting cluster boundaries. This is problematic in datasets with occasional aberrant physiological readings (e.g., faulty sensors or brief artefacts).\nPreference for spherical clusters of similar size: k-means implicitly assumes that clusters are separated by roughly equal-distance boundaries. It performs poorly when the true structure involves elongated, nested, or unevenly sized clusters.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#hierarchical-clustering",
    "href": "clustering.html#hierarchical-clustering",
    "title": "8  Clustering",
    "section": "8.3 Hierarchical clustering",
    "text": "8.3 Hierarchical clustering\n\n8.3.1 Overview\nHierarchical clustering is a family of methods designed to uncover structure in a dataset by building a hierarchy of groups rather than committing to a single, flat partition. Instead of choosing a fixed number of clusters up front, the method produces a tree-like representation—called a dendrogram—that displays how observations merge into progressively larger clusters or, alternatively, how a large group can be subdivided into smaller, more refined subgroups. This hierarchical perspective is valuable whenever the data may contain structure at multiple resolutions, something that flat clustering methods such as k-means cannot capture on their own.\nIn contrast to algorithms that search for k centers and optimize an objective function, hierarchical clustering works by repeatedly evaluating how similar points or clusters are to one another. Depending on the strategy, the method either merges the closest clusters step by step, or it starts from a single large cluster and recursively splits it apart. In either direction, the result is a nested arrangement of groups that reveals not only which observations belong together but also how tightly or loosely they relate as we move through the hierarchy.\nA dendrogram provides a compact visual representation of this process. At the bottom of the dendrogram, each observation begins as its own cluster. As we move upward, the most similar clusters are joined by horizontal lines. The height at which these merges occur reflects how different the clusters are: merges at low heights indicate high similarity, whereas merges occurring at greater heights correspond to more distinct groups. Cutting the dendrogram at a particular height yields a set of flat clusters, and the analyst can adjust this cut to examine structure at coarser or finer levels.\nHierarchical clustering is particularly useful in settings where the data naturally exhibit nested organization. Genomic applications are a classic example: genes with related regulatory behaviour may form tight modules, which themselves combine into broader functional pathways. Capturing this layered organisation is central to understanding biological processes, and hierarchical clustering provides a principled way to reveal these relationships.\nThere are two main approaches to constructing the hierarchy. In agglomerative clustering, each observation begins in its own cluster, and the algorithm iteratively merges the two closest clusters until everything has been combined into a single group. In divisive clustering, we begin with the entire dataset as one cluster and progressively split it into increasingly smaller groups. Both perspectives build the same kind of dendrogram but interpret the “construction” of the tree in opposite directions.\n\n\n8.3.2 Motivational example\nTo see why hierarchical clustering is so useful in biomedical data analysis, imagine we are working with a small panel of immune-related genes measured in blood samples from several patients. Each gene belongs to a different part of the immune system—some reflect interferon signalling, others track inflammatory activation, and others relate to cellular stress responses. Because these pathways interact and sometimes activate in coordinated waves, the expression patterns across genes often contain multiple layers of structure. Some genes behave almost identically, forming tight groups, while others share broader patterns associated with higher-level biological programmes.\nThis kind of multi-scale organisation makes hierarchical clustering especially appropriate. Instead of forcing the data into a preset number of clusters, the algorithm builds a tree showing how genes join together as similarity increases. Small co-regulated gene sets appear near the bottom of the tree; related pathways merge at intermediate heights; and, at the top, we see broader immune axes that describe global patterns across the panel. For students working with ATT topics, this type of analysis illustrates how biological systems naturally organise into nested modules—exactly the kind of insight useful for biomarker discovery, mechanistic interpretation, and patient stratification.\nWe will work with a miniature gene-expression panel designed for this purpose. The code below loads the matrix of gene expression values (genes × samples) and an accompanying vector that encodes which biological programme each gene belongs to. This annotation will later be used to label rows in the heatmap.\n\n# ===============================================================\n# Load simulated genomic data & run hierarchical clustering\n# ===============================================================\n\nlibrary(tidyverse)\nlibrary(pheatmap)\n\n# ---------------------------------------------------------------\n# 1. Load dataset\n# ---------------------------------------------------------------\nexpr_matrix &lt;- readRDS(\"~/att_ai_ml/att_book/data/simulated_genomic_matrix.rds\")\ngroup       &lt;- readRDS(\"~/att_ai_ml/att_book/data/simulated_genomic_groups.rds\")\n\n# Annotation for heatmap\nann &lt;- data.frame(Group = group)\nrownames(ann) &lt;- rownames(expr_matrix)\n\nIn the next sections, we will use this gene panel to illustrate how hierarchical clustering constructs its tree, how the dendrogram should be interpreted, and how to choose a meaningful cut that summarises the underlying biology without losing important detail.\n\n\n8.3.3 Approaches to hierarchical clustering\nAgglomerative methods start from the most fragmented possible view of the data: every observation begins as a cluster of size one. At each iteration, the algorithm identifies the two clusters that are most similar and fuses them. Repeating this process gradually reduces the number of clusters until a single, ultra-broad cluster remains. Because it repeatedly merges the “closest” groups, this class of algorithms tends to build very interpretable dendrograms and is widely used in genomics, ecology, text mining, and other high-dimensional applications.\nIn practical terms, agglomerative clustering follows four conceptual steps: 1. Compute the pairwise distance matrix between all observations. 2. Identify the closest pair of clusters (initially, individual observations). 3. Merge them into a new cluster. 4. Recompute distances between this new cluster and the remaining clusters.\nDesenhar figura na mao aqui\n\n8.3.3.1 Agglomerative\n\n8.3.3.1.1 Steps\n\n\n8.3.3.1.2 Linkage methods\n\n\n\n8.3.3.2 Divisive\n\n8.3.3.2.1 DIANA\n\n\n\n\n8.3.4 Building and agglomerative clustering model\n\n# For convenience, create a scaled version\nexpr_scaled &lt;- scale(expr_matrix)\n# ---------------------------------------------------------------\n# 2. Compute distance matrix (Euclidean)\n# ---------------------------------------------------------------\ndist_mat &lt;- dist(expr_scaled, method = \"euclidean\")\n\n\n# ---------------------------------------------------------------\n# 3. Fit hierarchical clustering\n# ---------------------------------------------------------------\nhc_genomic &lt;- hclust(dist_mat, method = \"ward.D2\")\n\n\n# ---------------------------------------------------------------\n# 4. Convert to dendrogram for plotting\n# ---------------------------------------------------------------\ndend_genomic &lt;- as.dendrogram(hc_genomic)\n\nplot(dend_genomic, leaflab = \"none\",\n     main = \"Hierarchical Clustering of Genomic Profiles\")\n\n\n\n\n\n\n\n\n\n# ---------------------------------------------------------------\n# 5. Select a flat clustering (e.g., k = 3)\n# ---------------------------------------------------------------\nk_clusters &lt;- 3\ngenomic_cut &lt;- cutree(hc_genomic, k = k_clusters)\n\nplot(dend_genomic, leaflab = \"none\")\nrect.hclust(hc_genomic, k = k_clusters, border = 2:5)\n\n\n\n\n\n\n\n\n\n# ---------------------------------------------------------------\n# 6. Internal metrics function\n# ---------------------------------------------------------------\nlibrary(clusterSim)\nlibrary(clValid)\n\ncluster_metrics &lt;- function(data, clusters, dist_matrix) {\n  list(\n    db    = clusterSim::index.DB(data, clusters)$DB,\n    G1    = clusterSim::index.G1(data, clusters),\n    dunn  = clValid::dunn(dist_matrix, clusters),\n    k     = length(unique(clusters))\n  )\n}\n\n\n# ---------------------------------------------------------------\n# 7. Generate bootstrap samples\n# ---------------------------------------------------------------\nlibrary(purrr)\nlibrary(dplyr)\n\nboot_samples &lt;- map(1:10, ~ {\n  expr_scaled %&gt;%\n    as_tibble() %&gt;%\n    sample_n(size = nrow(.), replace = TRUE)\n})\n\n\n# ---------------------------------------------------------------\n# 8. Calculate internal cluster metrics for bootstraps\n# ---------------------------------------------------------------\nmetrics_df &lt;- map_df(boot_samples, function(boot) {\n\n  d &lt;- dist(boot, method = \"euclidean\")\n  cl &lt;- hclust(d, method = \"ward.D2\")\n\n  map_df(3:6, function(k) {\n    cut_k &lt;- cutree(cl, k = k)\n    cluster_metrics(boot, clusters = cut_k, dist_matrix = d)\n  })\n})\n\n\n# ---------------------------------------------------------------\n# 9. Reshape for visualization\n# ---------------------------------------------------------------\nmetrics_long &lt;- metrics_df %&gt;%\n  mutate(bootstrap = factor(rep(1:10, each = 4))) %&gt;%\n  tidyr::pivot_longer(cols = c(db, G1, dunn),\n                      names_to = \"Metric\",\n                      values_to = \"Value\")\n\n\n# ---------------------------------------------------------------\n# 10. Plot bootstrap internal metrics\n# ---------------------------------------------------------------\nggplot(metrics_long, aes(as.factor(k), Value)) +\n  facet_wrap(~ Metric, scales = \"free_y\") +\n  geom_line(aes(group = bootstrap), alpha = 0.2) +\n  geom_line(stat = \"summary\", fun = mean, linewidth = 1.2, color = \"black\") +\n  stat_summary(fun.data = \"mean_cl_boot\",\n               geom = \"crossbar\", width = 0.5, fill = \"white\") +\n  labs(x = \"Number of clusters\", y = \"Metric value\",\n       title = \"Bootstrap Internal Cluster Metrics (Genomic Dataset)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n# ---------------------------------------------------------------\n# 11. Cluster stability analysis\n# ---------------------------------------------------------------\nlibrary(fpc)\npar(mfrow = c(3, 4))\n\nclust_stab &lt;- clusterboot(\n  dist_mat,\n  B = 10,\n  clustermethod = disthclustCBI,\n  k = k_clusters,\n  cut = \"number\",\n  method = \"ward.D2\",\n  showplots = TRUE\n)\n\nboot 1 \n\n\nboot 2 \n\n\nboot 3 \n\n\nboot 4 \n\n\nboot 5 \n\n\nboot 6 \n\n\nboot 7 \n\n\nboot 8 \n\n\nboot 9 \n\n\nboot 10 \n\n\n\n\n\n\n\n\nclust_stab\n\n* Cluster stability assessment *\nCluster method:  hclust \nFull clustering results are given as parameter result\nof the clusterboot object, which also provides further statistics\nof the resampling results.\nNumber of resampling runs:  10 \n\nNumber of clusters found in data:  3 \n\n Clusterwise Jaccard bootstrap (omitting multiple points) mean:\n[1] 1 1 1\ndissolved:\n[1] 0 0 0\nrecovered:\n[1] 10 10 10\n\n\n\n# ---------------------------------------------------------------\n# 12. Add cluster labels to tibble\n# ---------------------------------------------------------------\nexpr_df &lt;- as_tibble(expr_scaled) %&gt;%\n  mutate(Cluster = factor(genomic_cut))\n\n# Use a subset of representative genes for ggpairs\nrepresentative_genes &lt;- colnames(expr_df)[1:5]\n\nGGally::ggpairs(\n  expr_df,\n  columns = representative_genes,\n  aes(color = Cluster),\n  upper = list(continuous = \"density\"),\n  lower = list(continuous = wrap(\"points\", alpha = 0.4, size = 0.5))\n) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n8.3.4.1 Choosing the number of clusters\n\n\n8.3.4.2 Cut selecting methods to select flat clusters\n\n\n\n8.3.5 Cluster stability",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#density-based-clustering",
    "href": "clustering.html#density-based-clustering",
    "title": "8  Clustering",
    "section": "8.4 Density based clustering",
    "text": "8.4 Density based clustering\n\n8.4.1 DBSCAN\n\n\n8.4.2 OPTICS",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "clustering.html#mixture-modelling-clustering",
    "href": "clustering.html#mixture-modelling-clustering",
    "title": "8  Clustering",
    "section": "8.5 Mixture modelling clustering",
    "text": "8.5 Mixture modelling clustering",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Clustering</span>"
    ]
  },
  {
    "objectID": "interpretable.html",
    "href": "interpretable.html",
    "title": "9  Interpretable AI",
    "section": "",
    "text": "9.1 Interpretable",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Interpretable AI</span>"
    ]
  },
  {
    "objectID": "genai_foundations.html",
    "href": "genai_foundations.html",
    "title": "10  GenAI: Foundations",
    "section": "",
    "text": "10.1 Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GenAI: Foundations</span>"
    ]
  },
  {
    "objectID": "genai_foundations.html#running-code",
    "href": "genai_foundations.html#running-code",
    "title": "10  GenAI: Foundations",
    "section": "10.2 Running Code",
    "text": "10.2 Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>GenAI: Foundations</span>"
    ]
  },
  {
    "objectID": "genai_app.html",
    "href": "genai_app.html",
    "title": "11  GenAI: Applications",
    "section": "",
    "text": "11.1 Quarto\nQuarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GenAI: Applications</span>"
    ]
  },
  {
    "objectID": "genai_app.html#running-code",
    "href": "genai_app.html#running-code",
    "title": "11  GenAI: Applications",
    "section": "11.2 Running Code",
    "text": "11.2 Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n1 + 1\n\n[1] 2\n\n\nYou can add options to executable code like this\n\n\n[1] 4\n\n\nThe echo: false option disables the printing of code (only output is displayed).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>GenAI: Applications</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bell, David E., Howard Raiffa, and Amos Tversky, eds. 1988. Decision\nMaking: Descriptive, Normative, and Prescriptive Interactions.\nCambridge: Cambridge University Press. https://www.cambridge.org/core/books/decision-making/D29E12748C31B85FE83C9CEDEE9D4D88.\n\n\nEisenhauer, Elizabeth A., Patrick Therasse, Jan Bogaerts, Lawrence H.\nSchwartz, Daniel Sargent, Rebecca Ford, Janet Dancey, et al. 2009.\n“New Response Evaluation Criteria in Solid Tumours: Revised RECIST\nGuideline (Version 1.1).” European Journal of Cancer 45\n(2): 228–47. https://doi.org/10.1016/j.ejca.2008.10.026.\n\n\nHasin, Yoram, Michael Seldin, and Aldons Lusis. 2017. “Multi-Omics\nApproaches to Disease.” Genome Biology 18: 83. https://doi.org/10.1186/s13059-017-1215-1.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.\n“Deep Residual Learning for Image Recognition.” In\nProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 770–78. https://arxiv.org/abs/1512.03385.\n\n\nHilbert, Martin, and Priscila López. 2011. “The World’s\nTechnological Capacity to Store, Communicate, and Compute\nInformation.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nHoward, Ronald A. 1988. “Decision Analysis: Practice and\nPromise.” Management Science 34 (6): 679–95.\n\n\nHripcsak, George, and David J. Albers. 2013. “Next-Generation\nPhenotyping of Electronic Health Records.” Journal of the\nAmerican Medical Informatics Association 20 (1): 117–21. https://doi.org/10.1136/amiajnl-2012-001145.\n\n\nJensen, Peter B., Lars J. Jensen, and Søren Brunak. 2012. “Mining\nElectronic Health Records: Towards Better Research Applications and\nClinical Care.” Nature Reviews Genetics 13 (6): 395–405.\nhttps://doi.org/10.1038/nrg3208.\n\n\nKarczewski, Konrad J., and Michael P. Snyder. 2018. “Integrative\nOmics for Health and Disease.” Nature Reviews Genetics\n19: 299–310. https://doi.org/10.1038/nrg.2018.4.\n\n\nMooney, Paul Timothy. 2018. “Chest x-Ray Images\n(Pneumonia).” https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia.\n\n\nRaghupathi, Wullianallur, and Viju Raghupathi. 2014. “Big Data\nAnalytics in Healthcare: Promise and Potential.” Health\nInformation Science and Systems 2 (3): 1–10. https://doi.org/10.1186/2047-2501-2-3.\n\n\nSarti, Danilo Augusto. 2013. “Uncertainty Management Through\nDecision Analysis: Applications to Production Optimization and Uncertain\nDemands.” PhD thesis, Universidade de São Paulo.\n\n\nSpetzler, Carl, Hannah Winter, and Jennifer Meyer. 2016. Decision\nQuality: Value Creation from Better Business Decisions. Hoboken,\nNJ: John Wiley & Sons.\n\n\nStephens, Zachary D., Skylar Y. Lee, Faraz Faghri, R. Ilkayeva Campbell,\net al. 2015. “Big Data: Astronomical or Genomical?”\nPLOS Biology 13 (7): e1002195. https://doi.org/10.1371/journal.pbio.1002195.\n\n\nTan, Mingxing, and Quoc V. Le. 2019. “EfficientNet: Rethinking\nModel Scaling for Convolutional Neural Networks.” In\nInternational Conference on Machine Learning (ICML), 6105–14.\nhttps://arxiv.org/abs/1905.11946.\n\n\nTopol, Eric J. 2019. Deep Medicine: How Artificial Intelligence Can\nMake Healthcare Human Again. New York: Basic Books.\n\n\nWhite, Douglas John. 1970. Decision Theory. New Brunswick, NJ:\nRoutledge / Transaction Publishers. https://www.routledge.com/Decision-Theory/White/p/book/9780202308982.\n\n\nYang, Jiancheng, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke,\nHanspeter Pfister, and Bingbing Ni. 2023. “MedMNIST V2-a\nLarge-Scale Lightweight Benchmark for 2D and 3D Biomedical Image\nClassification.” Scientific Data 10 (1): 41.\n\n\nYang, Jiancheng, Rui Shi, Donglai Wei, Lequan Yu, Zaiyi Zhang, Liwei\nWang, Dong Ni, and Pheng-Ann Heng. 2021. “MedMNIST Classification\nDecathlon: A Lightweight AutoML Benchmark for Medical Image\nAnalysis.” Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 14817–28. https://doi.org/10.1109/ICCV48922.2021.01454.",
    "crumbs": [
      "References"
    ]
  }
]