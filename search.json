[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Machine Learning and AI for Health Sciences",
    "section": "",
    "text": "Preface\nTo be written.\nThis is a beta version of a book. Use it carefully!",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction_AI.html",
    "href": "introduction_AI.html",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "",
    "text": "1.1 Introduction",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#data-era",
    "href": "introduction_AI.html#data-era",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.2 Data era",
    "text": "1.2 Data era\nThis is an introductory chapter about data and data modelling for evidence based actions, decisions and policy making with a particular interest in advanced therapeutic technologies. Data are representations of facts, observations, or measurements structured or unstructured that describe the attributes or states of objects, events, or phenomena. They are symbols or signals without inherent meaning until interpreted within a context or combined with other information. In scientific and computational contexts, data are typically considered the raw inputs from which information and knowledge are derived through processes of organization, analysis, and interpretation. Data can assume several different forms including, numeric readings from a sensor, characters in a database, or pixels in an image, text in a message, electronic information, sequence of bases in DNA and RNA, etc. Data are evidence of our immediate reality and contexts therefore being crucial for fact based argumentation.\nWe live in an era of explosion in the availability of data, driven by the rapid expansion of the internet, advances in computational power, and the massive growth of digital storage and connectivity devices . Over recent decades, the cost of computing and storage has plummeted while the ability to collect, transmit, and analyze information has risen exponentially; in many domains, the data generated in the last few years surpass those produced in all prior human history (Hilbert and López 2011; Stephens et al. 2015). Historical analyses of data production and storage capacity show orders-of-magnitude increases year over year, reshaping how science, industry, and society operate, imposing almost naturally a new data driven paradigm (Hilbert and López 2011).\nA particular example of transformative shift is the omics era in the life sciences, a period in which substantial amount of data has been produced in genomics, transcriptomics, proteomics, metabolomics, implying in massive, complex datasets capturing biological systems in high resolution (Stephens et al. 2015; Hasin, Seldin, and Lusis 2017). Similar surges occur across disciplines from astronomy to agriculture and environmental monitoring where instruments and sensors continuously collect structured and unstructured data at scales that exceed traditional analytical capacities (Stephens et al. 2015).\nIn health care, data streams such as electronic health records (EHR), laboratory tests, medical imaging, wearable sensors, and omics profiles create a multidimensional view of patients (Jensen, Jensen, and Brunak 2012; Hripcsak and Albers 2013). The later kind of view can be integrated and enhanced with the possibilities of contrasting omics and biological data with other sources including social economic information bringing an opportunity for new solutions in health care.\nA particular challenge and frontier of data era is the need for integration and to fuse heterogeneous sources into coherent, trustworthy insights for predictive modeling, personalized treatment, and clinical decision support (Raghupathi and Raghupathi 2014; Karczewski and Snyder 2018; Jensen, Jensen, and Brunak 2012). Such an integration can enhance outcomes while maintaining safety, fairness, and transparency (Topol 2019) when done responsibly.\nSeveral concepts, and ideas can help us to understand and navigate this complex landscape of data sourcing, storing, processing and modelling for evidence actions. The rest of this chapter will cover these topics.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#sources-of-data",
    "href": "introduction_AI.html#sources-of-data",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.3 Sources of data",
    "text": "1.3 Sources of data\nAI and machine-learning methods depend fundamentally on the quality and nature of the data used to train them. In health sciences and particularly in the field of Advanced Therapeutic Technologies (ATT), data can emerge from a wide range of processes, yet most of them can be meaningfully grouped into two broad classes: observational and experimental. Understanding the differences between these types is essential because each one carries specific strengths, limitations, and implications for modelling and decision-making.\nObservational data arise from situations in which no intervention is imposed by the researcher; instead, information is simply recorded as it naturally occurs in clinical or biological settings. Much of modern health data falls into this category. Electronic health records, for instance, routinely capture demographic information, laboratory results, diagnoses, procedures, and medication histories as part of everyday clinical care. Wearable sensors provide continuous streams of physiological measurements such as heart rate, oxygen saturation, sleep quality, or activity levels, reflecting the patient’s lived environment rather than a controlled experiment. Medical imaging repositories CT, MRI, ultrasound, histopathology are also observational in nature, as they document the state of the patient without manipulating it. Registries, whether for cancer, transplantation, rare diseases or adverse events, add another important layer, offering longitudinal and population-level perspectives. In ATT contexts, observational data play a crucial role in understanding real-world performance of new therapeutic technologies, monitoring long-term safety after regulatory approval, and identifying predictors of treatment success or failure within routine clinical practice.\nIn contrast, experimental data are generated in controlled settings where a researcher actively intervenes by assigning treatments, conditions, or exposures according to a predefined design. This category encompasses a wide spectrum of studies, from early in vitro experiments and animal models to human clinical trials. For example, before a new gene or cell therapy reaches patients, it undergoes a series of structured tests designed to quantify biological response, evaluate safety profiles, and characterize potential off-target effects. Controlled laboratory experiments may examine cellular behavior when exposed to a regenerative biomaterial or probe the efficiency of a CRISPR-based gene-editing protocol. Preclinical in vivo studies evaluate therapeutic effects and toxicity under standardized conditions. At the clinical end of the spectrum, randomized controlled trials compare one treatment against another (or against standard care) under carefully regulated environments, providing the most rigorous evidence for causality. These experimental datasets form the backbone for regulatory assessments in ATT, supporting decisions related to trial approval, dosing strategies, safety monitoring, and eventual market authorization.\nAlthough observational and experimental data differ in structure and purpose, they are deeply complementary. Observational data offer a broad, naturalistic view of patient populations and allow researchers to capture the complexity of real-world clinical behavior. Experimental data provide the depth and rigor needed to establish causal relationships and quantify the true therapeutic effect of an intervention. In practice, robust AI and ML applications in health care frequently rely on both. Predictive models developed from observational data may later be validated or even refined using experimental evidence. Similarly, insights from experimental studies often guide the design of models applied to real-world datasets. Together, these data sources provide a foundation on which trustworthy, transparent, and clinically meaningful AI systems can be built.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#models",
    "href": "introduction_AI.html#models",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.4 Models",
    "text": "1.4 Models\nThe definition of what is a model is the first useful conceptual tool we will discuss intuitively with the example depicted in Figure 1.1. In the center right of the Figure we have a cylinder that was cut in half and which could be represented in different manners, including, the obvious half cylinder, but also as triangle, an ellipsis and a semi-ellipsis, depending on the point of view. If we consider the cylinder as the reality we want to represent, we see that we have at least three different possible representations. The word representation has an interesting synonym: model. Thus, models can be understood and seen merely as representations of a given situation we want to represent. Figure 1.1 is a didactic example that also brings to our perception the fact that for each situation we may have different possible models with different structures and characteristics, with the implication that such models are comparable regarding their capacity to represent reality. Another important point about models is that they are built based on assumptions, e.g, we assume that the structure of the cylinders is not modified when we are creating the models, and that we can represent a 3-D structure with a 2-D figure. When creating models we should be always aware of our assumptions and of the need to check if they are valid in the particular situation we are addressing.\n\n\n\n\n\n\nFigure 1.1: A conceptual example of creation of models. A half cylinder illustrating an instance in reality is approximated by 3 different 2-D representations.\n\n\n\nOnce a model is basically a representation it can be built in several different manners including figures, flowcharts, and with the usage of equations and other statistical mathematical tools. Consider the dataset att_demo2 in Table 1.1 containing observations of 5 patients regarding a heart condition (health or Heart Condition), the level of activity (rest or moderate), the age in years, the heart rate in bpm and saturation per oxygen (spo2). As discussed in the first section of the chapter this dataset is factual evidence about the patients, representing a reality about they health status (their reality statuts), the equivalent to the half cyllinder in our Figure 1.1. For this dataset, we could creat different possible representations depending on our goals, particularly, statistical and mathematical representations. Suppose we want to explain the heart rate and spo2 using the rest of information in the dataset, we could describe this notation with the following notation:\n\\[\n(hr,\\, spO_2) = f(\\text{condition},\\, \\text{activity},\\, \\text{age})\n\\tag{1.1}\\]\nThe model/representation in Equation 1.1 is a specific example of a generic model in which we say that the heart rate and saturation of patients are a function of their heart condition, level of activity and age. Equation Equation 1.1 for now is an example of a mathematical function (we assume no error). We could create another representation like in Equation 1.2, and assume that we also have an error implied in using condition, activity and age to explain, heart rate and spo2, in that case we have a statistical model.\n\\[ (hr,spo2)=f(condition,activity,age) + error  \\tag{1.2}\\]\nMajor part of the rest of this book will be dedicate on different ways to define the function that will be connecting the things we want to explain with we use to explain them.\nIn this section we learnt that models are representations we use to assess a reality, situation or context, passible to be comparable. We also learnt that we can use equations to create models based on datasets.\n\n\n\nTable 1.1: Simulated ATT micro-dataset (n = 5) with heart condition (binary) and age (continuous).\n\n\natt_demo2\n\n  patient_id heart_condition activity age_years heart_rate_bpm spo2_percent\n1        P01         Healthy     Rest      52.4             73         98.9\n2        P02  HeartCondition Moderate      73.1             88         93.4\n3        P03  HeartCondition     Rest      61.2             73         95.8\n4        P04  HeartCondition Moderate      36.0             90         94.0\n5        P05         Healthy     Rest      53.7             72         98.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#decisions",
    "href": "introduction_AI.html#decisions",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.5 Decisions",
    "text": "1.5 Decisions\nIn an analogous manner we will introduce the idea of decision using first an intuitive example. Consider Figure 1.2 that depicts a crossroad in which some different routes can be chosen when someone is travelling and stopping at the stop signal. After the STOP one can turn to the left, turn to the right, go ahead, reverse and come back, etc.\nThe case in the Figure Figure 1.2 brings to our mind some core notions. First we have a state of ambiguity between different routes, each of these routes will bring us to different places, when reaching the crossroad we may know exactly or not the outcomes of each of the routes, etc. Generally speaking we say that we have some ambiguity in the courses of action we can take, because at a single moment we can take only one of them and not all of them at the same time, we have different alternatives of choice, each of them leading to a consequence regarding our journey.\n\n\n\n\n\n\nFigure 1.2: The crossroad for decision learning.\n\n\n\nFormalizing the ideas discussed in the previous paragraph we can say, following (White 1970), that a decision is a cognitive process \\(\\theta\\), clearly identifiable, that resolves a state of ambiguity \\(Q\\) within a state of knowledge \\(K\\), such that \\((Q, K) \\rightarrow q \\in Q\\), where \\(q\\) is a possible alternative within the state of ambiguity. Formalizing the ideas illustrated in the crossroads example, we can express the act of choosing a route as a decision process in White’s sense. In the case of the crossroads, \\(Q\\) corresponds to the set of possible paths available to the traveler, and \\(K\\) represents the traveler’s knowledge or expectations about the consequences of each path perhaps partial, uncertain, or even incorrect. The cognitive process \\(\\theta\\) is the mechanism by which the traveler evaluates this information and selects one route, \\(q\\), from among the available alternatives. Thus, the simple act of deciding which way to go operationalize the theoretical construct of decision-making: resolving ambiguity under a given state of knowledge to produce a concrete action.\nExamples in health care and advanced therapeutic technology include:\n\nSelect a treatment strategy for a patient using clinical history, imaging, genetics, and comorbidities.\nDesign and optimize clinical trials (inclusion criteria, dose/regimen, endpoints, adaptive rules).\nChoose a therapeutic modality (gene therapy, cell therapy, device, pharmacological intervention) for a given indication.\nInitiate/continue/discontinue therapy based on longitudinal biomarkers, response, and adverse events.\nIntegrate multi-omics evidence (genomics, proteomics, metabolomics) to guide precision medicine.\nAllocate scarce resources (ICU beds, organs, specialized equipment) under ethical and operational constraints.\nAssess risk–benefit and cost-effectiveness for adopting a new drug, device, or regenerative technology.\nDetermine optimal dosing/delivery via PK/PD modeling, model-informed precision dosing, or digital-twin simulation.\nPrioritize R&D pipelines based on clinical potential, unmet need, feasibility, and budget impact.\nEvaluate AI decision-support systems for safety, interpretability, bias/robustness, and regulatory compliance.\n\nDifferent theories approach decisions via different methods but can be broadly aggregated in the three main classes depending on how the \\(\\theta\\) process is addressed Bell, Raiffa, and Tversky (1988).\n\n\n\nTable 1.2: Types of approaches to the decision-making process (adapted from Bell, Raiffa, and Tversky (1988).\n\n\n\n\n\n\n\n\n\n\n\nCharacteristics\nNormative Models\nDescriptive Models\nPrescriptive Models\n\n\n\n\nDecision approach (\\(\\theta\\))\nAxiomatic, based on rational principles and expected utility theory.\nDescribe how the decision process actually occurs.\nFocus on how to decide well given cognitive and practical constraints.\n\n\nGeneral dynamics\nHow people should ideally make decisions.\nHow people actually make decisions, including cognitive limitations.\nHow people can act to make better decisions.\n\n\nAreas of use and examples\nMicroeconomics (expected utility paradigm), contingent state insurance.\nCognitive psychology (heuristics, biases, bounded rationality).\nEngineering, risk and uncertainty management, operations research.\n\n\n\n\n\n\nFrom the different approaches proposed in Table 1.2 we will give a particular focus on the prescriptive approaches specifically in one named Decision Analysis.\n\n1.5.1 Decision Analysis\nDecision analysis is a science that develop methods allowing the application of results from decision theory and elements from normative, prescriptive, and descriptive models to the solution of practical problems (Howard 1988). The main objective of this tool is to support the construction of sound decisions, given the cognitive and contextual realities of decision makers. In this sense, the field can also be referred to as decision engineering Sarti (2013).\n\nWhat exactly is the decision problem, and how should it be modeled?\n\nWhat are the possible alternatives, and how can they be identified?\n\nWhich variables should be included in the analysis?\n\nWhat level of detail should the analysis consider?\n\nHow much should be invested, and what is the nature of the information to be used?\n\nHow can the uncertainties involved be modeled?\n\nHow can the decision problem be clearly communicated?\n\nHow can the principles of decision theory be applied to the resolution of practical problems?\n\nHoward (1988) and Sarti (2013) describe the concept of a decision basis as a tool for addressing the questions outlined above and illustrated in Figure . The concept consists of a set of information, denoted as \\(BD\\), in which the alternatives, consequences, and uncertainties or risks associated with a decision are made explicit. The decision basis can be implemented through tables, graphical parameters, or, more formally, by means of influence diagrams and decision trees. The construction of the decision basis should be an iterative process, carried out until sufficient information has been gathered to support a decision. The dynamics of formulating the decision basis and the agents involved in the process are summarized in Figure 1.3. In this figure, we can observe that the process of analysis and construction of the decision basis begins with a state of obscurity regarding the problem.\nFrom that point, the technical experts, the analyst, and the decision makers act in an integrated manner, complementing their expertise with the available literature related to the issue.\nThrough this exchange of knowledge emerges the foundation for building the decision basis, which makes explicit the alternatives, consequences, preferences of those involved for each alternative, and the associated uncertainties. The process should be repeated iteratively until a decision is reached.\n\n\n\n\n\n\nFigure 1.3: Decision Basis\n\n\n\nSpetzler, Winter, and Meyer (2016) further develops the notion of decision basis proposing a whole cycle of decision quality as implemented via six elements: 1) Appropriate framing, 2) Creative Alternatives, 3) Relevant and Reliable Information, 4) Clear Values and Trade Offs, 5) Sound Reasoning and 6) Commitment to Action. Going through these six steps when approaching a decision helps us to avoid common pitfalls and biases of judgement.\nStep 3 in the last sequence involves the usage of relevant and reliable information, implying in the need for data and good representations of data, therefore, good modelling strategies, for achieving good quality decisions. This step is the conceptual motivation of models as decision support tools and main practical motivation for our course.\n\n\n\n\n\n\n\n\nFigure 1.4: Decision Quality elements adapted from Spetzler, Winter & Meyer (2016).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#defining-ml-and-ai",
    "href": "introduction_AI.html#defining-ml-and-ai",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.6 Defining ML and AI",
    "text": "1.6 Defining ML and AI\nIn the last sections of this chapter we had motivated and illustrated the reasons why we need to dedicate our time to learn how to create models or representations of data. We learnt that models are important information providers being key for the quality of decision making. In this section, we will define what is Intelligence, Artificial Intelligence and Machine Learning and show how these techniques can classified regarding the tasks they realize and much more interesting things.\n\n1.6.1 Intelligence\n\n1.6.1.1 Etymological and philosophical perspectives\nEtymologically, the word intelligence derives from the Latin intelligere, meaning “the capacity to comprehend or understand,” formed from inter (“between”) and legere (“to read” or “to choose”). Philosophically, intelligence has often been linked to the capacity for apprehending truth, understood as the correspondence or coincidence between facts and ideas.\nAn interesting nuance in this philosophical view is that intelligence and thought are not identical. We often think without necessarily judging whether something is true or false. Thought can operate freely in imagination or association, whereas intelligence involves an orientation toward truth and understanding. This distinction also separates intelligence from mere skills or procedural abilities, philosophically speaking intelligence implies discernment, not only execution.\n\n\n1.6.1.2 Computer science perspective\nFrom a computational standpoint, machines do not possess intelligence in the philosophical sense. Computers are fundamentally symbol-manipulating systems: they operate on data according to predefined rules, algorithms, and formal structures, without awareness or access to meaning. What we call machine intelligence (as in Artificial Intelligence and Machine Learning) refers to the ability of systems to perform tasks that, for humans, require cognitive effort such as recognizing patterns, making predictions, or generating language. However, these systems do not understand or apprehend truth; they only evaluate consistency or likelihood within the frameworks they are programmed with or have learned statistically.\nIn formal logic or mathematics, a computer can determine whether a statement is derivable or satisfiable according to given axioms but not whether it is “true” in an absolute or semantic sense. In empirical contexts, it can approximate “truth” through probabilistic inference based on data, but that remains a syntactic or statistical notion of truth, not a semantic or philosophical one. Thus, from a computer science perspective, truth is always relative to a system of rules, models, or data, while human intelligence seeks truth as correspondence between thought and reality.\n\n\n1.6.1.3 Artificial Intelligence\n\n1.6.1.3.1 Definition and Scope\nArtificial Intelligence (AI) can be broadly defined as the field of study and practice dedicated to creating systems capable of performing tasks that, if carried out by humans, would require cognitive processes. This includes perception, reasoning, learning, language understanding, decision-making, and problem-solving. AI is not a single technology but a multidisciplinary domain that integrates computer science, mathematics, cognitive psychology, statistics, linguistics, and philosophy. In practical terms, AI systems operate by processing inputs (data) through computational models designed to produce outputs that emulate aspects of human cognition for instance, recognizing an object in an image, predicting the outcome of a treatment, or generating coherent text.\n\n\n\nRelationship between AI and Machine Learning\n\n\nClassic examples of AI applications include:\n\nExpert systems that replicate human decision-making in domains such as medicine or engineering;\nSpeech and image recognition systems such as digital assistants or facial recognition software;\nAutonomous systems such as self-driving vehicles or robotic surgery;\nNatural language processing applications such as machine translation, chatbots, and summarization tools.\n\n\n\n\n1.6.1.4 Machine Learning\nMachine Learning (ML) is a core subfield of AI focused on building models that learn from data. Instead of being explicitly programmed with step-by-step rules, ML methods discover patterns, relationships, or functions from examples, allowing them to make predictions, classifications, or decisions when confronted with new data. The building block of machine learning methods are algorithms that consist in sets of rules and steps that method needs to do to performe a task. In short we can also say that machine learning is a context in which a machine learns to perform a task in the presence of data and improves its performance the more data we offer.\n\n\n\nTypes of Machine Learning\n\n\nIn a typical ML workflow, a model is trained on a dataset where both inputs and desired outputs are known (the training phase), and its performance is later evaluated on unseen data (the testing phase). Through iterative optimization, the model adjusts internal parameters to minimize prediction errors.\nExamples of ML applications include:\n\nPredicting disease risk from medical records;\nClassifying tumor images as benign or malignant;\nRecommending personalized treatments or products;\nForecasting energy demand or environmental variables;\nDetecting fraud in financial transactions.\n\nML methods can be supervised (trained with labeled outcomes), unsupervised (discovering structure without labels, e.g., clustering), or reinforcement-based (learning by trial and reward) ?fig-ml_classes. We will discuss what these terms mean when detailing the anatomy of a ML method in a future section.\n\n\n1.6.1.5 Predictive and Generative AI\nThe recent expansion of AI can be viewed through two complementary paradigms: predictive AI and generative AI.\nPredictive AI focuses on forecasting outcomes or estimating probabilities based on existing data. It aims to answer questions such as “What will happen next?” or “What is the most likely class or value?”. In our course predictive AI will be sometimes referred merely as Machine Learning and vice versa.\nExamples:\n\nPredicting patient readmission risk in healthcare;\nForecasting market behavior or demand in economics;\nAnticipating equipment failure in industrial systems.\n\nPredictive models rely on statistical and ML techniques (e.g., regression, decision trees, ensemble methods, neural networks) to generalize patterns from past data to future cases.\nGenerative AI, in contrast, focuses on creating new data that resemble observed patterns. These systems learn the underlying structure of their training data and then generate plausible new instances, such as text, images, code, or molecular structures.\nExamples:\n\nLarge Language Models (LLMs) that produce human-like text;\nGenerative Adversarial Networks (GANs) that create realistic images;\nVariational Autoencoders (VAEs) that synthesize complex data distributions;\nText-to-image models used for design, art, and simulation.\n\nGenerative AI represents a major shift from systems that merely predict to systems that can synthesize and imagine, extending AI from decision support to creative collaboration.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#bridging-predictive-and-generative-paradigms",
    "href": "introduction_AI.html#bridging-predictive-and-generative-paradigms",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.7 Bridging Predictive and Generative Paradigms",
    "text": "1.7 Bridging Predictive and Generative Paradigms\nWhile predictive AI is predominantly analytical, generative AI is synthetic. In practice, modern intelligent systems often integrate both: generative models can simulate scenarios for decision-making, while predictive models can assess the likelihood or impact of those simulated outcomes.\nIn the context of Advanced Therapeutic Technologies (ATT) and health sciences, this convergence allows AI not only to anticipate risks and outcomes but also to propose new molecules, optimize treatment protocols, and design personalized therapeutic strategies.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#more-on-models-for-data-and-anatomy-of-an-ai-method",
    "href": "introduction_AI.html#more-on-models-for-data-and-anatomy-of-an-ai-method",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.8 More on Models for data and anatomy of an AI method",
    "text": "1.8 More on Models for data and anatomy of an AI method\nWe will use again the dataset in Table 1.1 to illustrate a general scheme we will use for the rest of the book and to introduce some other key terms and jargon.\n\n\n  patient_id heart_condition activity age_years heart_rate_bpm spo2_percent\n1        P01         Healthy     Rest      52.4             73         98.9\n2        P02  HeartCondition Moderate      73.1             88         93.4\n3        P03  HeartCondition     Rest      61.2             73         95.8\n4        P04  HeartCondition Moderate      36.0             90         94.0\n5        P05         Healthy     Rest      53.7             72         98.0\n\n\nEach row in a dataset is called an instance, and each column represents a feature (or variable). Depending on the goal of the analysis, some features can be used as predictors (inputs), or explanatory variables, and others as responses (outputs) or responses. Sometimes we may refer to a variable as a label, the known value of the output (response) that we want a model to predict.\nOur dataset example can be viewed from a modelers point of view line in Figure 1.5.\n\n\n\n\n\n\nFigure 1.5: The way modellers see a dataset!\n\n\n\nIn the case each Y in Figure 1.5 is modelled alone we have an uni-response model and in the case they are modeled simultaneously we say it is multi-response.\nMachine learning tasks are classified in supervised, unsupervised, reinforcement and generative tasks. All methods share a general anatomy consisted of the data and motivational context, the task the algorithms perform, and measures of performance. Whenever studying a method we will use always describe it in this context of data, algorithm and performance.\n\n\n\n\n\n\nFigure 1.6: Anatomy of Machine Learning. All methods require data to be implemented, different algorithms as function of the kind of task. All methods need to have their performance evaluated via metrics.\n\n\n\nThe features are also classified into\n\n\n\n\n\n\n\n\n\nFeature Type\nDefinition\nExample from Model\nExplanation\n\n\n\n\nCategorical\nVariables that represent distinct groups or classes with no inherent order.\ncondition = {Healthy, HeartCondition}; activity = {Rest, Moderate}\nThese describe qualitative differences between individuals. One level is not “more” or “less” than another only different. Encoded typically as dummy variables (0/1).\n\n\nOrdinal\nVariables that have a clear, ranked order, but where differences between levels are not necessarily uniform.\ne.g., if activity were coded as {Rest &lt; Moderate &lt; Vigorous}\nHere, the order matters “Moderate” implies higher exertion than “Rest” but the distance between levels is undefined. These often appear in surveys or clinical scoring scales (e.g., pain level: mild &lt; moderate &lt; severe).\n\n\nContinuous\nVariables measured on a numeric scale with meaningful and consistent intervals.\nage (in years)\nThe differences are measurable: a 10-year increase has the same quantitative meaning regardless of where it occurs (e.g., 40→50 vs 60→70). Continuous variables allow arithmetic operations and smooth modeling (e.g., linear effects).\n\n\n\nIn the simulated dataset:\n\nheart_condition and activity are categorical.\nage_years is continuous.\nIf an ordered variable (like “disease severity” or “activity level”) were added, it would be ordinal.\n\n\n1.8.0.0.1 Supervised Learning\nUses labeled data: inputs (features) + known output (label).\n\nGoal: learn a mapping from features → label.\nExample: Predict heart_condition (Healthy vs HeartCondition) from activity, age_years, heart_rate_bpm, spo2_percent\nTypical algorithms: Logistic regression, random forest, SVM, neural networks\nOutput: class label or numeric value (classification/regression).\n\n\n\n1.8.0.0.2 Unsupervised Learning\nUses unlabeled data: only features, no target.\n\nGoal: find structure/patterns (clusters, components).\n\nExample: Cluster patients by heart_rate_bpm, spo2_percent, age_years to reveal low/high-risk profiles without using heart_condition.\n\nTypical algorithms: k-means, hierarchical clustering, Gaussian mixtures, PCA.\n\nOutput: groupings, embeddings, or density estimates.\n\n\n\n1.8.0.0.3 Reinforcement Learning\nAn agent interacts with an environment and learns via rewards/penalties over time.\n\nGoal: learn a policy that maximizes long-term reward.\n\nExample: A wearable adjusts activity level to keep spo2_percent and heart_rate_bpm in range; rewards for healthy ranges, penalties otherwise.\n\nTypical algorithms: Q-learning, DQN, policy gradients.\n\nOutput: policy for sequential decisions.\n\n\n\n1.8.0.0.4 Generative Learning\nLearns the data distribution to create new, realistic samples.\n\nGoal: model (p()) and generate synthetic data.\n\nExample: Create synthetic patient records with plausible heart_rate_bpm/spo2_percent/activity combinations for testing or augmentation.\n\nTypical algorithms: VAEs, GANs, transformer-based models.\n\nOutput: new samples; also used for simulation, augmentation, imputation.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#introduction-to-model-comparison",
    "href": "introduction_AI.html#introduction-to-model-comparison",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.9 Introduction to model comparison",
    "text": "1.9 Introduction to model comparison\nSo far we have learnt that models are representations of a given reality, which in AI contexts, relate to supervised, unsupervised, reinforcement and generative tasks. Intuitively, we also learnt from Figure 1.1 that for a same given reality we can have different models. In this section we will address the matter of how to compare the different possible machine learning methods for the same situation.\nIn our efforts, we will consider that the best model is the one that most accurately learns and represents the reality it is intended to approximate. To assess this, we rely on performance metrics that quantify how closely a model’s outputs align with the true or expected outcomes.\nIn supervised learning, these metrics measure the discrepancy between predicted and observed values  for instance we use concepets like accuracy, precision–recall, ROC-AUC for classification, or RMSE and MAE for regression. In unsupervised learning, where there are no predefined labels, we instead evaluate how well the model captures underlying structure, using metrics such as silhouette score, cluster purity, or reconstruction error.\nIn reinforcement learning, performance is gauged by the cumulative reward obtained through interaction with the environment, reflecting how well the agent learns optimal decisions over time.\nFinally, in generative modeling, evaluation focuses on fidelity, diversity, and realism of the generated data, often through metrics like FID (Fréchet Inception Distance), likelihood, or domain-specific validation.\nIn the next chapters when discussing the different methods we will always refer to metrics of model performance and how they could be used to select best models for a given situation.\nWe also can compare models regarding they usage for inference and for answering research questions.\nIn general we should advocate for a balance between predictive performance and interpretability x inference.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#understanding-the-aiml-predictive-pipeline",
    "href": "introduction_AI.html#understanding-the-aiml-predictive-pipeline",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.10 Understanding the AI/ML Predictive Pipeline",
    "text": "1.10 Understanding the AI/ML Predictive Pipeline\nFigure 1.7 illustrates a simplified version of the predictive pipeline commonly used in Artificial Intelligence and Machine Learning. Although implementations may vary across domains and algorithms, most supervised learning workflows follow the same fundamental structure: a training stage, in which the model learns patterns from data, and a deployment stage, in which the trained model is applied to new cases.\n\n1.10.1 Training Phase\nThe process begins with a dataset containing observations of several variables. In the example, each row corresponds to a patient and includes both:\nPredictors (or features): such as heart_condition, activity, and age_years\nResponses (or labels): such as heart_rate_bpm and spo2_percent\nTo evaluate how well a model can generalize, the dataset is divided into two distinct subsets:\nTraining set\n\nThis subset is used to fit the model. It provides the examples from which the algorithm learns the relationship between predictors and responses. During this stage, the model adjusts its internal parameters to capture regularities in the training data.\nTest set\n\nThis subset is intentionally withheld during model fitting. It serves as an independent benchmark to assess the model’s performance on new, unseen data. By comparing predicted values with the true responses in the test set, we obtain an estimate of the model’s ability to generalize beyond the data from which it learned.\nThis separation between training and testing is essential; without it, the evaluation would be overly optimistic and would fail to indicate whether the model can perform reliably outside the initial dataset.\nOnce a model has been trained and its performance has been verified, it can be deployed. Deployment refers to the use of the model in real or simulated operational settings, where it is asked to make predictions for entirely new instances.\nIn the example, new patients (P06 to P100) arrive with predictor information (e.g., heart condition, activity, age), but their outcomes are unknown. The trained model processes these inputs and generates predictions for variables such as expected heart rate. These predictions illustrate the core function of machine learning models: using previously learned structure to inform decision-making in new contexts.\n\n\n\n\n\n\nFigure 1.7: The pipeline of developing AI/ML solutions\n\n\n\nIf well trained a models works well in new unseen data in other terms this models generalizes well.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#a-very-short-introduction-to-model-bias-overfitting-and-crossvalidation",
    "href": "introduction_AI.html#a-very-short-introduction-to-model-bias-overfitting-and-crossvalidation",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.11 A very short introduction to model bias, overfitting and crossvalidation",
    "text": "1.11 A very short introduction to model bias, overfitting and crossvalidation\nWhen we build a model, the goal is to approximate reality well enough to make useful predictions and inferences. Models must generalize from the data they see to new data they have not seen. Three core ideas help us judge this: bias, variance/overfitting, and cross-validation.\n\n1.11.1 Model bias\nBias is the systematic difference between a model’s predictions and the true values. High-bias models make strong simplifying assumptions and typically under fit the data, missing important structure, see Figure 1.9.\nExample: predicting heart rate using only age while ignoring activity level and heart condition is too rigid, so real patterns are missed.\n\n\n1.11.2 Variance and overfitting\nVariance is a model’s sensitivity to the specific training set. High-variance models are very flexible; they fit random noise as if it were signal and thus overfit great training performance, poor test performance.\nExample: a highly tuned model that perfectly matches small fluctuations in five patients’ heart-rate/SpO₂ readings but fails on a sixth.\n\n\n\n\n\n\nFigure 1.8\n\n\n\n\n\n1.11.3 Cross-validation, training and test datasets\nCross-validation estimates how well a model will generalize. Instead of training and testing on the same data, we split the dataset into several folds. We train on some folds and test on the remaining fold, then rotate until every fold has served as a test set.\nCommon choices:\n\nk-fold cross-validation: split into k parts (for example, k = 5). Average the metric across folds for a stable estimate.\nLeave-one-out (LOOCV): useful for very small datasets; train on all but one observation and test on the left-out one, repeating for all observations.\n\n\n\n\n1.11.4 Importance of cross-validating\nBias, overfitting, and cross-validation address the same question: is the model learning meaningful signal rather than noise? A good model balances flexibility (to capture structure) with restraint (to avoid noise). Cross-validation provides an honest estimate of out-of-sample performance before using models to inform real decisions in clinical, laboratory, or policy settings.\n\n\n\n\n\n\n\n\nFigure 1.9: Three situations side-by-side: Good fit (well-tuned), High bias (underfit), and Overfit (too complex). The dashed line is the true signal.\n\n\n\n\n\n\n\n1.11.5 Causal and non causal machine learning\nMachine learning models can be grouped according to how they treat the data-generating process (DGP). The main difference between causal and non-causal approaches lies in whether the model tries to represent how the data were produced or only what patterns appear in them.\n\n1.11.5.1 Non-causal (Associational) Machine Learning\nNon-causal machine learning assumes that the available data contain stable relationships between variables but does not try to represent the mechanisms that produced those data. Algorithms such as linear regression, random forest, and neural networks are trained to find statistical associations that best predict an outcome (Y) from a set of inputs (X):\n[ = f(X) ]\nThe goal is to minimize prediction error. However, the model ignores the process that generated (X) and (Y). If the environment changes, for example when an intervention or policy modifies the DGP, these models may perform poorly. This situation is known as a distributional shift or a non-stationarity problem.\n\n\n1.11.5.2 Causal Machine Learning\nCausal machine learning explicitly represents the mechanisms that generate the data. It seeks to estimate how outcomes would change if a variable were intervened upon, moving from correlation toward causal inference. This requires assumptions about the DGP, which are often expressed using a causal graph or a structural causal model (SCM):\n[ Y = f(X, U) ]\nwhere (U) represents unobserved causes. Causal machine learning answers questions such as: “What would happen to (Y) if (X) were changed while keeping everything else constant?”\n\n\n\n\n\n\n\n\nAspect\nNon-causal ML\nCausal ML\n\n\n\n\nMain goal\nAccurate prediction\nEstimation of causal effects\n\n\nData used\nObserved variables\nObserved and counterfactual variables\n\n\nDGP considered\nImplicit or ignored\nExplicitly modeled\n\n\nOutput\n( = f(X) )\nEffect of (X) on (Y)\n\n\nLimitations\nSensitive to confounding and bias\nRequires strong causal assumptions such as no hidden confounders\n\n\n\n\n\n1.11.5.3 Associational pattern without explicit DGP\n\n\n\n\n\nflowchart LR\n  X[X features] --&gt; Y[Outcome Y]\n  style X fill:#e8f1ff,stroke:#2f63c0\n  style Y fill:#e8f1ff,stroke:#2f63c0\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\nZ[Confounder Z] --&gt; X[Exposure X]\nZ --&gt; Y[Outcome Y]\nX --&gt; Y\nstyle X fill:#e8f1ff,stroke:#2f63c0\nstyle Y fill:#e8f1ff,stroke:#2f63c0\nstyle Z fill:#fff4d6,stroke:#c0902f\n\n\n\n\n\n\nThe first diagram at the top represents a purely associational pattern.\nIt shows that we can find a statistical relationship between a set of predictors ( X ) (or features) and an outcome ( Y ), even without knowing how these data were generated.\nThis is the standard setup of non-causal machine learning, where the goal is to learn a predictive mapping:\n[ = f(X) ]\nSuch models can predict ( Y ) given ( X ), but they do not explain why the relationship exists.\nIf the data-generating process changes (for example, after a new treatment policy or environmental shift), the learned pattern might no longer hold.\nThe second diagram introduces a causal structure with a confounder ( Z ) that affects both the exposure ( X ) and the outcome ( Y ).\nThis setup represents a situation where the association between ( X ) and ( Y ) is partly due to a third variable ( Z ) that influences both.\nTo correctly identify the effect of ( X ) on ( Y ), we must account for ( Z ).\nIf ( Z ) is not measured or controlled for, the observed relationship between ( X ) and ( Y ) can be biased this is known as confounding.\nInterpretation.\nTo estimate the true causal effect of ( X ) on ( Y ), the confounder ( Z ) must be included in the model.\nIgnoring ( Z ) can lead to misleading conclusions about the direction or strength of the effect.\nTakeaway.\n- Non-causal machine learning captures what tends to occur together.\n- Causal machine learning aims to estimate what would happen if we changed something, considering the data-generating process and possible confounders.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#coding-and-version-control",
    "href": "introduction_AI.html#coding-and-version-control",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.12 Coding and Version Control",
    "text": "1.12 Coding and Version Control\nIn other to perform tasks machine learning algorithms and methods need to be implemented in computers. To communicate to the machine what it is necessary to do we use programming languages in the form of code. Below in Table 1.3 we have an example of code that creates a vector of numbers, calculates the averages of such numbers, prints a message and creates a simulated data frame.\n\n\n\nTable 1.3: A tiny example of coding\n\n\n# 1) Create a small numeric vector\nx &lt;- c(72, 75, 70, 78, 74)\n\n# 2) Compute a simple summary\navg &lt;- mean(x)\n\n# 3) Print a message that mixes text and a computed value\ncat(\"The average value is:\", avg, \"\\n\")\n\nThe average value is: 73.8 \n\n# 4) Make a tiny data frame and show the first rows\ndf &lt;- data.frame(id = 1:5, value = x)\nhead(df)\n\n  id value\n1  1    72\n2  2    75\n3  3    70\n4  4    78\n5  5    74\n\n\n\n\nWhen creating code we usually start with an initial version which often is modified or adapted, created by a single individual or by several people’s contributions which are using the same machines or not, in the same place or not among other details. These dynamics bring the need for mechanism that facilitate the control of code version, allowing for remote contributions and collaborative creation of code which are very well implemented in version control technologies like Git.\nGit is a version control system (VCS) designed to track changes in code and other text files over time. It allows developers to record, compare, and revert modifications, ensuring that every update is safely stored and that collaboration does not overwrite previous work. Each version of your project is stored as a commit, which captures the exact state of all tracked files at a given point in time, along with a descriptive message about what changed.\nWhile Git works locally on your computer, platforms such as GitHub, GitLab, and Bitbucket provide remote repositories, online spaces that store your project’s history, enable backup, and make team collaboration possible from anywhere.\nWe recommend the readers to set up a github account, install git in their system and integrate git with R and Rstudio.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#set-up-git-github-for-rrstudio-workflows",
    "href": "introduction_AI.html#set-up-git-github-for-rrstudio-workflows",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.13 Set up Git & GitHub (for R/RStudio workflows)",
    "text": "1.13 Set up Git & GitHub (for R/RStudio workflows)\nWe recommend that readers set up a GitHub account, install Git, and integrate it with R and RStudio to make their workflows versioned, collaborative, and reproducible.\n\n1.13.1 1) Create a GitHub account\n\nGo to https://github.com and create a free account.\nPick a concise username (becomes part of your repo URLs).\n(Optional) Enable 2FA: Settings → Password and authentication.\nLearn key concepts: repository, commit, branch, pull request.\n\n\n\n1.13.2 2) Install Git\nCheck if Git is already available:\ngit --version\nIf not, install it for your OS:\nWindows\n\nDownload and install: https://git-scm.com/download/win\n(or via Winget):\nwinget install --id Git.Git -e\n\nmacOS\n\nEasiest (Apple CLT):\nxcode-select --install\n(Optional) With Homebrew:\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nbrew install git\n\nLinux (Debian/Ubuntu)\nsudo apt update\nsudo apt install -y git\n\n\n1.13.3 3) Configure Git (one-time)\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"you@example.com\"\ngit config --global init.defaultBranch main\n# Optional:\ngit config --global pull.rebase false   # use merge on pull\ngit config --global core.autocrlf input # good default on macOS/Linux (use \"true\" on Windows)\n\n\n1.13.4 4) Authenticate with GitHub (Personal Access Token)\nFrom R (recommended):\ninstall.packages(c(\"usethis\",\"gitcreds\"))\nusethis::create_github_token()  # opens browser to create a PAT (scopes: repo, read:org, workflow)\ngitcreds::gitcreds_set()        # paste the token once\ngitcreds::gitcreds_get()        # verify stored token\nTip: If you’re prompted for a “password” during git push, paste the PAT (not your GitHub password).\n\n\n1.13.5 5) Enable Git in RStudio\n\nRStudio → Tools → Global Options → Git/SVN\n\nEnsure Git executable is detected (browse to it if needed).\nTick Enable version control interface.\n\nClick Apply (restart RStudio if prompted).\n\n\n\n1.13.6 6) Create a repo and push to GitHub\nOption A via RStudio UI\n\nFile → New Project → choose a directory → tick Create a git repository.\nMake a change → Git tab → Commit → Push.\n\nOption B via terminal (works on both macOS and Windows)\n# Run inside your project folder\ngit init\ngit add .\ngit commit -m \"Initial commit\"\ngit branch -M main\ngit remote add origin https://github.com/&lt;youruser&gt;/&lt;repo&gt;.git\ngit push -u origin main\n\n\n1.13.7 7) Daily workflow\ngit pull                      # get others' changes\ngit add .                     # stage your changes\ngit commit -m \"Meaningful message\"\ngit push                      # publish your work\n\n\n1.13.8 8) (Optional) Use SSH instead of HTTPS\nGenerate a key and add it to GitHub:\nmacOS/Linux\nssh-keygen -t ed25519 -C \"you@example.com\"\ncat ~/.ssh/id_ed25519.pub\nWindows (PowerShell)\nssh-keygen -t ed25519 -C \"you@example.com\"\ntype $env:USERPROFILE\\.ssh\\id_ed25519.pub\n\nCopy the .pub key → GitHub → Settings → SSH and GPG keys → New SSH key.\nPoint your repo to SSH:\n\ngit remote set-url origin git@github.com:&lt;youruser&gt;/&lt;repo&gt;.git\n\n\n1.13.9 9) Quick fixes\n\n“git: command not found” (macOS) → run xcode-select --install or brew install git.\nRStudio doesn’t see Git → set Git path in Tools → Global Options → Git/SVN.\nDivergent branches on pull → choose one:\ngit pull --no-rebase   # merge (default if configured)\n# or\ngit pull --rebase      # rebase to keep linear history\nToken expired → in R:\ngitcreds::gitcreds_set()\nWrong email in commits → update config and amend latest commit:\ngit config --global user.email \"you@newmail.com\"\ngit commit --amend --reset-author\ngit push --force-with-lease\n\n\n\n1.13.10 A minimal portfolio structure for data professionals\nEvery craft needs the right setup: carpenters need a bench and tools; data professionals need a basic, reliable workspace. If you’re starting in AI/ML and data science, invest early in a lean, well-organized portfolio. The simplest and most effective format is a set of GitHub repositories that showcase your work each with clear documentation, runnable code, and reproducible results. Over time, this portfolio becomes your toolkit and your calling card: easy to share, easy to maintain, and easy for others to trust.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "introduction_AI.html#reproducibility",
    "href": "introduction_AI.html#reproducibility",
    "title": "1  Introduction to AI and Machine Learning",
    "section": "1.14 Reproducibility",
    "text": "1.14 Reproducibility\n\nAs discussed in the previous sections of this chaoter, Data are our evidence about the world; models turn that evidence into information for decision-making. Most AI and ML projects follow the same steps: collect and organize data, build a model, evaluate performance and communicate results. Across all these stages, one principle matters above all: reproducibility, defined as the ability for others (or your future self) to repeat the process and obtain the same results for learning, auditing, or verification.\nReproducibility is best achieved with a minimal, transparent workflow: make data (when permissible) and code available so that someone else can rerun the analysis and arrive at the same outputs. Version control (e.g., Git) is essential here. Whenever data governance, intellectual property, and legal constraints allow, we recommend sharing both data and code to support reproducible science.\nIn health applications, reproducible analyses also underpin evidence-based regulation. National authorities issue guidelines for approving drugs, procedures, and other therapeutic technologies, typically grounded in literature and empirical evidence. Researchers and developers are therefore frequently called upon to produce transparent, well-documented analyses that regulatory boards can evaluate and trust.\n\n\n\n\nBell, David E., Howard Raiffa, and Amos Tversky, eds. 1988. Decision Making: Descriptive, Normative, and Prescriptive Interactions. Cambridge: Cambridge University Press. https://www.cambridge.org/core/books/decision-making/D29E12748C31B85FE83C9CEDEE9D4D88.\n\n\nHasin, Yoram, Michael Seldin, and Aldons Lusis. 2017. “Multi-Omics Approaches to Disease.” Genome Biology 18: 83. https://doi.org/10.1186/s13059-017-1215-1.\n\n\nHilbert, Martin, and Priscila López. 2011. “The World’s Technological Capacity to Store, Communicate, and Compute Information.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nHoward, Ronald A. 1988. “Decision Analysis: Practice and Promise.” Management Science 34 (6): 679–95.\n\n\nHripcsak, George, and David J. Albers. 2013. “Next-Generation Phenotyping of Electronic Health Records.” Journal of the American Medical Informatics Association 20 (1): 117–21. https://doi.org/10.1136/amiajnl-2012-001145.\n\n\nJensen, Peter B., Lars J. Jensen, and Søren Brunak. 2012. “Mining Electronic Health Records: Towards Better Research Applications and Clinical Care.” Nature Reviews Genetics 13 (6): 395–405. https://doi.org/10.1038/nrg3208.\n\n\nKarczewski, Konrad J., and Michael P. Snyder. 2018. “Integrative Omics for Health and Disease.” Nature Reviews Genetics 19: 299–310. https://doi.org/10.1038/nrg.2018.4.\n\n\nRaghupathi, Wullianallur, and Viju Raghupathi. 2014. “Big Data Analytics in Healthcare: Promise and Potential.” Health Information Science and Systems 2 (3): 1–10. https://doi.org/10.1186/2047-2501-2-3.\n\n\nSarti, Danilo Augusto. 2013. “Uncertainty Management Through Decision Analysis: Applications to Production Optimization and Uncertain Demands.” PhD thesis, Universidade de São Paulo.\n\n\nSpetzler, Carl, Hannah Winter, and Jennifer Meyer. 2016. Decision Quality: Value Creation from Better Business Decisions. Hoboken, NJ: John Wiley & Sons.\n\n\nStephens, Zachary D., Skylar Y. Lee, Faraz Faghri, R. Ilkayeva Campbell, et al. 2015. “Big Data: Astronomical or Genomical?” PLOS Biology 13 (7): e1002195. https://doi.org/10.1371/journal.pbio.1002195.\n\n\nTopol, Eric J. 2019. Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again. New York: Basic Books.\n\n\nWhite, Douglas John. 1970. Decision Theory. New Brunswick, NJ: Routledge / Transaction Publishers. https://www.routledge.com/Decision-Theory/White/p/book/9780202308982.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to AI and Machine Learning</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html",
    "href": "supervised_regression.html",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "",
    "text": "2.1 Setting up R\nRun this code in your R studio to make sure you have all the packages required installed!\n# Packages needed in this section\nreq_pkgs &lt;- c(\n  \"dplyr\",      # data wrangling\n  \"ggplot2\",    # plotting\n  \"tidyr\",      # tidying\n  \"readr\",      # read/write csv\n  \"tibble\",     # tibbles/printing\n  \"gridExtra\",  # simple plot grids\n  \"emmeans\",    # adjusted means / contrasts\n  \"effsize\",     # effect sizes (Cohen's d, Cliff's delta)\n \"DataExplorer\" ,#for missingness\n  \"glmnet\",\n \"lmtest\" ,# for checking the \n \"pROC\"\n )\n\n# Install any that are missing\nto_install &lt;- setdiff(req_pkgs, rownames(installed.packages()))\nif (length(to_install) &gt; 0) {\n  install.packages(to_install, dependencies = TRUE)\n}\n# Load all (silently)\ninvisible(lapply(req_pkgs, require, character.only = TRUE))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#recall",
    "href": "supervised_regression.html#recall",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.2 Recall",
    "text": "2.2 Recall\nIn the introduction to AI section we learnt that models are representations that provide information for evidence based decisions. We also learnt that for each machine learning task we may have different models that use different learning algorithms, their performance needs to be evaluated. Comparison between such models is better performed via metrics. We also learnt the importance of cross validation to avoid variance, bias and over-fitting.\nIn this chapter with a motivational example we will start with the first kind of supervised learning technique, in which the models will learn from the relation between continuous or binary labels and explanatory features, this kind of task is named regression.\nThe introductory chapter defined a generic representation of a model using an equation:\n\\[Y=f(x1,x2,...,xn)+error\\] Where Y is attribute considered as label or response and x’s are the explanatory features and error is due to random process os measure errors usually notate as \\(\\epsilon\\)\nIn this chapter we will give attention to a series of models in which this mathematical function f can be defined as:\n\\[Y= \\mu + w1 * x1 +.... wn *xn+ \\epsilon \\tag{2.1}\\] Once all \\(w_i\\)s are raised to the power of 1 we say this model is linear on the parameters or simply linear. \\(\\mu\\) and \\(w_i\\)s are named parameters which we can estimate using different algorithms. The \\(w_is\\) will be weighting the importance of feature \\(x_i\\) when explaining Y.\nIn this chapter we will cover three main classes of models and respective algorithms to determine the parameters as follows:\n\nlinear regression which parameters are estimated by least squares (which use the great idea of minimizing the errors between the real values and the predicted values)\n\nand two models that consider the notion of regularization for avoiding model complexity (using an idea of shrinkage - defined later)\n\nlasso regression, estimated via Least Absolute Shrinkage and Selection Operator (LASSO) and L1 regularization\nRidge regresion estimated via L2 norm.\n\nThe idea of Lasso and Ridge regression is making the values of \\(w_i\\)s smaller as possible (ridge) or zero (lasso) for the least important features.\nOnce we will be covering regression tasks we will also address the logistic regression model that is useful when we want to model a binary response variable Y= 0 or 1 using explanatory features.\nAfter running a machine learning method and estimating the parameters, also named weights, we can rewrite it as:\n\\[\\hat{Y}= \\hat{\\mu} + \\hat{w_1}* x1 +....+\\hat{w_n} *xn\\] The values with a hat are the estimatives for each parameter an assume values depending on the model fitted. The \\(\\hat{Y}\\) is named predicted value. It is the value returned by the modell fitted for a given set of explanatory features multiplied by the parameters estimated. We will further see the importance of this value for estimating measures of error and model performance.\nBefore seeing the details about regression models we will start by describing a particular case in which they will be applied.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#motivational-context-chemotherapy-clinical-trial-focus-on-variables-and-practical-interpretation",
    "href": "supervised_regression.html#motivational-context-chemotherapy-clinical-trial-focus-on-variables-and-practical-interpretation",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.3 Motivational context : chemotherapy clinical trial (focus on variables and practical interpretation)",
    "text": "2.3 Motivational context : chemotherapy clinical trial (focus on variables and practical interpretation)\nAs a motivational example we will use a simulated randomized clinical trial (1:1) ,a typical kind of experiment conducted to determine efficacy of drugs, in our case a chemoterapeutic one, including only patients with tumors.\nHalf of the patients receive chemotherapy (chemo), and the other half do not (no_chemo). The practical goal is to evaluate whether patients treated with chemotherapy show greater tumor reduction, and to understand how gene expression and clinical characteristics influence this response.\n\n2.3.1 What is measured (and how to interpret each variable)\nUnit of analysis: patients. We have 10000 thousand patients.\nTreatment - treatment (factor: no_chemo, chemo)\nRepresents whether the patient received the chemotherapy.\nIn regression models, the coefficient for chemo directly quantifies the average difference in tumor reduction compared with no_chemo.\nDose - dose_intensity (continuous, ~0 for no_chemo and 0.8–1.1 for chemo)\nMeasures how strong the chemotherapy was for treated patients.\nA positive regression coefficient means that higher dose intensity is associated with greater tumor shrinkage.\nOutcomes - baseline_tumor_mm (continuous, mm): tumor size before treatment\n- post_tumor_mm (continuous, mm): tumor size after treatment\n- response_percent (continuous, 0–100): percentage of tumor shrinkage,\ncalculated as 100 × (baseline − post) / baseline\n→ Higher values mean better therapeutic response.\n- high_response (binary, 0/1): equal to 1 if response_percent ≥ 30 (similar to RECIST clinical criteria) Eisenhauer et al. (2009).\n→ Used in logistic regression to model the probability of a strong response.\nThe RECIST (Response Evaluation Criteria in Solid Tumors) Eisenhauer et al. (2009) are standardized, internationally accepted criteria used to assess tumor response to treatment in clinical trials. Developed by an international collaboration between the EORTC, NCI (U.S.), and NCIC (Canada), RECIST provides quantitative guidelines for measuring changes in tumor size using imaging (typically CT or MRI). Under RECIST version 1.1 (2009), tumor response is classified as: Complete Response (CR): Disappearance of all target lesions. Partial Response (PR): ≥30% decrease in the sum of the diameters of target lesions (relative to baseline). Progressive Disease (PD): ≥20% increase in the sum of diameters (plus an absolute increase of ≥5 mm) or appearance of new lesions. Stable Disease (SD): Neither sufficient shrinkage nor sufficient increase to qualify as PR or PD.\nClinical profile - patient_age (continuous, years): older age may slightly reduce treatment effect.\n- tumor_grade (factor: G1, G2, G3): aggressiveness of the tumor.\nMore aggressive tumors may respond more strongly because of higher proliferation rates.\n- performance_score (ordinal, 0–2): functional status (ECOG-like scale).\nHigher scores indicate poorer condition and typically lower treatment benefit due to toxicity or fragility.\nGene expression (omics) - gene_01 … gene_20000 (continuous, log2-CPM normalized)\nRepresent quantitative gene expression levels.\nSome genes were simulated as causal (directly affecting response),\nothers as correlated (co-expressed with causal ones),\nand the rest as noise.\nThe above example will be considered the reality for which we want to create a model representation. In penalized models (LASSO or Ridge), we expect the causal and correlated genes to receive higher weights (non-zero coefficients), while noisy ones will shrink toward zero.\n\n\n2.3.2 Experimental objectives and practical questions\n\nMain question: Do patients treated with chemotherapy (chemo) show higher average tumor shrinkage than those who do not (no_chemo)?\n\nClinical modulators: Do age, tumor grade, or performance score modify this effect?\n\nMolecular biomarkers: Which genes are associated with treatment response?\n\nPredictive modeling: Given a patient’s clinical and molecular profile, how well can we predict their tumor reduction and probability of high response?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#reading-the-dataset-into-our-environment",
    "href": "supervised_regression.html#reading-the-dataset-into-our-environment",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.4 Reading the dataset into our environment",
    "text": "2.4 Reading the dataset into our environment\nLets read the dataset in our R studio and check its structure of the first 15 columns\n\ntrial_ct &lt;- readRDS(\"~/att_ai_ml/data/trial_ct_chemo_cont.rds\")\nstr(trial_ct[, 1:15])   # peek\n\n'data.frame':   10000 obs. of  15 variables:\n $ patient_id       : chr  \"P001\" \"P002\" \"P003\" \"P004\" ...\n $ treatment        : Factor w/ 2 levels \"no_chemo\",\"chemo\": 2 1 1 1 2 2 1 2 1 2 ...\n $ dose_intensity   : num  1.08 0 0 0 1.01 ...\n $ patient_age      : num  81 61 81 74 41 74 22 61 26 22 ...\n $ tumor_grade      : Factor w/ 3 levels \"G1\",\"G2\",\"G3\": 2 2 3 2 2 2 1 2 3 2 ...\n $ performance_score: int  1 1 1 0 1 1 2 0 1 0 ...\n $ baseline_tumor_mm: num  52.2 43.3 43.2 63.8 54.2 44.9 51.5 95.1 69.5 54.7 ...\n $ post_tumor_mm    : num  25.8 43.3 43.2 61.2 33.6 37 51.5 60.4 69.5 29.8 ...\n $ response_percent : num  50.6 0 0 4.2 38 17.5 0 36.5 0 45.6 ...\n $ high_response    : int  1 0 0 0 1 0 0 1 0 1 ...\n $ gene_01          : num  11.01 9.26 10.09 8.99 9.01 ...\n $ gene_02          : num  9.31 7.79 8.95 7.8 8.33 ...\n $ gene_03          : num  9.51 8.02 9.45 7.78 8.23 ...\n $ gene_04          : num  10.27 9.16 9.42 8.87 9.03 ...\n $ gene_05          : num  7.31 8.14 7.83 7.49 7.23 ...\n\nstr(trial_ct)\n\n'data.frame':   10000 obs. of  2010 variables:\n $ patient_id       : chr  \"P001\" \"P002\" \"P003\" \"P004\" ...\n $ treatment        : Factor w/ 2 levels \"no_chemo\",\"chemo\": 2 1 1 1 2 2 1 2 1 2 ...\n $ dose_intensity   : num  1.08 0 0 0 1.01 ...\n $ patient_age      : num  81 61 81 74 41 74 22 61 26 22 ...\n $ tumor_grade      : Factor w/ 3 levels \"G1\",\"G2\",\"G3\": 2 2 3 2 2 2 1 2 3 2 ...\n $ performance_score: int  1 1 1 0 1 1 2 0 1 0 ...\n $ baseline_tumor_mm: num  52.2 43.3 43.2 63.8 54.2 44.9 51.5 95.1 69.5 54.7 ...\n $ post_tumor_mm    : num  25.8 43.3 43.2 61.2 33.6 37 51.5 60.4 69.5 29.8 ...\n $ response_percent : num  50.6 0 0 4.2 38 17.5 0 36.5 0 45.6 ...\n $ high_response    : int  1 0 0 0 1 0 0 1 0 1 ...\n $ gene_01          : num  11.01 9.26 10.09 8.99 9.01 ...\n $ gene_02          : num  9.31 7.79 8.95 7.8 8.33 ...\n $ gene_03          : num  9.51 8.02 9.45 7.78 8.23 ...\n $ gene_04          : num  10.27 9.16 9.42 8.87 9.03 ...\n $ gene_05          : num  7.31 8.14 7.83 7.49 7.23 ...\n $ gene_06          : num  7.44 7.22 7.41 6.28 7.01 ...\n $ gene_07          : num  9.13 9.24 8.96 8.23 7.96 ...\n $ gene_08          : num  1.722 -1.205 -1.211 -0.574 0.824 ...\n $ gene_09          : num  7.23 4.41 6.01 4.67 5.36 ...\n $ gene_10          : num  6.94 6.52 6.99 5.92 5.97 ...\n $ gene_11          : num  7.21 7 7.23 6.81 7.42 ...\n $ gene_12          : num  5.81 6.78 6.54 7.85 7.44 ...\n $ gene_13          : num  9.33 8.92 8.48 9.07 9.69 ...\n $ gene_14          : num  1.344 -1.212 -0.934 -0.639 1.07 ...\n $ gene_15          : num  8.79 9.1 8.95 9.33 9.22 ...\n $ gene_16          : num  6.79 7.36 7.23 7.74 7.94 ...\n $ gene_17          : num  7.31 5.58 6.11 5.7 6.14 ...\n $ gene_18          : num  7.56 7.82 7.98 7.97 7.66 ...\n $ gene_19          : num  1.504 -1.567 -1.099 -1.003 0.616 ...\n $ gene_20          : num  10.1 10.7 10.8 11.2 11.1 ...\n $ gene_21          : num  9.42 8.45 8.92 7.9 9.4 ...\n $ gene_22          : num  9.68 11.12 10.65 9.82 10.17 ...\n $ gene_23          : num  8.95 10.27 9.73 9.33 9.36 ...\n $ gene_24          : num  10.34 9.81 9.98 9.37 9.3 ...\n $ gene_25          : num  10.37 10 9.73 10.4 10.87 ...\n $ gene_26          : num  6.69 6.77 6.88 7.61 7.91 ...\n $ gene_27          : num  7.91 5.81 6.16 6.38 6.81 ...\n $ gene_28          : num  7.91 8.34 7.18 7.39 6.98 ...\n $ gene_29          : num  8.34 8.97 8.75 8.97 8.69 ...\n $ gene_30          : num  7.89 7.97 7.81 7.64 7.98 ...\n $ gene_31          : num  12.4 11.5 12.5 11 12 ...\n $ gene_32          : num  8.8 8.66 9.46 8.28 8.03 ...\n $ gene_33          : num  6.61 6.65 6.79 7.05 7.23 ...\n $ gene_34          : num  8.43 8.08 8.87 7.6 7.54 ...\n $ gene_35          : num  8.34 8.18 8.06 8.79 8.8 ...\n $ gene_36          : num  7.84 8.3 8.2 7.82 8.05 ...\n $ gene_37          : num  8.43 9.52 9.07 8.28 8.88 ...\n $ gene_38          : num  8.78 9.98 9.22 9.27 8.98 ...\n $ gene_39          : num  6.87 6.19 6.3 5.78 6.11 ...\n $ gene_40          : num  11.1 10.7 10.8 10.7 11.4 ...\n $ gene_41          : num  9.21 7.15 8.6 5.2 5.9 ...\n $ gene_42          : num  9 6.96 7.74 7.21 7.66 ...\n $ gene_43          : num  7.39 6.07 6.79 6.7 6.46 ...\n $ gene_44          : num  9.48 8.83 8.49 8.74 10.09 ...\n $ gene_45          : num  4.78 7.44 7.09 6.89 7.13 ...\n $ gene_46          : num  7.84 9.75 8.41 9.7 9.39 ...\n $ gene_47          : num  9.17 8.53 8.83 7.74 7.46 ...\n $ gene_48          : num  8.42 8.41 8.95 7.38 7.06 ...\n $ gene_49          : num  7.18 7.25 7.45 6.99 6.94 ...\n $ gene_50          : num  7.93 7.18 7.6 7.37 7.6 ...\n $ gene_51          : num  5.43 5.76 5.72 6.2 5.69 ...\n $ gene_52          : num  6.68 6.6 6.35 7.35 6.98 ...\n $ gene_53          : num  8.27 7.81 8.48 7.42 8.01 ...\n $ gene_54          : num  7.73 7.84 7.61 9.21 8.28 ...\n $ gene_55          : num  7.65 7.43 7.61 6.57 7.12 ...\n $ gene_56          : num  9.82 11.92 10.52 12.27 11.62 ...\n $ gene_57          : num  9.97 9.99 9.66 10.4 10.09 ...\n $ gene_58          : num  7.87 5.55 6.54 4.29 4.83 ...\n $ gene_59          : num  9.06 10.87 10.13 9.65 9.71 ...\n $ gene_60          : num  6.87 7.84 8.28 7.42 7.47 ...\n $ gene_61          : num  6.91 5.87 6.01 6.52 6.67 ...\n $ gene_62          : num  8.68 8.81 8.92 9.36 8.77 ...\n $ gene_63          : num  5.12 4.97 5.13 4.89 5.17 ...\n $ gene_64          : num  10.66 8.63 9.46 8.34 8.96 ...\n $ gene_65          : num  8.93 8.82 8.35 8.07 7.76 ...\n $ gene_66          : num  9.62 8.71 8.94 8.74 8.56 ...\n $ gene_67          : num  6.77 6.92 6.94 6.64 6.64 ...\n $ gene_68          : num  9.85 8.11 9.24 8.39 8.94 ...\n $ gene_69          : num  6.43 6.69 6.63 5.66 5.91 ...\n $ gene_70          : num  7.23 6.44 7.12 6.02 6.42 ...\n $ gene_71          : num  8.98 8.83 8.74 9.01 8.74 ...\n $ gene_72          : num  9.38 8.19 8.7 8.47 8.84 ...\n $ gene_73          : num  8.33 8.69 9.39 9.03 7.84 ...\n $ gene_74          : num  8.87 10.39 10.11 9.81 9.93 ...\n $ gene_75          : num  9.42 7.99 8.5 7.07 7.32 ...\n $ gene_76          : num  8.83 8.06 8.09 7.54 7.83 ...\n $ gene_77          : num  5.13 6.01 5.6 5.5 5.26 ...\n $ gene_78          : num  6.5 7.77 7.28 6.79 6.29 ...\n $ gene_79          : num  7.19 7.43 8.1 8.45 8.17 ...\n $ gene_80          : num  7.84 7.63 7.82 8.5 8.5 ...\n $ gene_81          : num  7.53 8.77 7.86 7.33 7.27 ...\n $ gene_82          : num  10.29 6.95 8.81 7.3 8.14 ...\n $ gene_83          : num  7.21 6.96 6.79 6.25 6.76 ...\n $ gene_84          : num  8.53 8.66 8.53 7.92 8.42 ...\n $ gene_85          : num  8.72 7.01 7.91 7.4 7.52 ...\n $ gene_86          : num  9.76 7.75 8.29 9.36 9.29 ...\n $ gene_87          : num  7.36 6.59 6.37 7.87 7.61 ...\n $ gene_88          : num  8.1 8.88 8.67 9.1 9.03 ...\n $ gene_89          : num  9.56 7.91 8.79 7.01 7.39 ...\n  [list output truncated]\n\n\n\n2.4.1 Notes for analysis\nAn important first stage in any statistical or machine learning modelling is the exploratory analysis in which we use simple metrics of visualization tools to have a first glimpse of the dataset we have in hands. In this sense, we can use figures like histograms or boxplots that represent the distribution of continuous features and scatterplots that show the relationship between two continuous variables\nFor example, we can produce, Boxplots of response_percent by treatment, histograms of response_percent, and scatter plots of baseline_tumor_mm vs post_tumor_mm.\nIn summary, in our clinical trial example, response_percent is the main continuous endpoint, high_response is its clinically relevant binary version, treatment and dose_intensity define the intervention, and clinical plus gene variables explain for whom and how much the chemotherapy works.\nOur objective in this chapter will be:\nFor linear regression:\n\nReport estimated coefficients, standard errors, t-values, and p-values for each predictor.\nEvaluate model fit using metrics such as R² and adjusted R² to assess explained variability.\nCheck residual diagnostics (normality, homoscedasticity, and influential points) to verify model assumptions.\nPlot fitted vs observed values and residual vs fitted to visually inspect model adequacy.\n\n\n\nReport confidence intervals for main effects (e.g., treatment effect, dose effect).\n\n\n\nSummarize predicted responses or marginal means by treatment group for interpretation and communication of clinical impact.\n\n\n\nFor LASSO, Ridge and Elastic Net: use cross-validation to select the regularization parameter λ, and compare their stability and sparsity.\n\nIn your report, distinguish clearly between:\n\nAverage treatment effects (chemo vs no_chemo)\n\nModulating effects (genes, age, grade, performance, dose intensity)\n\nFor logistic regression: report odds ratios with 95% CIs and evaluate ROC/AUC and calibration.\n\nWe start usually by understanding if we have any bits of the data that may be missing. Using this code below we can see that our dataset is indeed complete. We will dedicate a section on missing data in future chapters. In the case of our motivational example we have a complete dataset.\n\ntype_map &lt;- tibble::tibble(\n  variable    = names(trial_ct),\n  class       = sapply(trial_ct, \\(x) paste(class(x), collapse = \"/\")),\n  n_missing   = sapply(trial_ct, \\(x) sum(is.na(x))),\n  pct_missing = round(100 * sapply(trial_ct, \\(x) mean(is.na(x))), 2),\n  n_unique    = sapply(trial_ct, \\(x) dplyr::n_distinct(x)),\n  example     = sapply(trial_ct, \\(x) paste(utils::head(unique(x), 3), collapse = \", \"))\n)\ntype_map %&gt;% arrange(desc(class))\n\n# A tibble: 2,010 × 6\n   variable          class   n_missing pct_missing n_unique example             \n   &lt;chr&gt;             &lt;chr&gt;       &lt;int&gt;       &lt;dbl&gt;    &lt;int&gt; &lt;chr&gt;               \n 1 dose_intensity    numeric         0           0      302 1.085, 0, 1.007     \n 2 patient_age       numeric         0           0       61 81, 61, 74          \n 3 baseline_tumor_mm numeric         0           0      912 52.2, 43.3, 43.2    \n 4 post_tumor_mm     numeric         0           0      869 25.8, 43.3, 43.2    \n 5 response_percent  numeric         0           0      665 50.6, 0, 4.2        \n 6 gene_01           numeric         0           0    10000 11.0132382528657, 9…\n 7 gene_02           numeric         0           0    10000 9.31355524977464, 7…\n 8 gene_03           numeric         0           0    10000 9.51020822690023, 8…\n 9 gene_04           numeric         0           0    10000 10.272629360758, 9.…\n10 gene_05           numeric         0           0    10000 7.30636755315514, 8…\n# ℹ 2,000 more rows\n\n\n\np + scale_y_continuous(limits = c(0, NA), expand = expansion(mult = c(0, .05)))\n\n\n\n\n\n\n\n\nAn important first stage in any statistical or machine learning modelling is the exploratory analysis in which we use simple metrics of visualization tools to have a first glimpse of the dataset we have in hands. In this sense, we can use figures like histograms or boxplots that represent the distribution of continuous features and scatterplots that show the relationship between two continuous variables\nFor example, we can produce, Boxplots of response_percent by treatment, histograms of response_percent, and scatter plots of baseline_tumor_mm vs post_tumor_mm.\nBelow we have the codes to produce some exploratory graphics\n\np1 &lt;- ggplot(trial_ct, aes(response_percent)) + geom_histogram(bins = 30) +\n  labs(x = \"Response percent\", y = \"Count\")\np2 &lt;- ggplot(trial_ct, aes(factor(high_response))) + geom_bar() +\n  labs(x = \"High response (≥30%)\", y = \"Count\")\np3 &lt;- ggplot(trial_ct, aes(baseline_tumor_mm)) + geom_histogram(bins = 30) +\n  labs(x = \"Baseline tumor (mm)\", y = \"Count\")\np4 &lt;- ggplot(trial_ct, aes(patient_age)) + geom_histogram(bins = 30) +\n  labs(x = \"Age (years)\", y = \"Count\")\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol = 2)\n\n\n\n\nDistributions of outcomes and key covariates.\n\n\n\n\n\nggplot(trial_ct, aes(baseline_tumor_mm, post_tumor_mm, color = treatment)) +\n  geom_point(alpha = 0.6) +\n  geom_abline(slope = 1, intercept = 0, linetype = 2) +\n  labs(x = \"Baseline (mm)\", y = \"Post (mm)\", color = \"Treatment\")\n\n\n\n\nBaseline vs post-treatment tumor size.\n\n\n\n\n\nggplot(trial_ct, aes(treatment, response_percent)) +\n  geom_boxplot() +\n  labs(x = \"Treatment\", y = \"Response percent\")\n\n\n\n\nResponse by treatment.\n\n\n\n\n\nggplot(trial_ct, aes(treatment, response_percent)) +\n  geom_boxplot(outlier.shape = NA, width = 0.6) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, fill = \"white\") +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", width = 0.15) +\n  labs(x = \"Treatment\", y = \"Response percent\")\n\n\n\n\nResponse by treatment with raw points and mean ± 95% CI.\n\n\n\n\nWe can also create some summaries of the dataset. For example we can summarise the reponse_percent feature in each of the groups.\n\ntrial_ct %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(\n    n      = n(),\n    mean   = mean(response_percent),\n    sd     = sd(response_percent),\n    median = median(response_percent),\n    q1     = quantile(response_percent, 0.25),\n    q3     = quantile(response_percent, 0.75)\n  )\n\n# A tibble: 2 × 7\n  treatment     n  mean    sd median    q1    q3\n  &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 no_chemo   5078  4.83  6.81    0.8   0     8.2\n2 chemo      4922 37.6  10.9    37.3  30.2  44.8\n\n\nWe can also perform the calculation of some correlations to determine associations between features. Plotting this correlations is also important helping us to discuss the level of association between features. The code below calculates the correlation between the level of expression of each gene and the response_percent feature.\n\ngene_cols &lt;- grep(\"^gene_\", names(trial_ct), value = TRUE)\ncors &lt;- sapply(gene_cols, function(g) {\n  suppressWarnings(cor(trial_ct[[g]], trial_ct$response_percent, use = \"pairwise.complete.obs\"))\n})\ntop10_names &lt;- names(sort(abs(cors), decreasing = TRUE))[1:10]\ntop10_df &lt;- tibble::tibble(\n  gene = top10_names,\n  cor  = unname(cors[top10_names])\n) %&gt;%\n  dplyr::mutate(gene = reorder(gene, abs(cor)))\n\nggplot(top10_df, aes(x = gene, y = cor, fill = cor &gt; 0)) +\n  geom_col() +\n  coord_flip() +\n  guides(fill = \"none\") +\n  labs(x = \"Gene\", y = \"Correlation with response_percent\",\n       title = \"Top 10 genes by absolute correlation (signed)\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#metrics-for-model-comparison",
    "href": "supervised_regression.html#metrics-for-model-comparison",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.5 Metrics for model comparison",
    "text": "2.5 Metrics for model comparison\nBefore comparing regression methods, it is important to understand the metrics used to evaluate how well a model predicts continuous or binary outcomes. Each metric captures a different aspect of model performance: accuracy, precision, or calibration and helps interpret how reliable a model’s predictions are in practice.\nWe will focus on following main evaluation metrics commonly used for regression and classification.\n\n2.5.1 Error\nThe error (also called the residual) for each observation is:\n\\[\ne_i = \\hat{y}_i - y_i\n\\]\nWhere: (y_i) is the observed (true) value for subject (i); (_i) is the model prediction; (e_i) is the individual prediction error. Positive error = overestimation; negative error = underestimation.\nExample:\n\nsuppressPackageStartupMessages(library(ggplot2))\nset.seed(1)\ndf_err &lt;- data.frame(\n  patient   = 1:8,\n  observed  = c(20, 35, 40, 60, 75, 85, 95, 100),\n  predicted = c(18, 38, 37, 65, 71, 89, 92, 102)\n)\ndf_err$error &lt;- df_err$predicted - df_err$observed\n\nggplot(df_err, aes(x = patient)) +\n  geom_segment(aes(y = observed, yend = predicted, xend = patient), linewidth = 0.7) +\n  geom_point(aes(y = observed), size = 2) +\n  geom_point(aes(y = predicted), size = 2, shape = 17) +\n  labs(x = \"Patient\", y = \"Response (%)\",\n       title = \"Prediction error (eᵢ = ŷᵢ − yᵢ): observed vs predicted\")\n\n\n\n\n\n\n\nFigure 2.1\n\n\n\n\n\n\n\n2.5.2 Mean Absolute Error (MAE)\nThe mean absolute error summarizes the average magnitude of errors, ignoring their sign:\n\\[\n\\mathrm{MAE} = \\frac{1}{n}\\sum_{i=1}^{n} |e_i| = \\frac{1}{n}\\sum_{i=1}^{n} |\\hat{y}_i - y_i|\n\\]\nMAE gives equal weight to all errors and is less sensitive to outliers than MSE/RMSE.\nInterpretation: if MAE = 4, predictions are, on average, 4 units away from the observed values (e.g., 4 percentage points of tumor reduction).\n\n\n2.5.3 Mean Square Error (MSE)\nThe mean square error measures the average squared deviation:\n\\[\n\\mathrm{MSE} = \\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n} e_i^2\n\\]\nBy squaring, large errors are penalized quadratically, making MSE more sensitive to occasional large misses useful when big mistakes are costly (for example, underestimating toxicity).\n\n\n2.5.4 Root Mean Square Error (RMSE)\nThe root mean square error is the square root of MSE, returning to the original units of the outcome:\n\\[\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (\\hat{y}_i - y_i)^2} = \\sqrt{\\mathrm{MSE}}\n\\]\nRMSE can be read as the typical magnitude (standard deviation) of prediction errors; it penalizes large errors more than MAE but remains directly interpretable.\n\n\n2.5.5 Compact comparison\n\n\n\n\n\n\n\n\n\n\nMetric\nFormula\nPenalizes large errors strongly?\nUnits\nPractical meaning\n\n\n\n\nError\n\\((e_i = \\hat{y}_i - y_i)\\)\n\nOutcome units\nDirection and size of each residual\n\n\nMAE\n\\((\\frac{1}{n}\\sum\\)\ne_i\n)\nNo (linear)\n\n\nMSE\n\\((\\frac{1}{n}\\sum e_i^2)\\)\nYes (quadratic)\nSquared units\nAverage squared deviation\n\n\nRMSE\n\\((\\sqrt{\\frac{1}{n}\\sum e_i^2})\\)\nYes (quadratic)\nOutcome units\nTypical error size (√MSE)\n\n\n\n\n\n2.5.6 Visual comparison (how metrics respond to error size)\n\nsuppressPackageStartupMessages(library(ggplot2))\nsuppressPackageStartupMessages(library(dplyr))\nsuppressPackageStartupMessages(library(tidyr))\n\ndf_metrics &lt;- data.frame(error = seq(-10, 10, by = 0.25)) |&gt;\n  mutate(\n    abs_error = abs(error),\n    sq_error  = error^2,\n    rmse_unit = sqrt(error^2)  # same as abs(error), shown for relation to RMSE\n  ) |&gt;\n  pivot_longer(\n    cols = c(abs_error, sq_error, rmse_unit),\n    names_to = \"metric\", values_to = \"value\"\n  ) |&gt;\n  mutate(\n    # Ensure fixed factor level order so colors map correctly\n    metric = factor(metric, levels = c(\"abs_error\", \"sq_error\", \"rmse_unit\"))\n  )\n\nlabs_map &lt;- c(abs_error = \"MAE contribution |e|\",\n              sq_error  = \"MSE contribution e²\",\n              rmse_unit = \"RMSE unit √(e²)\")\n\n# Map colors by named vector to avoid level-order surprises\ncol_map &lt;- c(\n  abs_error = \"#2f63c0\",  # blue\n  sq_error  = \"#ff7f50\",  # orange/coral\n  rmse_unit = \"#66c2a5\"   # green\n)\n\nggplot(df_metrics, aes(x = error, y = value, color = metric, linetype = metric)) +\n  geom_line(linewidth = 1) +\n  scale_color_manual(values = col_map, breaks = names(labs_map), labels = labs_map) +\n  scale_linetype_manual(values = c(abs_error = \"solid\", sq_error = \"dashed\", rmse_unit = \"dotdash\"),\n                        breaks = names(labs_map), labels = labs_map) +\n  labs(x = \"Error (eᵢ = ŷᵢ − yᵢ)\", y = \"Contribution\", color = NULL, linetype = NULL,\n       title = \"How MAE, MSE, and RMSE respond to increasing error magnitude\") +\n  theme_minimal()\n\n\n\n\n\n\n\nFigure 2.2\n\n\n\n\n\n\n\n2.5.7 Worked numeric example to illustrate how to calculate errors, mae, mse and rmse and to show them in a dataframe.\n\nerrors &lt;- c(-5, 2, -3, 8, 1, -2, 4, -1)\nmae  &lt;- mean(abs(errors))\nmse  &lt;- mean(errors^2)\nrmse &lt;- sqrt(mse)\ndata.frame(Metric = c(\"MAE\",\"MSE\",\"RMSE\"),\n           Value  = c(mae, mse, rmse))\n\n  Metric     Value\n1    MAE  3.250000\n2    MSE 15.500000\n3   RMSE  3.937004\n\n\n\n\n2.5.8 Notes for analysis\n\nUse MAE when you want an intuitive average error magnitude, robust to outliers.\n\nUse RMSE when large errors must be penalized more heavily (and to keep units interpretable).\n\nReport both MAE and RMSE for a balanced view; and residual diagnostics (validity).\n\nAlways compute metrics with cross-validation to estimate out-of-sample performance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#fitting-simple-linear-regression-to-our-data",
    "href": "supervised_regression.html#fitting-simple-linear-regression-to-our-data",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.6 Fitting simple linear regression to our data",
    "text": "2.6 Fitting simple linear regression to our data\nWe will fit the following model for our motivational example\n\\[Y = w_0 + w_1 X_1 + w_2 X_2 + \\ldots + w_p X_p + \\varepsilon\\]\nThat in our particular case will be:\n\\[\n\\text{response\\_percent} =\nw_0 +\nw_1(\\text{compound\\_dose}) +\nw_2(\\text{patient\\_age}) +\nw_3(\\text{disease\\_type}) +\nw_4(\\text{gene\\_expression\\_1}) +\nw_5(\\text{gene\\_expression\\_2}) +\n\\ldots +\nw_{2000}(\\text{gene\\_expression\\_2000}) +\n\\varepsilon\n\\]\nThe response variable (outcome) is response_percent the continuous measure of how much the tumor shrank (efficacy minus toxicity). The explanatory variables (predictors) include everything else in the dataset (.) except: patient_id (just an identifier) high_response (the binary version of the same outcome)\nSo our explanatory variables include: Gene expression features (gene_expression_1 … gene_expression_20) Clinical variables (compound_dose, patient_age, disease_type, etc.)\n\n2.6.1 Model assumptions for linear regression\nWhen we fit a linear regression model, we make several assumptions about the relationship between the predictors (X’s) and the outcome (Y). These assumptions matter because they affect whether the estimated coefficients and statistical tests can be trusted.\n\nplot_pair &lt;- function(data_good, data_bad, xvar, yvar, title_good, title_bad, xlabel, ylabel) {\np1 &lt;- ggplot(data_good, aes({{xvar}}, {{yvar}})) +\ngeom_point(alpha = 0.7, color = \"#2f63c0\") +\ngeom_smooth(method = \"lm\", se = FALSE, color = \"#ff7f50\") +\nlabs(title = paste(\"Correct:\", title_good), x = xlabel, y = ylabel) +\ntheme_minimal()\np2 &lt;- ggplot(data_bad, aes({{xvar}}, {{yvar}})) +\ngeom_point(alpha = 0.7, color = \"#2f63c0\") +\ngeom_smooth(method = \"lm\", se = FALSE, color = \"#ff7f50\") +\nlabs(title = paste(\"Incorrect:\", title_bad), x = xlabel, y = ylabel) +\ntheme_minimal()\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n}\n\n\nLinearity The relationship between each predictor and the outcome is assumed to be linear (on the model’s scale). If the true relationship is curved or nonlinear, the model may systematically under- or over-predict. Check: residuals vs fitted values or vs individual predictors should look patternless (no curves).\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nIndependence of errors Residuals (errors) should be independent across observations. This is especially important for time series or clustered data (e.g., repeated measures, multi-center studies). Violation: autocorrelation or clustering inflates apparent precision.\n\n\n\n\n\n\nIndependence of errors: correct (i.i.d.) vs incorrect (strong AR(1)). Each row shows residuals over order and their ACF.\n\n\n\n\n\nHomoscedasticity (constant variance) The spread of residuals should be roughly constant across fitted values. If residuals fan out as predictions increase, that’s heteroskedasticity. Check: residuals vs fitted plot where spread should be similar across the range. Fixes: transform the outcome, use weighted least squares, or robust (HC) standard errors.\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nNormality of residuals Residuals should be approximately normally distributed around zero. This mainly underpins valid p-values and confidence intervals (less critical for pure prediction). Check: Q–Q plot or histogram of standardized residuals.\n\n\n\n\n\n\n\n\n\n\n\nNo multicollinearity Predictors should not be highly correlated with each other. Severe collinearity makes individual coefficient estimates unstable and hard to interpret. Check: Variance Inflation Factor (VIF); values &gt; 5 (or &gt; 10) suggest issues.\n\n\n\n\n\n\n\n\n\n\n\nNo influential outliers A few extreme points should not unduly determine the fit. Check: leverage and Cook’s distance; large Cook’s D indicates influential observations worth investigation.\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nWhen these assumptions are reasonably met, Ordinary Least Squares (OLS) yields unbiased, efficient estimates and meaningful inference about how predictors relate to the response. In practice, diagnostic plots and simple tests help verify whether the model behaves well for the data at hand.\nWe know use the fabulous property of R to create functions and we will create three functions one to calculate mae and rmse which will be performed simultaneosly by the eval_perform function.\n\n# ---- 0) Metrics (MAE and RMSE) ----\n\nmae  &lt;- function(y, yhat) mean(abs(y - yhat))\nrmse &lt;- function(y, yhat) sqrt(mean((y - yhat)^2))\n\n# ---- evaluation function (MAE and RMSE) ----\neval_perf &lt;- function(y_true, y_pred) {\n  tibble(\n    MAE  = mae(y_true, y_pred),\n    RMSE = rmse(y_true, y_pred)\n  )\n}\n# alias to avoid mistyping\neval_perform &lt;- eval_perf\n\nAfter that we split the data set into training (70%) of the data and test (30%) of the data.\n\nset.seed(42)\n# ---- 1) Train/Test split (70/30) ----\nn  &lt;- nrow(trial_ct)\nix &lt;- sample.int(n, size = floor(0.7 * n))\ntrain &lt;- trial_ct[ix, , drop = FALSE]\ntest  &lt;- trial_ct[-ix, , drop = FALSE]\n\nThen we can remove some unecessary columns from the dataset.\n\n# ---- 2) Remove columns that must NOT enter the model ----\n# (ID and the binary endpoint; keeps the supervised task as a pure regression)\n\ndrop_cols &lt;- intersect(names(train), c(\"patient_id\", \"high_response\", \"baseline_tumor_mm\", \"post_tumor_mm\"))\ntrain_nopii &lt;- dplyr::select(train, -dplyr::all_of(drop_cols))\ntest_nopii  &lt;- dplyr::select(test,  -dplyr::all_of(drop_cols))\n\nAfter that we write some code that will inform to R the formula we want to consider for this model.\n\n# Common formula for OLS (and to derive terms/dummies for glmnet)\nf_ols &lt;- response_percent ~ .\n\nThe function for fitting a linear regression in R is `lm` to which we indicate the formula of the model and the training dataset to be used for construction of the model. When we write ols_tmp &lt;- lm(f_ols, data = train_nopii) we are fitting a temporary linear model on the trainingset only to let R learn the exact design it should use: which predictors are in the model, how factors are encoded (their levels and the chosen reference), and the precise structure of any interactions or transformations. Extracting ols_terms &lt;- terms(ols_tmp) gives us that blueprint of the design matrix.\nLater, when we build matrices with model.matrix(ols_terms, data = train_nopii) and model.matrix(ols_terms, data = test_nopii), both training and test data are transformed with the same blueprint. This prevents issues like “factor has new levels in test,” mismatched dummy columns, or different column ordering problems that would otherwise break penalized models (e.g., glmnet) or yield inconsistent predictions. In short: we lock in the training design so the test set is encoded identically, guaranteeing compatible inputs for all models.\n\nThese steps are important because we will use the same training and testing datasets for running Lasso and Ridge Regression later using the glmnet package.\n\n# ---- 3) Capture TRAIN terms to ensure identical dummies in TEST ----\n# (fit a temporary OLS only to extract terms & factor levels)\n\nols_tmp   &lt;- lm(f_ols, data = train_nopii)\nols_terms &lt;- terms(ols_tmp)\n\n\n# Consistent design matrices for glmnet (no intercept column)\nX_train &lt;- model.matrix(ols_terms, data = train_nopii)[, -1, drop = FALSE]\nX_test  &lt;- model.matrix(ols_terms, data = test_nopii)[,  -1, drop = FALSE]\ny_train &lt;- train_nopii$response_percent\ny_test  &lt;- test_nopii$response_percent\n\nNo we proceed with the Ordinary Least Square regression analysis of our motivational example.\nIn this block of code we are fitting and evaluating our baseline Ordinary Least Squares (OLS) regression model. The command mod_ols &lt;- lm(f_ols, data = train_nopii) fits a linear regression model using only the training data. The formula f_ols expresses that we want to predict response_percent, our continuous measure of tumor shrinkage, using all available explanatory variables except for patient_id and high_response. These two variables are removed because one is simply an identifier and the other is a binary version of the same outcome, which would leak information into the model. The resulting object mod_ols contains the estimated coefficients and fitted values for the training set.\nNext, pred_ols_train &lt;- predict(mod_ols, newdata = train_nopii) generates predictions for the same training data used to fit the model. These are the in-sample predictions. They allow us to evaluate how well the model fits the data it has already seen and to compute basic performance metrics such as the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE). They are also used to inspect diagnostic plots that reveal potential problems such as nonlinearity, heteroskedasticity, or outliers.\nFinally, pred_ols_test &lt;- predict(mod_ols, newdata = test_nopii) applies the trained OLS model to the independent test dataset, which was not used during model fitting. These predictions are used to assess the model’s ability to generalize to new data. Comparing the errors on the training and test sets helps us detect whether the model is overfitting (performing much better on training data than on unseen data) or underfitting. Because we ensured earlier that categorical variables and factor levels are defined consistently between training and test sets, the prediction step runs smoothly without level-mismatch errors. In summary, this three-step process fits the baseline OLS model, obtains fitted and predicted values, and provides the foundation for fair comparison with the regularized models (LASSO and Ridge) that will be trained using the same data split.\n\n# ---- 4) OLS (uses data.frame; glmnet uses X/y) ----\nmod_ols        &lt;- lm(f_ols, data = train_nopii)\npred_ols_train &lt;- predict(mod_ols, newdata = train_nopii)\npred_ols_test  &lt;- predict(mod_ols, newdata = test_nopii)\n\nIn this block we prepare and run standard OLS diagnostics to check whether the linear model assumptions look reasonable. First, we load helper packages: ggplot2 and dplyr for plotting and data handling, lmtest for tests like Breusch–Pagan and Durbin–Watson, sandwich for robust variance estimators, and car for tools such as variance inflation factors, component-plus-residual plots, and outlier checks. Then par(mfrow = c(2, 2)) tells base R to arrange four plots in a 2 by 2 grid. The call plot(mod_ols) draws the default diagnostic panel for a fitted lm object: Residuals vs Fitted to look for nonlinearity or heteroskedasticity, Normal Q-Q to assess approximate normality of residuals, Scale–Location to check whether residual spread is roughly constant across fitted values, and Residuals vs Leverage with Cook’s distances to flag influential observations. Together these plots provide a quick visual screening of model adequacy before we proceed to formal tests or alternative specifications.\n\n  #| label: ols-assumptions\n  #| message: false\n  #| warning: false\n  suppressPackageStartupMessages({\n    library(ggplot2)\n    library(dplyr)\n    library(lmtest)    # bptest(), dwtest()\n    library(sandwich)  # robust (HC) variance estimators\n    library(car)       # vif(), crPlots(), outlierTest()\n  })\n  \n  # --- 1) Quick base-R diagnostic panel (residuals, QQ, Scale-Location, Residuals vs Leverage)\nop &lt;- par(mfrow = c(2, 2))\nplot(mod_ols)\n\n\n\n\n\n\n\n\nResiduals vs Fitted\nThis plot examines whether the residuals are centered around zero and whether there is any systematic pattern. The points here are scattered roughly around the horizontal line with no obvious curve, suggesting that the relationship between predictors and outcome is reasonably linear. There is a slight spread increase for larger fitted values, but it does not seem severe. Overall, the assumption of linearity and constant variance appears acceptable.\nNormal Q–Q\nThe Q–Q plot compares the standardized residuals to what would be expected if they followed a normal distribution. Most points lie very close to the diagonal reference line, except for a few at the extreme tails. This indicates that the residuals are approximately normal, with only minor deviations that are unlikely to affect inference materially.\nScale–Location (Spread–Location)\nThis plot checks whether the variance of residuals is constant across the range of fitted values (homoskedasticity). The red line is nearly flat, and the spread of the points is fairly uniform across the x-axis. There is no strong funnel shape or trend, so the homoskedasticity assumption is reasonably satisfied.\nResiduals vs Leverage\nThis plot identifies influential cases observations that have both high leverage (unusual predictor combinations) and large residuals (poorly fitted). Most points lie within the Cook’s distance contours, indicating that no single case is exerting excessive influence on the fitted model. A few observations (such as those labeled 800 or 1920) have higher leverage, but they do not appear to distort the overall fit.\nOverall interpretation\nTaken together, these diagnostics suggest that the OLS model fits the data adequately. The linearity, normality, and constant-variance assumptions hold reasonably well, and there are no major outliers or influential points. Minor departures at the extremes are typical in real data and do not undermine the general validity of the model.\nThe next block of code is a compact toolkit to inspect, interpret, and summarize your fitted OLS model from several complementary angles. It starts with summary(mod_ols), which is the classic regression report. You get one row per coefficient with its estimate, standard error, t statistic, and p value, plus model-level diagnostics such as the residual standard error, R², adjusted R², and the F test for the null that all slopes are zero. Read this first to see direction and magnitude of effects and whether they are statistically distinguishable from zero after adjusting for the other variables in the model.\nNext it calls anova(mod_ols), which produces a Type I (sequential) ANOVA table. Here, sums of squares and p values are computed in the order that predictors enter the model. This is useful when there is a natural hierarchy or pre-specified entry order, but results can change if you reorder columns. If you need hypothesis tests that adjust for all other terms regardless of order (especially with factors and interactions), you would use car::Anova(mod_ols, type = 3) instead, which provides Type III tests.\nThen it builds a tidy coefficient table with broom::tidy(mod_ols, conf.int = TRUE). This converts the model output into a clean data frame that includes coefficient estimates, standard errors, test statistics, p values, and 95% confidence intervals. The code then arranges rows by p value and prints everything, which is convenient for scanning the most and least significant terms in a reproducible, table-friendly format.\nThe next step creates a quick ranking of “importance” by absolute t statistic. It removes the intercept, computes abs_t = |t|, sorts descending, and shows the top terms with their estimates, standard errors, test statistics, p values, and confidence intervals. This does not replace more formal variable-importance methods, but it is a fast way to see which coefficients have the strongest signal relative to their uncertainty within this linear specification.\nFinally, broom::glance(mod_ols) provides a one-row summary of overall fit metrics. You get R² and adjusted R² (explanatory power with and without a penalty for model size), sigma (residual standard deviation), the model F statistic and its p value, and information criteria such as AIC and BIC for comparing alternative models on a goodness-of-fit versus complexity trade-off.\nTaken together, these outputs let you check individual effects with uncertainty, evaluate sequential or fully adjusted hypothesis tests, order terms by signal-to-noise, and assess global model quality, all in tidy objects that can be reported or plotted downstream.\nIn the context of ANOVA, significance tells us whether a factor or variable has a real, measurable effect on the outcome, rather than the observed differences being just due to random chance.\nThe p-value is the probability of seeing a difference (or a larger one) in the data if, in reality, the factor had no effect at all that is, if the null hypothesis were true.\nWhen a p-value is very small (typically below 0.05), it means such a difference would be very unlikely to appear by random chance, so we have evidence to reject the null hypothesis and conclude that the variable probably does influence the outcome.\nIn simple terms, ANOVA uses p-values as a decision criterion:\n\nA small p-value (below 0.05) → the group differences or predictor effects are statistically significant.\nA large p-value (above 0.05) → the observed differences could easily occur by chance, so we do not have strong evidence of a real effect.\n\nSignificance does not measure the size or importance of the effect it only measures how confident we are that an effect exists at all.\nIn a regression model, the parameters (often called coefficients or betas) quantify how much the response variable changes when a given predictor changes, while holding all other predictors constant.\nEach parameter represents the direction and magnitude of that predictor’s influence on the outcome.\n\nA positive coefficient means that as the predictor increases, the response tends to increase as well.\nA negative coefficient means that higher values of the predictor are associated with lower response values.\nA coefficient close to zero suggests that the variable has little or no linear impact on the outcome, once the other predictors are accounted for.\n\nThe absolute value of the coefficient reflects how strong the relationship is (how sensitive the response is to changes in that variable), but the units of measurement matter: one unit of a gene-expression score does not mean the same as one year of age, so direct comparisons of raw coefficients can be misleading.\nWhen variables are standardized (converted to the same scale), larger absolute coefficients indicate stronger effects.\nStatistical tests (t values and p values) assess whether each coefficient is significantly different from zero. A small p value suggests that the estimated effect is unlikely to be due to random noise, providing evidence that this predictor truly contributes to explaining variation in the response.\nFollowing we have the code to generat the items above for our mod_ols object. We will avoid printing them here because each of the items would have thousands of elements… This brings to our minds the complexities of informing outputs of linear models with thousand of explanatories mantained by the model.\nThe next chunck of code evaluates how well the OLS model predicts the tumor response in both the training and test datasets, using two standard regression error metrics: MAE (Mean Absolute Error) and RMSE (Root Mean Square Error).\nThe first two lines extract the true outcome values the observed tumor shrinkage percentages (response_percent) from the training and testing subsets. The next two blocks call the function eval_perf(), which calculates MAE and RMSE by comparing the true values (y_train or y_test) to the model’s predictions (pred_ols_train or pred_ols_test).\nRecalling:\n\nMAE represents the average absolute difference between predicted and observed values it tells how far off the predictions are, on average, in the same units as the outcome (percentage points of tumor reduction).\nRMSE is similar but gives more weight to large errors, making it more sensitive to occasional poor predictions.\n\nThe results are stored in small tibbles and then combined into one table with the model name (“OLS”) and dataset origin (“Train” or “Test”).\n\n# --- Evaluate OLS with MAE and RMSE (using eval_perf) ---\n# y vectors (true values)\ny_train &lt;- train_nopii$response_percent\ny_test  &lt;- test_nopii$response_percent\n\nperf_ols_train &lt;- eval_perf(y_train, pred_ols_train) |&gt;\n  dplyr::mutate(Model = \"OLS\", Dataset = \"Train\")\n\nperf_ols_test &lt;- eval_perf(y_test, pred_ols_test) |&gt;\n  dplyr::mutate(Model = \"OLS\", Dataset = \"Test\")\n\n# Compact table\ndplyr::bind_rows(perf_ols_train, perf_ols_test)\n\n# A tibble: 2 × 4\n    MAE  RMSE Model Dataset\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  \n1  1.27  1.59 OLS   Train  \n2  1.83  2.28 OLS   Test   \n\n\nThe model fits the training data quite closely the average prediction error is about 1.3 percentage points. When applied to unseen data (the test set), the errors increase moderately to around 1.8–2.3 points.\nThis comparison is crucial because it shows generalization performance  how well the model performs on new data that were not used for training.\n\nThe training performance tells you how well the model explains patterns already seen during fitting.\nThe testing performance reveals how well those learned relationships extend to new, unseen patients.\n\nIf the test errors are only slightly higher than the training errors, as in your case, it suggests a good model that generalizes reasonably well. If test errors were much larger, it would indicate overfitting  the model memorized the training data instead of learning the general structure. Conversely, if both errors were high, it would point to underfitting, meaning the model is too simple to capture important relationships.\nIn summary, checking both training and test performance allows you to balance fit quality and predictive reliability, ensuring that the regression model is not only accurate on known data but also trustworthy for future predictions.\n\nAs a general guideline, whenever you a see a work quoting a linear regression try to understand if the cross validation technique was applied. You will be amazed on how many works do not bother about these details.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#some-words-on-regularization",
    "href": "supervised_regression.html#some-words-on-regularization",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.7 Some words on regularization",
    "text": "2.7 Some words on regularization\nWe discussed in a previous section that the parameters of the model are important for interpretation because they tell us how each variable contributes to explaining the outcome. However, in practice, when we move from small, well-controlled models to modern biomedical or omics data, we often face a very different scenario: instead of a few predictors such as age, dose, or tumor grade, we may have hundreds or thousands of gene-expression features. In this context, interpreting the individual coefficients becomes nearly impossible. Many of them will be correlated with one another, some may carry redundant information, and others may simply represent random noise. Traditional linear regression tends to overfit in such high-dimensional settings it tries to give every variable a non-zero weight, which leads to unstable and unreliable estimates that generalize poorly to new data.\nTo address this, machine-learning methods introduce the idea of regularization, also called shrinkage. The key idea is to constrain or penalize the size of the coefficients so that the model prefers simpler explanations that still fit the data well. Regularization discourages the algorithm from assigning large weights to predictors that do not truly improve predictive accuracy.\nTwo common approaches are Ridge regression and LASSO regression. Ridge regression applies an L2 penalty, which shrinks all coefficients toward zero but rarely eliminates them completely; it is particularly effective when many predictors have small, distributed effects. LASSO regression, in contrast, uses an L1 penalty, which can shrink some coefficients exactly to zero, performing variable selection at the same time as estimation. This means that LASSO can automatically identify a smaller subset of genes or variables that carry most of the predictive signal, making the model both simpler and easier to interpret.### Ordinary Least Squares Sum of Squares\n\n2.7.1 Ordinary Least Squares Sum of Squares\nThe residual sum of squares (RSS) minimized by Ordinary Least Squares (OLS) is:\n\\[\n\\text{RSS} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n           = \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2\n\\]\nOLS estimates the parameters ( ) that minimize this quadratic error term, without any regularization.\n\n\n2.7.2 Ridge Regression (L2 regularization)\nRidge regression introduces a penalty proportional to the L2 norm of the coefficients:\n\\[\nL_{\\text{ridge}}(\\boldsymbol{\\beta})\n= \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2\n+ \\lambda \\sum_{j=1}^{p} \\beta_j^2\n\\]\nwhere ( ) controls the penalty strength.\nThe L2 term shrinks all coefficients toward zero but does not set them exactly to zero.\nIt is especially useful when predictors are highly correlated.\n\n\n2.7.3 Lasso Regression (L1 regularization)\nLasso regression adds a penalty proportional to the L1 norm of the coefficients:\n\\[\nL_{\\text{lasso}}(\\boldsymbol{\\beta})\n= \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2\n+ \\lambda \\sum_{j=1}^{p} |\\beta_j|\n\\]\nThe L1 term encourages sparsity, meaning that some coefficients are driven exactly to zero, effectively performing variable selection.\n\n\n2.7.4 Elastic Net (Combination of L1 and L2)\nThe Elastic Net combines both Ridge (L2) and Lasso (L1) penalties:\n\\[\nL_{\\text{elastic-net}}(\\boldsymbol{\\beta})\n= \\sum_{i=1}^{n} (y_i - \\mathbf{x}_i^\\top \\boldsymbol{\\beta})^2\n+ \\lambda \\left[\n  \\alpha \\sum_{j=1}^{p} |\\beta_j|\n  + (1 - \\alpha) \\sum_{j=1}^{p} \\beta_j^2\n\\right]\n\\]\nwhere ( 0 ) controls the mix between L1 and L2 regularization:\n\n( = 1 ) → Lasso\n\n( = 0 ) → Ridge\n\n( 0 &lt; &lt; 1 ) → Elastic Net\n\nElastic Net is particularly effective when there are many correlated predictors: it keeps Ridge’s stability while still allowing Lasso-style variable selection.\nIn the next section we will see how to perform these three types of regression: LASSO, ridge regression, and elastic net.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#hyperparameters",
    "href": "supervised_regression.html#hyperparameters",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.8 Hyperparameters",
    "text": "2.8 Hyperparameters\nIn \\(Y=f\\left(x_1, x_2, \\ldots, x_n\\right)+\\) error, the parameters are the quantities inside the function \\(f\\) that the algorithm learns from the data to make \\(f\\) fit well (e.g., the \\(b\\) s in a linear model\nA hyperparameter is a setting that controls how f is learned, not something learned directly from the data by the usual fitting step. Hyperparameters define the shape/complexity of the function class you allow and how aggressively you search within it. They live outside f, but they constrain and guide the learning of f.\nHyperparameters are configuration choices that control the learning process and the capacity of the model class used to approximate \\(f(\\cdot)\\). They are not learned from the training loss directly; instead, they are selected (e.g., via cross-validation) to achieve good generalization, helping \\(f\\) capture the signal in \\(Y\\) without fitting the random error.\nIn supervised learning we seek to approximate an unknown function \\(f(\\cdot)\\) that links explanatory features \\(x_1, x_2, \\ldots, x_n\\) to a response variable \\(Y\\) :\n\\[\nY=f\\left(x_1, x_2, \\ldots, x_n\\right)+\\varepsilon,\n\\]\nwhere \\(\\varepsilon\\) captures random noise and unmeasured influences. When \\(f(\\cdot)\\) is assumed to be linear, the model becomes\n\\[\n\\hat{Y}=w_0+\\beta_1 x_1+\\cdots+w_p x_p,\n\\]\nand the learning task consists of estimating the coefficients \\(w_j\\) that minimize prediction error. These coefficients are the model parameters-they are learned directly from the data.\nHowever, in modern regression we often introduce an additional layer of control: hyperparameters, which determine how the coefficients are estimated and how much flexibility the model is allowed to have. Hyperparameters live outside the function \\(f(\\cdot)\\); they are not part of the fitted equation but instead regulate the learning process.\nOLS and the absence of hyperparameters Ordinary Least Squares (OLS) minimizes the Residual Sum of Squares (RSS):\n\\[\n\\mathrm{RSS}=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2 .\n\\]\nThe solution for \\(\\beta\\) has a closed analytical form and depends only on the data. Because there is no external control over model complexity, OLS has no hyperparameters. Its flexibility is entirely determined by the number of predictors in the model.\nWhile OLS is unbiased and efficient under ideal conditions, it becomes unstable when predictors are highly correlated or when \\(p\\) (number of variables) is large relative to \\(n\\).\nTo improve generalization, we introduce regularization-penalties that shrink coefficients toward zero and prevent overfitting.\nRidge, Lasso, and Elastic Net: controlling complexity with penalties Regularized regression modifies the OLS loss by adding a penalty term that constrains the magnitude of the coefficients.\nThe resulting objective function is\n\\[\n\\operatorname{RSS}_{\\text {penalized }}=\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2+\\lambda P(\\boldsymbol{\\beta}),\n\\]\nwhere \\(P(\\boldsymbol{\\beta})\\) defines the type of penalty and \\(\\lambda&gt;0\\) is a hyperparameter that controls its strength.\n\n\n\n\n\n\n\n\n\nModel\nPenalty term ( P() )\nMain hyperparameters\nInterpretation\n\n\n\n\nRidge\n\\(( \\sum_j \\beta_j^2 )\\) (L2)\n\\(( \\lambda )\\)\nShrinks coefficients toward zero smoothly; keeps all variables.\n\n\nLasso\n\\(( \\sum_j |\\beta_j| )\\) (L1)\n\\(( \\lambda )\\)\nShrinks some coefficients exactly to zero → automatic variable selection.\n\n\nElastic Net\n\\(( (1-\\alpha)\\sum_j \\beta_j^2/2 + \\alpha\\sum_j |\\beta_j|)\\)\n\\(( \\lambda, \\alpha )\\)\nCombines both effects; balances stability (Ridge) and sparsity (Lasso).\n\n\n\nUnderstanding \\(\\lambda\\) : the bias-variance control knob The hyperparameter \\(\\lambda\\) regulates how strongly the model is penalized: - Small \\(\\lambda \\rightarrow\\) minimal penalty, coefficients close to OLS estimates.\nLow bias, high variance (risk of overfitting). - Large \\(\\lambda \\rightarrow\\) heavy penalty, coefficients shrink strongly.\nHigher bias, lower variance (risk of underfitting). Tuning \\(\\lambda\\) therefore manages the bias-variance trade-off, shaping the smoothness and generalization capacity of the learned function \\(f(\\cdot)\\).\nTuning and selection Unlike \\(\\beta\\), hyperparameters are not optimized by minimizing training error. If we simply fitted the model for the smallest training loss, \\(\\lambda\\) would always shrink toward zero (i.e., revert to OLS). Instead, hyperparameters are chosen using cross-validation, evaluating predictive error on unseen folds of the data.\nThe value of \\(\\lambda\\) (and \\(\\alpha\\) for Elastic Net) that minimizes cross-validated error-or is within one standard error of the minimum (the 1-SE rule)-is selected as optimal.\nAfter choosing the hyperparameters, the model is re-trained on the full training set to estimate the final coefficients.\nConceptual summary - Parameters ( \\(w_j\\) ) define the learned relationship \\(f(\\cdot)\\). - Hyperparameters ( \\(\\lambda, \\alpha\\) ) define how the relationship is learned-controlling model flexibility and generalization. - OLS has no hyperparameters; Ridge, Lasso, and Elastic Net introduce \\(\\lambda\\) (and possibly \\(\\alpha\\) ) to regularize the model. - Choosing good hyperparameters ensures \\(f(X)\\) captures the true signal in \\(Y\\) rather than noise in \\(\\varepsilon\\)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#fitting-lasso-regression",
    "href": "supervised_regression.html#fitting-lasso-regression",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.9 Fitting LASSO Regression",
    "text": "2.9 Fitting LASSO Regression\nThe next block of code uses the glmnet package to fit a LASSO regression and to choose its penalty strength by cross-validation. The call to cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10) runs a ten-fold cross-validation loop over a grid of lambda values with the L1 penalty, which defines the LASSO. For each lambda the algorithm fits the model on nine folds and evaluates prediction error on the remaining fold, then averages the error across folds. glmnet standardizes predictors internally by default, which is important so that the penalty treats variables on the same footing regardless of their scale.\nFrom this cross-validation object we extract lambda.min, the lambda that achieved the smallest mean cross-validated error. We then refit the model on the full training set with glmnet(X_train, y_train, alpha = 1, lambda = cv_lasso$lambda.min). This produces a single LASSO model whose coefficients have been shrunk toward zero. Many uninformative coefficients are exactly zero, which performs built-in variable selection and improves interpretability while controlling variance.\nFinally, we obtain predicted values for both the training and the test sets with predict(mod_lasso, newx = X_train)and predict(mod_lasso, newx = X_test). These predictions allow us to compute performance metrics such as MAE and RMSE on data used to fit the model and on held-out data. Evaluating both is useful because the training metrics show how well the model can fit observed samples, while the test metrics indicate how well the model generalizes to new patients. Some analysts also report lambda.1se, which is the largest lambda within one standard error of the minimum error. That choice usually yields a sparser model with similar predictive accuracy and can be attractive when parsimony is a priority.\nThe next three chunks’ outputs show the fitted LASSO regression model and how it performs on the training and testing data. Unlike ordinary least squares, which gives every variable a non-zero coefficient, LASSO includes a penalty that forces many coefficients exactly to zero. The result is a simpler model that keeps only the most informative predictors.\nThe table of coefficients lists all variables that remain active after regularization. The first line, the intercept (27.99), represents the baseline predicted tumor response (in percentage points) when all other predictors are at their reference or zero levels. The next few coefficients correspond to clinical factors. For instance, treatmentchemo = 3.11 means that, on average and holding other variables constant, patients who received chemotherapy are predicted to have about three percentage points higher tumor shrinkage than those who did not. The coefficient for dose_intensity = 3.34 indicates that stronger chemotherapy doses are associated with greater reductions in tumor size. Age has a small positive coefficient, suggesting a very mild increase in response with age, while the negative sign for performance_score shows that patients with worse functional status tend to respond less effectively to treatment. Tumor grade also has a clear effect, with higher grades showing larger coefficients and therefore stronger responses.\nAfter the clinical covariates, the list continues with gene expression variables. Only a fraction of the hundreds of available genes appear, meaning that the LASSO has automatically selected those with the most predictive value. For example, gene_08, gene_14, and gene_19 have relatively large positive coefficients (around five), identifying them as strong predictors of greater tumor reduction. In contrast, genes such as gene_05 or gene_1778 have large negative coefficients, implying that higher expression of these genes is associated with poorer therapeutic response. The many small coefficients near zero reflect genes with weak or marginal contributions that the model nonetheless retained under the chosen regularization strength. In situation where the number of features to be considered is giant LASSO returns a smaller reasonable amount of variables compared with the original number, making interpretation of the model more facilitated.\nThe last section of the output shows model performance using the eval_perf() function. The MAE (mean absolute error) and RMSE (root mean square error) values quantify how far, on average, the predictions are from the true tumor responses. The training errors are MAE = 1.49 and RMSE = 1.87, while the test errors are MAE = 1.57 and RMSE = 1.96. The similarity of these values suggests that the model generalizes well: it fits the training data closely but not excessively and maintains similar predictive accuracy on unseen test patients.\nOverall, this LASSO fit yields a parsimonious model that identifies a handful of relevant clinical variables and a limited subset of genes that together explain much of the variation in tumor response. The regularization penalty has successfully balanced interpretability and predictive performance by shrinking or eliminating irrelevant coefficients while preserving the key biological and clinical signals in the data.\n\n# ---- 5) LASSO (alpha = 1) with CV to choose lambda ----\n# Note: glmnet standardizes features by default (standardize = TRUE).\ncv_lasso  &lt;- cv.glmnet(X_train, y_train, alpha = 1, nfolds = 10)\nmod_lasso &lt;- glmnet(X_train, y_train, alpha = 1, lambda = cv_lasso$lambda.min)\npred_lasso_train &lt;- as.numeric(predict(mod_lasso, newx = X_train))\npred_lasso_test  &lt;- as.numeric(predict(mod_lasso, newx = X_test))\n\n\n# Nonzero coefficients selected by LASSO\ncoef_lasso &lt;- coef(mod_lasso)\nnz &lt;- which(coef_lasso != 0)\nas.matrix(coef_lasso[nz, , drop = FALSE])\n\n                             s0\n(Intercept)       27.9894094231\ntreatmentchemo     3.1094150609\ndose_intensity     3.3438592381\npatient_age        0.0132882299\ntumor_gradeG2      0.1316015652\ntumor_gradeG3      0.5547952038\nperformance_score -0.2943083530\ngene_01            0.8494564076\ngene_05           -2.7310539064\ngene_08            5.1436742116\ngene_12            0.4288133459\ngene_14            5.1013463595\ngene_19            5.1746627223\ngene_27            0.0180460908\ngene_30           -0.0863467996\ngene_32           -0.0361982838\ngene_104           0.0211677652\ngene_116          -0.0218406711\ngene_117          -0.0052744538\ngene_123           0.0056347167\ngene_146           0.0498361359\ngene_157          -0.0279668435\ngene_165          -0.0080882163\ngene_187           0.0222580351\ngene_210          -0.0036941301\ngene_221           0.0096723727\ngene_225           0.0644057360\ngene_242           0.0684388670\ngene_253          -0.0077187155\ngene_287           0.0498245130\ngene_311          -0.0424858360\ngene_359          -0.0232784490\ngene_373          -0.0568150918\ngene_416           0.0008864799\ngene_459           0.0443358981\ngene_484           0.0190060279\ngene_490           0.0172107152\ngene_503          -0.0079158529\ngene_514          -0.0248283328\ngene_535          -0.0577584654\ngene_537          -0.0072821908\ngene_550           0.0375150155\ngene_576          -0.0704458018\ngene_600           0.0158202468\ngene_605           0.0196117982\ngene_627           0.0103392261\ngene_648          -0.0034662864\ngene_669           0.0008621344\ngene_670           0.0132388638\ngene_696          -0.0155218791\ngene_720          -0.0435232964\ngene_757           0.0366479160\ngene_760          -0.0310746046\ngene_795          -0.0038151634\ngene_804           0.0071026491\ngene_840           0.0162446672\ngene_864          -0.0570875844\ngene_874           0.0108188945\ngene_877          -0.0084290925\ngene_911           0.0303575087\ngene_916          -0.0135076583\ngene_956          -0.0240584781\ngene_969          -0.0054693674\ngene_976          -0.0569288581\ngene_980           0.0015706121\ngene_1013          0.0291775413\ngene_1024          0.0417414081\ngene_1051          0.0308411643\ngene_1059         -0.0030128212\ngene_1071         -0.0383864043\ngene_1084          0.0129566818\ngene_1096         -0.0226222064\ngene_1117          0.0243128673\ngene_1120          0.0600242242\ngene_1137         -0.0634424258\ngene_1150         -0.0576782902\ngene_1161          0.0050430799\ngene_1237          0.0121748732\ngene_1241          0.1164865560\ngene_1255          0.0025642628\ngene_1265         -0.0059853938\ngene_1292         -0.0114866186\ngene_1306          0.0069838046\ngene_1363         -0.0465489365\ngene_1376         -0.0113973153\ngene_1419          0.1117123380\ngene_1421          0.0343775734\ngene_1428         -0.1140875119\ngene_1437         -0.0166104156\ngene_1442          0.0553966751\ngene_1476         -0.0179387369\ngene_1490         -0.0272055274\ngene_1506          0.1071247869\ngene_1518         -0.0434573068\ngene_1556          0.0845555983\ngene_1564         -0.0539480912\ngene_1602         -0.0595800034\ngene_1610         -0.0390650740\ngene_1678          0.0569013521\ngene_1685         -0.0028281900\ngene_1691         -0.0119213530\ngene_1708          0.0561465041\ngene_1737          0.0029350844\ngene_1738          0.0040687069\ngene_1746          0.0015397073\ngene_1748         -0.0079077542\ngene_1761          0.0616664687\ngene_1763          0.0344047529\ngene_1775          0.0004665486\ngene_1778         -0.1198702861\ngene_1846         -0.0227816308\ngene_1858         -0.0464153923\ngene_1871         -0.0011798398\ngene_1892          0.0823542888\ngene_1927          0.0532531012\ngene_1976         -0.1120061133\ngene_1994          0.0020424817\n\n\n\n# Compare errors with your eval_perf()\nperf_lasso_train &lt;- eval_perf(y_train, pred_lasso_train) |&gt; dplyr::mutate(Model=\"LASSO\", Dataset=\"Train\")\nperf_lasso_test  &lt;- eval_perf(y_test,  pred_lasso_test)  |&gt; dplyr::mutate(Model=\"LASSO\", Dataset=\"Test\")\ndplyr::bind_rows(perf_lasso_train, perf_lasso_test)\n\n# A tibble: 2 × 4\n    MAE  RMSE Model Dataset\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  \n1  1.49  1.87 LASSO Train  \n2  1.57  1.96 LASSO Test",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#fitting-ridge-regression",
    "href": "supervised_regression.html#fitting-ridge-regression",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.10 Fitting Ridge Regression",
    "text": "2.10 Fitting Ridge Regression\nIn the following chunks of code we will implement a ridgre regression model.\nThe Ridge model is trained using cv.glmnet() with alpha = 0, which specifies the L2 penalty. The algorithm performs 10-fold cross-validation over a grid of possible penalty values (lambda) and identifies the one that minimizes the mean prediction error. The value of lambda.min = 1.83 is the penalty that achieved the lowest average cross-validation error, while lambda.1se = 2.01 corresponds to a slightly stronger penalty that still performs within one standard error of the minimum. The model is then refitted on the entire training dataset with this optimal lambda, and predictions are generated for both training and test sets.\nThe code then computes the Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) for each model OLS, LASSO, and Ridge on both the training and test sets using the eval_perf() function. These metrics quantify how close the model’s predictions are to the observed tumor response. The Ridge model shows MAE = 1.43 and RMSE = 1.79 on the training data, and MAE = 1.77 and RMSE = 2.23 on the test data. Compared to OLS (MAE = 1.83, RMSE = 2.28 on the test set), Ridge performs slightly better, reducing both bias and variance without overfitting. Its performance is similar to that of LASSO but typically smoother, since Ridge keeps all variables in the model rather than setting some coefficients exactly to zero.\nThe Ridge model coefficients provide additional insight. In contrast to LASSO, which eliminates many predictors, Ridge keeps all coefficients non-zero (2,006 in this dataset) but shrinks them toward zero depending on their contribution strength. The list of the top fifteen coefficients shows that the variables with the strongest influence on tumor response are consistent with previous models: dose_intensity and treatmentchemo have the largest positive effects, indicating that higher doses and chemotherapy are associated with greater tumor shrinkage. Several genes such as gene_14, gene_19, and gene_08 also have strong positive associations, while gene_05 has a large negative coefficient, suggesting a detrimental effect on treatment response. Other predictors, such as tumor grade and performance score, have smaller but directionally meaningful coefficients that align with clinical expectations.\nOverall, these results illustrate the essence of Ridge regularization: instead of discarding predictors as LASSO does, it shrinks all coefficients toward zero, reducing overfitting and stabilizing estimates when many correlated variables (such as thousands of gene expressions) are present. The outcome is a model that generalizes better than OLS, retains all variables but with moderated influence, and highlights which features exert the most consistent effects across the dataset.\n\n# ---- 6) Ridge (alpha = 0) with CV ----\ncv_ridge  &lt;- cv.glmnet(X_train, y_train, alpha = 0, nfolds = 10)\nmod_ridge &lt;- glmnet(X_train, y_train, alpha = 0, lambda = cv_ridge$lambda.min)\npred_ridge_train &lt;- as.numeric(predict(mod_ridge, newx = X_train))\npred_ridge_test  &lt;- as.numeric(predict(mod_ridge, newx = X_test))\n\n\n# ---- Performance (MAE, RMSE) using eval_perform / eval_perf ----\nperf_ridge_train &lt;- eval_perform(y_train, pred_ridge_train) |&gt;\n  dplyr::mutate(Model = \"Ridge\", Dataset = \"Train\")\n\nperf_ridge_test &lt;- eval_perform(y_test, pred_ridge_test) |&gt;\n  dplyr::mutate(Model = \"Ridge\", Dataset = \"Test\")\n\ndplyr::bind_rows(perf_ridge_train, perf_ridge_test)\n\n# A tibble: 2 × 4\n    MAE  RMSE Model Dataset\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  \n1  1.43  1.79 Ridge Train  \n2  1.77  2.23 Ridge Test   \n\n# ---- Quick model “summary” for discussion ----\n# Lambdas chosen by CV\ncv_ridge$lambda.min\n\n[1] 1.82942\n\ncv_ridge$lambda.1se\n\n[1] 2.007787\n\n# Coefficients at lambda.min\nridge_coefs &lt;- as.matrix(coef(mod_ridge))\nridge_coef_tbl &lt;- tibble::tibble(\n  term     = rownames(ridge_coefs),\n  estimate = as.numeric(ridge_coefs[, 1])\n)\n\n# How many non-zero coefficients (excluding intercept)\nnnz &lt;- ridge_coef_tbl |&gt;\n  dplyr::filter(term != \"(Intercept)\", estimate != 0) |&gt;\n  nrow()\n\nnnz\n\n[1] 2006\n\n# Top coefficients by absolute magnitude (exclude intercept)\nridge_top &lt;- ridge_coef_tbl |&gt;\n  dplyr::filter(term != \"(Intercept)\") |&gt;\n  dplyr::mutate(abs_est = abs(estimate)) |&gt;\n  dplyr::arrange(dplyr::desc(abs_est)) |&gt;\n  dplyr::slice(1:15)\n## seeing the top 15 features regarding estimate\nridge_top\n\n# A tibble: 15 × 3\n   term              estimate abs_est\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n 1 dose_intensity       5.28    5.28 \n 2 treatmentchemo       5.03    5.03 \n 3 gene_14              4.33    4.33 \n 4 gene_19              4.30    4.30 \n 5 gene_08              4.29    4.29 \n 6 gene_05             -3.50    3.50 \n 7 gene_01              0.983   0.983\n 8 tumor_gradeG3        0.949   0.949\n 9 gene_12              0.526   0.526\n10 performance_score   -0.517   0.517\n11 tumor_gradeG2        0.283   0.283\n12 gene_1419            0.269   0.269\n13 gene_1506            0.233   0.233\n14 gene_1976           -0.215   0.215\n15 gene_907            -0.213   0.213\n\n\n\n# ---- 7) Elastic Net (tune alpha and lambda via CV) ----\nsuppressPackageStartupMessages({\n  library(glmnet)\n  library(dplyr)\n  library(tibble)\n  # assumes eval_perform() already defined and X_train, X_test, y_train, y_test exist\n})\n\nset.seed(42)\n\n# Grid of alpha values (0=ridge, 1=lasso)\nalpha_grid &lt;- seq(0.05, 0.95, by = 0.05)\n\n# For each alpha, run cv.glmnet to pick lambda; record CV error at lambda.min\ncv_list &lt;- lapply(alpha_grid, function(a) {\n  cv.glmnet(X_train, y_train, alpha = a, nfolds = 10)\n})\n\ncv_errors &lt;- sapply(cv_list, function(cv) min(cv$cvm))  # lower is better\nbest_idx  &lt;- which.min(cv_errors)\nbest_alpha &lt;- alpha_grid[best_idx]\nbest_cv    &lt;- cv_list[[best_idx]]\nbest_lambda_min &lt;- best_cv$lambda.min\nbest_lambda_1se &lt;- best_cv$lambda.1se\n\nbest_alpha\n\n[1] 0.15\n\nbest_lambda_min\n\n[1] 0.1811088\n\nbest_lambda_1se\n\n[1] 0.2883763\n\n# Fit final Elastic Net at best alpha/lambda\nmod_enet &lt;- glmnet(X_train, y_train, alpha = best_alpha, lambda = best_lambda_min)\n\n# Predictions\npred_enet_train &lt;- as.numeric(predict(mod_enet, newx = X_train))\npred_enet_test  &lt;- as.numeric(predict(mod_enet, newx = X_test))\n\n# Performance\nperf_enet_train &lt;- eval_perform(y_train, pred_enet_train) |&gt;\n  mutate(Model = \"Elastic Net\", Dataset = \"Train\", alpha = best_alpha, lambda = best_lambda_min)\n\nperf_enet_test &lt;- eval_perform(y_test, pred_enet_test) |&gt;\n  mutate(Model = \"Elastic Net\", Dataset = \"Test\", alpha = best_alpha, lambda = best_lambda_min)\n\nbind_rows(perf_enet_train, perf_enet_test)\n\n# A tibble: 2 × 6\n    MAE  RMSE Model       Dataset alpha lambda\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1  1.50  1.88 Elastic Net Train    0.15  0.181\n2  1.57  1.95 Elastic Net Test     0.15  0.181\n\n# Coefficients summary\nenet_coefs &lt;- as.matrix(coef(mod_enet))\nenet_coef_tbl &lt;- tibble(\n  term     = rownames(enet_coefs),\n  estimate = as.numeric(enet_coefs[, 1])\n)\n\n# Count non-zero (excluding intercept)\nenet_nnz &lt;- enet_coef_tbl |&gt;\n  filter(term != \"(Intercept)\", estimate != 0) |&gt;\n  nrow()\nenet_nnz\n\n[1] 87\n\n# Top coefficients by absolute value (exclude intercept)\nenet_top &lt;- enet_coef_tbl |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(abs_est = abs(estimate)) |&gt;\n  arrange(desc(abs_est)) |&gt;\n  slice(1:20)\n\nenet_top\n\n# A tibble: 20 × 3\n   term              estimate abs_est\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n 1 gene_19             5.08    5.08  \n 2 gene_08             5.01    5.01  \n 3 gene_14             4.98    4.98  \n 4 dose_intensity      3.54    3.54  \n 5 treatmentchemo      3.49    3.49  \n 6 gene_05            -2.92    2.92  \n 7 gene_01             0.863   0.863 \n 8 tumor_gradeG3       0.606   0.606 \n 9 gene_12             0.456   0.456 \n10 performance_score  -0.320   0.320 \n11 tumor_gradeG2       0.141   0.141 \n12 gene_1976          -0.110   0.110 \n13 gene_1419           0.107   0.107 \n14 gene_1241           0.106   0.106 \n15 gene_1428          -0.0990  0.0990\n16 gene_1778          -0.0947  0.0947\n17 gene_1506           0.0947  0.0947\n18 gene_1892           0.0793  0.0793\n19 gene_1556           0.0705  0.0705\n20 gene_30            -0.0676  0.0676",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#elastic-net",
    "href": "supervised_regression.html#elastic-net",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.11 Elastic Net",
    "text": "2.11 Elastic Net\nThe next chunk fits an Elastic Net regression and tunes its two key hyperparameters using cross-validation, then evaluates how well it predicts on unseen data and inspects the model’s coefficients.\nFirst, it defines a grid of candidate mixing parameters alpha from 0.05 to 0.95. Elastic Net blends Ridge and LASSO: alpha equal to 0 behaves like Ridge, alpha equal to 1 behaves like LASSO, values in between trade off between the two penalties. For each alpha in the grid, cv.glmnet() runs 10-fold cross-validation over a sequence of lambda values and records the mean cross-validated error. The code then picks the alpha that achieves the lowest error across its lambda path, identifies the corresponding best lambda at the minimum error (lambda.min) and also records lambda.1se which is a slightly stronger penalty within one standard error of the minimum. In your run, the best alpha is 0.15, with lambda.minabout 0.181 and lambda.1se about 0.288. This indicates that a model closer to Ridge than to LASSO gave the best cross-validated performance on this dataset.\nWith the best alpha and lambda.min fixed, the code refits a final Elastic Net model on the full training matrix and generates predictions for both training and test sets. It then computes MAE and RMSE via eval_perform(). Your results show MAE 1.50 and RMSE 1.88 on training, and MAE 1.57 and RMSE 1.95 on test. The similarity between train and test errors suggests good generalization with limited overfitting. Test performance is competitive with or slightly better than the individual Ridge and LASSO models you fitted earlier, which is a common outcome when Elastic Net can borrow strengths from both penalties in the presence of many correlated predictors.\nFinally, the code examines the coefficient vector. It converts the sparse coefficient matrix to a tibble, counts how many coefficients are non-zero excluding the intercept, and ranks predictors by absolute magnitude. You obtained 87 non-zero coefficients, which is far sparser than Ridge and far denser than a very aggressive LASSO, reflecting the balance enforced by alpha 0.15. The top effects are biologically and clinically interpretable: gene_19, gene_08, and gene_14 have the largest positive coefficients, dose_intensity and treatmentchemo are also strongly positive, while gene_05 is strongly negative, and performance_score is moderately negative. Tumor grade indicators and several additional genes appear with smaller but non-zero effects. This pattern is what Elastic Net is designed to produce in high-dimensional settings with correlated features: it keeps groups of correlated predictors, shrinks them toward zero to control variance, and still performs variable selection to improve interpretability and prediction.\n\n# ---- 7) Elastic Net (tune alpha and lambda via CV) ----\nsuppressPackageStartupMessages({\n  library(glmnet)\n  library(dplyr)\n  library(tibble)\n  # assumes eval_perform() already defined and X_train, X_test, y_train, y_test exist\n})\n\nset.seed(42)\n\n# Grid of alpha values (0=ridge, 1=lasso)\nalpha_grid &lt;- seq(0.05, 0.95, by = 0.05)\n\n# For each alpha, run cv.glmnet to pick lambda; record CV error at lambda.min\ncv_list &lt;- lapply(alpha_grid, function(a) {\n  cv.glmnet(X_train, y_train, alpha = a, nfolds = 10)\n})\n\ncv_errors &lt;- sapply(cv_list, function(cv) min(cv$cvm))  # lower is better\nbest_idx  &lt;- which.min(cv_errors)\nbest_alpha &lt;- alpha_grid[best_idx]\nbest_cv    &lt;- cv_list[[best_idx]]\nbest_lambda_min &lt;- best_cv$lambda.min\nbest_lambda_1se &lt;- best_cv$lambda.1se\n\nbest_alpha\n\n[1] 0.15\n\nbest_lambda_min\n\n[1] 0.1811088\n\nbest_lambda_1se\n\n[1] 0.2883763\n\n# Fit final Elastic Net at best alpha/lambda\nmod_enet &lt;- glmnet(X_train, y_train, alpha = best_alpha, lambda = best_lambda_min)\n\n# Predictions\npred_enet_train &lt;- as.numeric(predict(mod_enet, newx = X_train))\npred_enet_test  &lt;- as.numeric(predict(mod_enet, newx = X_test))\n\n# Performance\nperf_enet_train &lt;- eval_perform(y_train, pred_enet_train) |&gt;\n  mutate(Model = \"Elastic Net\", Dataset = \"Train\", alpha = best_alpha, lambda = best_lambda_min)\n\nperf_enet_test &lt;- eval_perform(y_test, pred_enet_test) |&gt;\n  mutate(Model = \"Elastic Net\", Dataset = \"Test\", alpha = best_alpha, lambda = best_lambda_min)\n\nbind_rows(perf_enet_train, perf_enet_test)\n\n# A tibble: 2 × 6\n    MAE  RMSE Model       Dataset alpha lambda\n  &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1  1.50  1.88 Elastic Net Train    0.15  0.181\n2  1.57  1.95 Elastic Net Test     0.15  0.181\n\n# Coefficients summary\nenet_coefs &lt;- as.matrix(coef(mod_enet))\nenet_coef_tbl &lt;- tibble(\n  term     = rownames(enet_coefs),\n  estimate = as.numeric(enet_coefs[, 1])\n)\n\n# Count non-zero (excluding intercept)\nenet_nnz &lt;- enet_coef_tbl |&gt;\n  filter(term != \"(Intercept)\", estimate != 0) |&gt;\n  nrow()\nenet_nnz\n\n[1] 87\n\n# Top coefficients by absolute value (exclude intercept)\nenet_top &lt;- enet_coef_tbl |&gt;\n  filter(term != \"(Intercept)\") |&gt;\n  mutate(abs_est = abs(estimate)) |&gt;\n  arrange(desc(abs_est)) |&gt;\n  slice(1:20)\n\nenet_top\n\n# A tibble: 20 × 3\n   term              estimate abs_est\n   &lt;chr&gt;                &lt;dbl&gt;   &lt;dbl&gt;\n 1 gene_19             5.08    5.08  \n 2 gene_08             5.01    5.01  \n 3 gene_14             4.98    4.98  \n 4 dose_intensity      3.54    3.54  \n 5 treatmentchemo      3.49    3.49  \n 6 gene_05            -2.92    2.92  \n 7 gene_01             0.863   0.863 \n 8 tumor_gradeG3       0.606   0.606 \n 9 gene_12             0.456   0.456 \n10 performance_score  -0.320   0.320 \n11 tumor_gradeG2       0.141   0.141 \n12 gene_1976          -0.110   0.110 \n13 gene_1419           0.107   0.107 \n14 gene_1241           0.106   0.106 \n15 gene_1428          -0.0990  0.0990\n16 gene_1778          -0.0947  0.0947\n17 gene_1506           0.0947  0.0947\n18 gene_1892           0.0793  0.0793\n19 gene_1556           0.0705  0.0705\n20 gene_30            -0.0676  0.0676",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#comparing-ols-lasso-ridge-and-elastic-net-regression",
    "href": "supervised_regression.html#comparing-ols-lasso-ridge-and-elastic-net-regression",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.12 Comparing OLS, LASSO, Ridge and Elastic Net Regression",
    "text": "2.12 Comparing OLS, LASSO, Ridge and Elastic Net Regression\nThis section compares the performance and interpretability of the four regression models Ordinary Least Squares (OLS), LASSO, Ridge, and Elastic Net using the same training and testing datasets. The code constructs two tables, one for the training set and one for the test set, summarizing each model’s Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). These metrics quantify how close the predicted tumor responses are to the observed values: lower numbers indicate more accurate predictions.\nOn the training data, OLS achieves the smallest apparent errors (MAE = 1.27, RMSE = 1.59), which is expected because it freely adjusts all coefficients without any penalty. However, its flexibility can also lead to overfitting, meaning it might perform worse on new, unseen data. The Ridge model (MAE = 1.43, RMSE = 1.79) and the Elastic Net (MAE = 1.50, RMSE = 1.88) show slightly higher training errors, reflecting the effect of regularization that constrains coefficient size to prevent overfitting. LASSO (MAE = 1.49, RMSE = 1.87) behaves similarly, as it shrinks and even eliminates some coefficients.\nThe test set results are more revealing, since they reflect how well each model generalizes. OLS now has the highest errors (MAE = 1.83, RMSE = 2.28), showing a clear drop in performance compared with the training data. In contrast, the regularized models maintain nearly identical errors across training and test sets. LASSO and Elastic Net both achieve MAE around 1.57 and RMSE near 1.95, while Ridge performs slightly worse but still better than OLS. This stability across datasets indicates that the regularization terms have successfully reduced overfitting, producing models that generalize better to new observations.\nBeyond predictive accuracy, each method differs in interpretability and in how easily its results can be communicated. OLS is the simplest to interpret: every coefficient represents the independent effect of a predictor on the outcome, assuming all other variables are fixed. However, in high-dimensional data such as gene-expression studies, OLS becomes unstable and hard to explain because of multicollinearity and noise. LASSO improves interpretability by setting many coefficients exactly to zero, leaving only a small, focused subset of relevant predictors that can be examined biologically or clinically. Ridge, by contrast, keeps all variables but shrinks their effects toward zero, which stabilizes the estimates but makes it harder to identify a small list of “important” predictors. Elastic Net combines both approaches, preserving correlated groups of variables while still performing selection, offering a compromise between the simplicity of LASSO and the robustness of Ridge.\nIn practical terms, these results illustrate the trade-off between fit, generalization, and interpretability. OLS fits training data best but generalizes poorly. Ridge and Elastic Net offer more stable and realistic predictions in complex, high-dimensional problems. LASSO and Elastic Net, in particular, produce more interpretable models that highlight a concise subset of genes and clinical variables that drive tumor response, making them easier to communicate in biomedical contexts where explanation is as important as prediction.\n\n# ---- 8) Side-by-side performance tables (MAE and RMSE)   now with Elastic Net ----\n# Assumes you already computed:\n#   pred_ols_train,  pred_ols_test\n#   pred_lasso_train, pred_lasso_test\n#   pred_ridge_train, pred_ridge_test\n#   pred_enet_train,  pred_enet_test\n# And (optionally) best_alpha, best_lambda_min from your Elastic Net CV\n\n# Helper: safely carry alpha/lambda for ENet if they exist\nalpha_enet  &lt;- if (exists(\"best_alpha\")) best_alpha else NA_real_\nlambda_enet &lt;- if (exists(\"best_lambda_min\")) best_lambda_min else NA_real_\n\n# Build TRAIN table\nmetrics_train &lt;- tibble(\n  Model = c(\"OLS\", \"LASSO (λ.min)\", \"Ridge (λ.min)\", \"Elastic Net\")\n) |&gt;\n  bind_cols(\n    dplyr::bind_rows(\n      eval_perf(y_train, pred_ols_train),\n      eval_perf(y_train, pred_lasso_train),\n      eval_perf(y_train, pred_ridge_train),\n      eval_perf(y_train, pred_enet_train)\n    )\n  ) |&gt;\n  mutate(Alpha = c(NA, 1.00, 0.00, alpha_enet),\n         Lambda = c(NA, NA, NA, lambda_enet))\n\n# Build TEST table\nmetrics_test &lt;- tibble(\n  Model = c(\"OLS\", \"LASSO (λ.min)\", \"Ridge (λ.min)\", \"Elastic Net\")\n) |&gt;\n  bind_cols(\n    dplyr::bind_rows(\n      eval_perf(y_test, pred_ols_test),\n      eval_perf(y_test, pred_lasso_test),\n      eval_perf(y_test, pred_ridge_test),\n      eval_perf(y_test, pred_enet_test)\n    )\n  ) |&gt;\n  mutate(Alpha = c(NA, 1.00, 0.00, alpha_enet),\n         Lambda = c(NA, NA, NA, lambda_enet))\n\n# Print results\nmetrics_train\n\n# A tibble: 4 × 5\n  Model           MAE  RMSE Alpha Lambda\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 OLS            1.27  1.59 NA    NA    \n2 LASSO (λ.min)  1.49  1.87  1    NA    \n3 Ridge (λ.min)  1.43  1.79  0    NA    \n4 Elastic Net    1.50  1.88  0.15  0.181\n\nmetrics_test\n\n# A tibble: 4 × 5\n  Model           MAE  RMSE Alpha Lambda\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 OLS            1.83  2.28 NA    NA    \n2 LASSO (λ.min)  1.57  1.96  1    NA    \n3 Ridge (λ.min)  1.77  2.23  0    NA    \n4 Elastic Net    1.57  1.95  0.15  0.181\n\n\n\n\n\n\n\n\n\n\n\nModel\nPenalty norm\nVariable selection/Regularization\nExplanation\n\n\n\n\nOLS\n\nNo\nNo regularization , all variables retained.\n\n\nRidge\nL2\nNo\nShrinks coefficients but keeps all variables.\n\n\nLasso\nL1\nYes\nL1 penalty can drive some coefficients to exactly zero.\n\n\nElastic Net\nL1 + L2 (mix)\nPartial\nCombines both effects some zeroing, some shrinkage.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#organizing-our-study-questions",
    "href": "supervised_regression.html#organizing-our-study-questions",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.13 Organizing our study questions",
    "text": "2.13 Organizing our study questions\n\n2.13.1 Main treatment effect\nIn order to answer the first study question we use the following code:\n\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(broom)\n  library(stringr)\n  library(tibble)\n  \n})\n\n# OLS: adjusted main effect of treatment (chemo vs reference)\n\ncoef_ols &lt;- tidy(mod_ols, conf.int = TRUE) %&gt;%\nfilter(grepl(\"^treatment\", term))\ncoef_ols\n\n# A tibble: 1 × 7\n  term           estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 treatmentchemo     3.47     0.420      8.26 1.90e-16     2.65      4.30\n\n# Regularized models: report the treatment coefficient (main effect)\ngrab_coef &lt;- function(m, name) {\nb &lt;- as.matrix(coef(m))\nif (!name %in% rownames(b)) return(NA_real_)\nas.numeric(b[name, 1])\n}\n\nmain_effects_tbl &lt;- tibble(\nModel = c(\"OLS\",\"LASSO\",\"Ridge\",\"Elastic Net\"),\nTreatment_Beta = c(\ncoef(mod_ols)[[\"treatmentchemo\"]],\ngrab_coef(mod_lasso, \"treatmentchemo\"),\ngrab_coef(mod_ridge, \"treatmentchemo\"),\ngrab_coef(mod_enet,  \"treatmentchemo\")\n)\n)\nmain_effects_tbl\n\n# A tibble: 4 × 2\n  Model       Treatment_Beta\n  &lt;chr&gt;                &lt;dbl&gt;\n1 OLS                   3.47\n2 LASSO                 3.11\n3 Ridge                 5.03\n4 Elastic Net           3.49\n\n\nThe last table reports the estimated main treatment effect the adjusted average difference in tumor shrinkage between patients who received chemotherapy and those who did not across four models. The OLS estimate of 3.47 percentage points suggests a clear benefit of chemotherapy after accounting for other covariates. Introducing L1 regularization with LASSO yields a slightly smaller effect of 3.11, consistent with the tendency of sparsity penalties to dampen coefficients in the presence of multicollinearity or weak predictors. Ridge regression, which applies an L2 penalty but retains all variables, produces a larger estimate of 5.03, indicating that when correlated predictors are jointly shrunk rather than selected away, the treatment signal can be expressed more strongly. Elastic Net, blending L1 and L2 penalties, returns 3.49 essentially aligning with OLS while offering greater stability than an unpenalized fit. Taken together, the results are directionally consistent and clinically coherent given the fact that regardless of modeling strategy, chemotherapy is associated with higher average tumor reduction, with the magnitude ranging from roughly three to five percentage points depending on how the method handles correlation and complexity in the predictors.\n\n\n2.13.2 Clinical covariates:\nWhat are the main-effect associations of age, tumor grade, and performance score with tumor shrinkage, controlling for treatment?\nTo answer this question we run the code below which helps us to extract the parameters associated with the variables of interest.\n\n# --- Clinical (no interactions): OLS + LASSO + RIDGE + ENET ------------------\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(broom)\n  library(stringr)\n  library(tibble)\n})\n\n# Helper: make a regex that matches `patient age` with or without backticks (case-insensitive)\nage_regex &lt;- regex(\"^`?patient_age`?$\", ignore_case = TRUE)\n\n# 1) OLS clinical table (with CIs and p-values)\n#    Keep: patient age, performance_score, and ALL tumor_grade dummies\nols_tidy &lt;- tidy(mod_ols, conf.int = TRUE)\n\nols_clinical &lt;- ols_tidy %&gt;%\n  filter(\n    str_detect(term, age_regex) |\n    term == \"performance_score\" |\n    str_detect(term, \"^tumor_grade\")\n  ) %&gt;%\n  arrange(p.value) %&gt;%\n  rename(\n    beta_ols   = estimate,\n    se_ols     = std.error,\n    t_ols      = statistic,\n    p_ols      = p.value,\n    ci_low_ols = conf.low,\n    ci_high_ols= conf.high\n  )\n\n# 2) Helper to extract glmnet coefficients by term ----------------------------\nget_glmnet_coefs &lt;- function(mod_glmnet, keep_regex = NULL, keep_exact_regex = NULL, colname = \"beta\"){\n  B &lt;- as.matrix(coef(mod_glmnet))\n  tab &lt;- tibble(term = rownames(B), !!colname := as.numeric(B[, 1])) %&gt;%\n    filter(term != \"(Intercept)\")\n  tab %&gt;%\n    filter(\n      (if (!is.null(keep_exact_regex)) str_detect(term, keep_exact_regex) else FALSE) |\n      (if (!is.null(keep_regex))       str_detect(term, keep_regex)       else FALSE)\n    )\n}\n\n# Keep exactly patient age (with/without backticks) and performance_score;\n# plus everything that starts with tumor_grade\nkeep_exact_regex &lt;- paste0(\"(\", paste(c(\"performance_score\", age_regex), collapse=\"|\"), \")\")\nkeep_regex_grade &lt;- \"^tumor_grade\"\n\nlasso_tab &lt;- get_glmnet_coefs(mod_lasso, keep_regex = keep_regex_grade,\n                              keep_exact_regex = keep_exact_regex, colname = \"beta_lasso\")\nridge_tab &lt;- get_glmnet_coefs(mod_ridge, keep_regex = keep_regex_grade,\n                              keep_exact_regex = keep_exact_regex, colname = \"beta_ridge\")\nenet_tab  &lt;- get_glmnet_coefs(mod_enet,  keep_regex = keep_regex_grade,\n                              keep_exact_regex = keep_exact_regex, colname = \"beta_enet\")\n\n# 3) Join everything by term --------------------------------------------------\nclin_compare &lt;- ols_clinical %&gt;%\n  select(term, beta_ols, se_ols, t_ols, p_ols, ci_low_ols, ci_high_ols) %&gt;%\n  left_join(lasso_tab, by = \"term\") %&gt;%\n  left_join(ridge_tab, by = \"term\") %&gt;%\n  left_join(enet_tab,  by = \"term\") %&gt;%\n  mutate(\n    beta_lasso = coalesce(beta_lasso, 0),\n    beta_ridge = coalesce(beta_ridge, 0),\n    beta_enet  = coalesce(beta_enet,  0)\n  )\n\ndata.frame(clin_compare)\n\n               term    beta_ols      se_ols     t_ols        p_ols  ci_low_ols\n1       patient_age  0.01708261 0.001573078 10.859354 3.614794e-27  0.01399868\n2 performance_score -0.40130130 0.041437926 -9.684396 5.477501e-22 -0.48253783\n3     tumor_gradeG3  0.69494056 0.074639941  9.310572 1.860192e-20  0.54861349\n4     tumor_gradeG2  0.23598487 0.066305941  3.559030 3.756879e-04  0.10599610\n  ci_high_ols  beta_lasso  beta_ridge   beta_enet\n1  0.02016653  0.01328823  0.02179867  0.01439859\n2 -0.32006476 -0.29430835 -0.51698819 -0.32020568\n3  0.84126763  0.55479520  0.94884736  0.60613069\n4  0.36597363  0.13160157  0.28258697  0.14149931\n\n\nThe last table compares the estimated effects of key clinical predictors on tumor response across four regression approaches ordinary least squares (OLS), LASSO, Ridge, and Elastic Netfocusing on main effects. The coefficients represent how much the tumor shrinkage percentage is expected to change for a one-unit increase in each variable, holding all others constant. The OLS model serves as a baseline reference. The coefficient for patient_age is positive (0.017 ± 0.0016, p ≈ 3.6 × 10⁻²⁷), indicating that, on average, each additional year of age is associated with roughly a 0.017 percentage-point increase in tumor shrinkage. Although small in magnitude, this effect is highly significant, suggesting a subtle but consistent relationship between age and treatment response. The performance_score shows a strong negative association (β = –0.40, p ≈ 5.5 × 10⁻²²), implying that patients with poorer functional status tend to experience smaller reductions in tumor size. Tumor grade, in contrast, shows clear positive effects: compared with the reference category (grade 1), both grade 2 (β = 0.24, p ≈ 3.8 × 10⁻⁴) and grade 3 (β = 0.69, p ≈ 1.9 × 10⁻²⁰) are associated with progressively greater shrinkage, consistent with more aggressive tumors responding more markedly to treatment. When regularization is introduced, the three penalized methods yield broadly similar patterns but with slightly shrunk coefficients. LASSO reduces effect magnitudes toward zero, particularly for performance_score (–0.29) and tumor_gradeG2 (0.13), reflecting its tendency to suppress weaker signals. Ridge regression, which applies a smooth L2 penalty, retains all variables and yields somewhat larger estimates (e.g., tumor_gradeG3 = 0.95), showing how correlated predictors can share information without being zeroed out. Elastic Net, which blends LASSO and Ridge penalties, produces intermediate values that balance sparsity and stability (e.g., performance_score = –0.32, tumor_gradeG3 = 0.61). Across all models, the direction of effects remains stable: older age and higher tumor grade predict better tumor shrinkage, while poorer performance status predicts worse response. Regularization mainly reduces the absolute size of coefficients but does not alter their interpretation. These results reinforce the robustness of the main clinical signals age, performance status, and tumor grade as reliable determinants of treatment response, even under different modeling assumptions and penalty schemes.\n\n# --- Q3. Molecular biomarkers: which genes are associated with response? ----\n# Goal: identify gene predictors associated with continuous response (response_percent)\n# Models used: OLS (with p-values and BH-FDR), LASSO, Ridge, Elastic Net (coefficients)\n# Assumptions:\n#   - Genes are encoded as columns whose names start with \"gene_\" in X_train/X_test\n#   - No interactions in the model\n#   - mod_ols was fit with response_percent ~ clinical + genes (no interactions)\n#   - mod_lasso, mod_ridge, mod_enet were fit with glmnet on (X_train, y_train)\nsuppressPackageStartupMessages({\n  library(dplyr)\n  library(stringr)\n  library(tibble)\n  library(broom)\n  library(purrr)\n})\n\n# 0) Identify gene columns from the model matrix you used to fit glmnet\ngene_terms &lt;- colnames(X_train)[str_detect(colnames(X_train), \"^gene_\")]\n\n# --- Helper to extract glmnet coefs for a list of terms ----------------------\nget_glmnet_coefs &lt;- function(mod_glmnet, terms_keep, colname = \"beta\") {\n  B &lt;- as.matrix(coef(mod_glmnet))\n  tibble(term = rownames(B), !!colname := as.numeric(B[, 1])) %&gt;%\n    filter(term != \"(Intercept)\", term %in% terms_keep)\n}\n\n# 1) OLS gene table with t-stat and FDR ---------------------------------------\nols_genes &lt;- tidy(mod_ols, conf.int = TRUE) %&gt;%\n  filter(term %in% gene_terms) %&gt;%\n  mutate(\n    beta_ols = estimate,\n    se_ols   = std.error,\n    t_ols    = statistic,\n    p_ols    = p.value,\n    fdr_bh   = p.adjust(p_ols, method = \"BH\"),\n    imp_ols  = abs(t_ols)             # importance = |t|\n  ) %&gt;%\n  select(term, beta_ols, se_ols, t_ols, p_ols, fdr_bh, imp_ols)\n\ntop10_ols &lt;- ols_genes %&gt;%\n  arrange(desc(imp_ols)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(Model = \"OLS\") %&gt;%\n  select(Model, term, beta = beta_ols, importance = imp_ols, t_ols, p_ols, fdr_bh)\n\n# 2) LASSO / Ridge / Elastic Net gene coefficients (importance = |beta|) ------\nlasso_genes &lt;- get_glmnet_coefs(mod_lasso, gene_terms, colname = \"beta_lasso\") %&gt;%\n  mutate(imp_lasso = abs(beta_lasso))\nridge_genes &lt;- get_glmnet_coefs(mod_ridge, gene_terms, colname = \"beta_ridge\") %&gt;%\n  mutate(imp_ridge = abs(beta_ridge))\nenet_genes  &lt;- get_glmnet_coefs(mod_enet,  gene_terms, colname = \"beta_enet\")  %&gt;%\n  mutate(imp_enet  = abs(beta_enet))\n\ntop10_lasso &lt;- lasso_genes %&gt;%\n  arrange(desc(imp_lasso)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(Model = \"LASSO\") %&gt;%\n  select(Model, term, beta = beta_lasso, importance = imp_lasso)\n\ntop10_ridge &lt;- ridge_genes %&gt;%\n  arrange(desc(imp_ridge)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(Model = \"Ridge\") %&gt;%\n  select(Model, term, beta = beta_ridge, importance = imp_ridge)\n\ntop10_enet &lt;- enet_genes %&gt;%\n  arrange(desc(imp_enet)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(Model = \"Elastic Net\") %&gt;%\n  select(Model, term, beta = beta_enet, importance = imp_enet)\n\n# 3) Combine into a single tidy table -----------------------------------------\ntop10_all_models &lt;- bind_rows(\n  top10_ols,\n  top10_lasso,\n  top10_ridge,\n  top10_enet\n) %&gt;%\n  # rank within model by importance (1 = most important)\n  group_by(Model) %&gt;%\n  arrange(desc(importance), .by_group = TRUE) %&gt;%\n  mutate(rank = row_number()) %&gt;%\n  ungroup()\n\n# View the result (one long table with 10 rows per model)#\n#top10_all_models\n\n# Optional: a wide, side-by-side comparison (terms only) ----------------------\ntop10_wide_terms &lt;- top10_all_models %&gt;%\n  arrange(Model, rank) %&gt;%\n  select(Model, rank, term) %&gt;%\n  tidyr::pivot_wider(names_from = Model, values_from = term)\n\ndata.frame(top10_wide_terms)\n\n   rank Elastic.Net     LASSO       OLS     Ridge\n1     1     gene_19   gene_19   gene_14   gene_14\n2     2     gene_08   gene_08   gene_08   gene_19\n3     3     gene_14   gene_14   gene_19   gene_08\n4     4     gene_05   gene_05   gene_05   gene_05\n5     5     gene_01   gene_01   gene_01   gene_01\n6     6     gene_12   gene_12   gene_12   gene_12\n7     7   gene_1976 gene_1778 gene_1024 gene_1419\n8     8   gene_1419 gene_1241   gene_27 gene_1506\n9     9   gene_1241 gene_1428 gene_1419 gene_1976\n10   10   gene_1428 gene_1976 gene_1927  gene_907\n\n# Optional: barplot for a quick visual per model ------------------------------\n# (Make sure ggplot2 is loaded.)\n# ggplot(top10_all_models, aes(x = reorder(term, importance), y = importance)) +\n#   geom_col() +\n#   coord_flip() +\n#   facet_wrap(~ Model, scales = \"free_y\") +\n#   labs(x = \"Gene\", y = \"Importance (|t| for OLS; |β| for penalized models)\",\n#        title = \"Top 10 genes by model\") +\n#   theme_minimal(base_size = 12)\n\nThe last table compares the top ten genes most strongly associated with tumor response according to four regression models OLS, LASSO, Ridge, and Elastic Net based on the magnitude of each gene’s estimated effect. While the specific coefficient values are not shown here, the ranking reveals how different modeling strategies emphasize or penalize predictors in slightly distinct ways. A clear pattern emerges across all methods: several genes consistently appear among the most influential. gene_19, gene_08, gene_14, gene_05, gene_01, and gene_12 are common to nearly every model, suggesting that these markers carry robust predictive information about treatment response. Their presence across OLS (which estimates effects freely), LASSO and Elastic Net (which perform variable selection), and Ridge (which shrinks but retains all coefficients) indicates that these signals are not artifacts of a particular modeling assumption but stable features of the dataset. The agreement among Elastic Net and LASSO is especially notable. Both methods rely on sparsity-inducing penalties, and they rank genes gene_19, gene_08, gene_14, gene_05, gene_01, and gene_12 in nearly identical order. This consistency supports the interpretation that these genes represent the core set most predictive of tumor shrinkage. The few differences such as the inclusion of gene_1976 or gene_1241 in the Elastic Net list reflect the method’s ability to keep correlated predictors that LASSO might exclude. The Ridge model, which shrinks coefficients without setting any to zero, yields a slightly more diverse list. It retains the same leading genes (gene_14, gene_19, gene_08, gene_05, gene_01, gene_12) but also highlights additional candidates such as gene_1419, gene_1506, gene_1976, and gene_907. This broader selection reflects Ridge’s tendency to distribute importance among correlated features, capturing groups of genes that may be co-expressed or functionally linked. Finally, OLS, which lacks regularization, identifies a very similar top tier dominated by gene_14, gene_08, and gene_19. Its overlap with the penalized models suggests that these genes have both strong individual associations and stability under penalization key indicators of biological and statistical relevance. In summary, despite methodological differences, all models converge on a common biological signature: a small set of genes (notably gene_19, gene_08, gene_14, gene_05, gene_01, and gene_12) emerge as consistent predictors of tumor response. The regularized approaches (especially Elastic Net) reinforce their robustness while reducing noise from less informative or redundant variables. Together, these findings highlight a reproducible molecular profile potentially linked to therapeutic sensitivity.\n\n\n2.13.3 Predictive modeling:\nGiven a patient’s main-effect clinical and molecular profile, how well do OLS, Ridge, LASSO, and Elastic Net predict tumor shrinkage (MAE/RMSE on train/test).\nThe answert to this question relates directly with the comparison we did regarding OLS, Rige, LASSO and Elastic net MAE and RMSEs.\n\n# Print results\nmetrics_train\n\n# A tibble: 4 × 5\n  Model           MAE  RMSE Alpha Lambda\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 OLS            1.27  1.59 NA    NA    \n2 LASSO (λ.min)  1.49  1.87  1    NA    \n3 Ridge (λ.min)  1.43  1.79  0    NA    \n4 Elastic Net    1.50  1.88  0.15  0.181\n\nmetrics_test\n\n# A tibble: 4 × 5\n  Model           MAE  RMSE Alpha Lambda\n  &lt;chr&gt;         &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 OLS            1.83  2.28 NA    NA    \n2 LASSO (λ.min)  1.57  1.96  1    NA    \n3 Ridge (λ.min)  1.77  2.23  0    NA    \n4 Elastic Net    1.57  1.95  0.15  0.181",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "supervised_regression.html#logistic-regression",
    "href": "supervised_regression.html#logistic-regression",
    "title": "2  Supervised Learning: Regression tasks",
    "section": "2.14 Logistic Regression",
    "text": "2.14 Logistic Regression\nLogistic regression models the probability that a patient is a high responder using a logistic ( S -shaped) link. Instead of predicting a continuous shrinkage value, we now predict \\(P(Y=1 \\mid X)\\), where \\(Y=1\\) means a clinically meaningful tumor reduction. The model is linear on the log-odds scale:\n\\[\n\\operatorname{Pr}(Y=1 \\mid X)=\\frac{1}{1+\\exp \\left\\{-\\left(w_0+w_1 x_1+\\cdots+w_p x_p\\right)\\right\\}}\n\\]\nCoefficients remain interpretable: a positive \\(w_j\\) increases the log-odds, which increases the probability of high response, holding other variables fixed. We will fit a clean, leakage-free specification that excludes identifiers and any variables used to compute the continuous outcome.\nAn alternative representation for a logistic regression model is the following diagram that has a structure of a network. We will come back this in later chapters. Can you guess which kind of networks is this?\n\n\n\n\n\nflowchart LR\n    x1[\"x₁\"] --&gt; w1[\"× w₁\"]\n    x2[\"x₂\"] --&gt; w2[\"× w₂\"]\n    xp[\"xₚ\"] --&gt; wp[\"× wₚ\"]\n\n    w1 --&gt; SUM\n    w2 --&gt; SUM\n    wp --&gt; SUM\n\n    SUM[\"Σ (wⱼ xⱼ + b)\"] --&gt; ACT[\"σ ( · )  (sigmoid)\"]\n    ACT --&gt; y[\"ŷ\"]\n\n\n\n\n\n\n\nMetrics we will use to compare models We will evaluate models with threshold-based metrics and threshold-free curves.\n\nAccuracy: fraction of correct classifications.\nSensitivity (recall): fraction of true positives captured.\nSpecificity: fraction of true negatives correctly rejected.\nPrecision (PPV): among predicted positives, fraction that are truly positive.\nF1: harmonic mean of precision and recall.\nROC and AUC: trade-off between TPR and FPR across all thresholds; AUC summarizes discrimination from 0.5 (random) a 1.0 (perfeito).\nPrecision-Recall and AUC-PR: useful when we have unbalanced data, focus on performance in the rare set.\n\n\n2.14.1 Understanding ROC and AUC through examples\nThe ROC curve (Receiver Operating Characteristic) and the AUC (Area Under the Curve) are fundamental tools for evaluating binary classification models. The series of plots generated by the next blocks of code illustrates how ROC curves behave under different modeling situations, helping us understand what a “good,” “bad,” or “misleading” model looks like. The examples are from simulated data and not related to our motivational context.\nEach plot shows sensitivity (true positive rate) on the vertical axis and 1 – specificity (false positive rate) on the horizontal axis. A diagonal line represents the performance of a random classifier. Curves that rise sharply toward the upper-left corner correspond to models that discriminate well between positive and negative cases. The AUC value, printed in the title of each facet, summarizes this ability numerically.\nIn the first scenario, labeled Excellent model, the two classes are highly separable. T\nThe name “logistic” comes from the logit, or log-odds, transformation that the model uses. Instead of modeling the probability itself, it models the log of the odds of success (for example, the log of the odds of being a high responder). This logit is linear in the predictors, which means we can still use familiar regression ideas while keeping the predictions bounded between 0 and 1\n\n\n2.14.2 Logistic Regression in our motivating example\nOur response variable is now high_response (binary, 0/1): equal to 1 if response_percent ≥ 30(similar to RECIST clinical criteria) Eisenhauer et al. (2009).\nThe next chunk of code makes sure that the response is binary, and prepare the datasets for analysis and the formula we will use in the R function `glm` that will fit the logistic regression for us.\nIn logistic regression, the model does not minimize squared errors like ordinary least squares does. Instead, it estimates the coefficients \\(\\boldsymbol{\\beta}\\) that make the observed data most probable under the assumed binomial distribution, a distribution that models binary outcomes. This is achieved through Maximum Likelihood Estimation (MLE). In essence, MLE finds the set of parameters \\(\\beta\\) that maximize the log-likelihood function, which measures how well the model’s predicted probabilities align with the actual outcomes. For each observation, the model computes the probability of belonging to class 1 (success) as \\(\\hat{p}_i=\\frac{1}{1+e^{-\\left(x_i^T w\\right)}}\\); the log-likelihood then accumulates the logarithm of these probabilities across all samples. Maximizing this quantity ensures that the estimated coefficients produce predicted probabilities that are as consistent as possible with the observed binary responses.\n\n\n2.14.3 ROC and AUC\nBefore moving to the code that implements these methods, it is useful to understand two fundamental concepts for evaluating the performance of binary classification models: the Receiver Operating Characteristic (ROC) curve and the Area Under the Curve (AUC).\nWhen a model predicts probabilities such as the likelihood that a patient will show a high tumor response it is not limited to a single classification threshold (for example, 0.5). Instead, the threshold can vary. For each possible threshold, the model produces a different balance between sensitivity (the proportion of true responders correctly identified) and specificity (the proportion of non-responders correctly rejected).\nThe ROC curve visualizes this trade-off. It plots sensitivity on the vertical axis against 1 – specificity on the horizontal axis, across all thresholds from 0 to 1. Each point on the curve represents a possible decision rule. A model that predicts perfectly has a curve that rises immediately to the top-left corner of the plot (sensitivity = 1, specificity = 1). A model that performs no better than random chance follows the diagonal line from (0, 0) to (1, 1).\nThe Area Under the ROC Curve (AUC) condenses this information into a single number between 0 and 1. An AUC of 1 indicates perfect discrimination: the model always ranks true responders above non-responders. An AUC of 0.5 corresponds to random guessing. Values between 0.7 and 0.8 are generally considered acceptable, 0.8 to 0.9 good, and above 0.9 excellent, though interpretation depends on the context and the consequences of errors.\nIn the context of our clinical trial, the ROC curve tells us how well the logistic regression model can distinguish patients who achieve a meaningful tumor reduction from those who do not. The AUC gives an overall summary of this discrimination ability, independent of any specific threshold. It is particularly useful in medicine, where the decision threshold may later be adjusted to achieve a desired balance between missing true responders (false negatives) and incorrectly labeling non-responders as high responders (false positives).\nhe ROC curve quickly approaches the top-left corner, and the AUC is close to 1. This means the model ranks nearly all true responders above non-responders, providing excellent discrimination.\nIn the second panel, Reasonable model, the curve still bows above the diagonal, but less dramatically. The AUC is around 0.8–0.9, which is common in real clinical prediction problems. This represents a solid model that balances sensitivity and specificity reasonably well.\nThe third example, Near random, has an ROC curve close to the diagonal with an AUC around 0.5. This model is essentially guessing; its predictions carry no discriminative information beyond random chance.\nThe fourth panel, Inverted score, shows a curve that falls below the diagonal. Here the model systematically reverses the ordering of cases high probabilities are assigned to negatives and low probabilities to positives. In practice, such a model can be “fixed” simply by reversing its decision rule, but its presence is a clear sign that something in the data or labeling is inverted.\nThe fifth case, Imbalanced classes, demonstrates a subtle but important limitation of the ROC curve. Even with a heavily skewed dataset (for example, many more non-responders than responders), the ROC may still appear fairly high. However, when the number of positive cases is small, precision the proportion of predicted positives that are correct can drop sharply even if the ROC looks good. For this reason, the code also produces Precision–Recall (PR) curves, which tend to give a clearer picture when class imbalance is severe. In these PR plots, precision is shown against recall, and a high-quality model maintains both values simultaneously at high levels.\nThe final figure illustrates the effect of changing the decision threshold. Each dot along the ROC curve corresponds to a different cutoff value for the predicted probability. Lowering the threshold increases sensitivity (the model captures more true responders) but also raises the false positive rate. Raising the threshold has the opposite effect: fewer false alarms, but more missed responders. Choosing the “best” threshold depends on clinical priorities whether we prefer to err on the side of overtreatment (high sensitivity) or undertreatment (high specificity).\nTaken together, these plots demonstrate that ROC curves and AUC values offer a concise but nuanced summary of model discrimination. A high AUC indicates that the model generally ranks positive cases above negative ones, but the exact trade-off between sensitivity and specificity still depends on how we set the classification threshold. Complementing ROC and AUC with PR curves and context-specific thresholds ensures a more complete understanding of predictive performance, especially in medical settings where the balance between false positives and false negatives carries real clinical implications.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.14.4 Penalized logistic models: LASSO, Ridge, and Elastic Net\nHigh dimensionality and correlated features make it hard to interpret thousands of coefficients and can hurt generalization. Penalized logistic regression addresses this by shrinking coefficients. Ridge (L2) shrinks all coefficients toward zero without removing variables. LASSO (L1) can set some coefficients exactly to zero, performing embedded feature selection. Elastic Net blends both penalties and is helpful when many predictors are correlated. We fit all three using the binomial family so the comparison with logistic regression is fair. We always evaluate probabilities from these models in ROC and PR calculations.\n\n\n2.14.5 Fitting Logistic Regression and equivalent LASSO, RIDGE,and Elastic Net Approaches\nTo make the results reproducible by users with any kind of machine we will first create a smaller version of our dataset containing only 100 genes.\nThe next chunk reads the smaller version of the data. In this dataset we have a subset of the 1000 genes, this is the reason for genes names not being 1….2000.\n\n# === Load the reduced dataset and quick checks ================================\nsuppressPackageStartupMessages({\n  library(dplyr)\n})\n\nct_reduced &lt;- readRDS(\"~/att_ai_ml/data/ct_reduced_v1.rds\")\n\n# Basic sanity checks\ndim(ct_reduced)\n\n[1] 10000   106\n\nnames(ct_reduced)[1:min(20, ncol(ct_reduced))]\n\n [1] \"treatment\"         \"dose_intensity\"    \"patient_age\"      \n [4] \"tumor_grade\"       \"performance_score\" \"gene_1242\"        \n [7] \"gene_397\"          \"gene_1754\"         \"gene_355\"         \n[10] \"gene_308\"          \"gene_234\"          \"gene_1598\"        \n[13] \"gene_1551\"         \"gene_588\"          \"gene_779\"         \n[16] \"gene_599\"          \"gene_2000\"         \"gene_1190\"        \n[19] \"gene_1568\"         \"gene_1321\"        \n\nstr(ct_reduced)\n\n'data.frame':   10000 obs. of  106 variables:\n $ treatment        : Factor w/ 2 levels \"no_chemo\",\"chemo\": 2 1 1 1 2 2 1 2 1 2 ...\n $ dose_intensity   : num  1.08 0 0 0 1.01 ...\n $ patient_age      : num  81 61 81 74 41 74 22 61 26 22 ...\n $ tumor_grade      : Factor w/ 3 levels \"G1\",\"G2\",\"G3\": 2 2 3 2 2 2 1 2 3 2 ...\n $ performance_score: int  1 1 1 0 1 1 2 0 1 0 ...\n $ gene_1242        : num  9.15 6.34 7.55 4.64 6.12 ...\n $ gene_397         : num  6.34 8.44 7.13 9.8 9.67 ...\n $ gene_1754        : num  5.34 8.27 6.9 5.74 5.49 ...\n $ gene_355         : num  8.06 9.06 8.91 6.84 7.21 ...\n $ gene_308         : num  10.39 8.81 9.73 7.17 7.9 ...\n $ gene_234         : num  8.38 8.83 8.4 10.43 10.81 ...\n $ gene_1598        : num  7.56 11 9.45 9.79 9.41 ...\n $ gene_1551        : num  5.96 9.13 7.99 7.87 7.28 ...\n $ gene_588         : num  8.79 4.28 6.6 5.04 5.94 ...\n $ gene_779         : num  8.29 4.05 5.63 4.39 5.12 ...\n $ gene_599         : num  9.35 5.89 7.31 5.74 6.55 ...\n $ gene_2000        : num  9.63 7.43 8.14 6.3 6.44 ...\n $ gene_1190        : num  7.8 6.28 7.72 5.2 5.3 ...\n $ gene_1568        : num  8.62 7.09 8.41 5.41 5.97 ...\n $ gene_1321        : num  6.94 4.74 5.6 6.14 6.53 ...\n $ gene_1379        : num  7.63 4.72 5.91 4.12 4.64 ...\n $ gene_746         : num  6.62 9.59 8.43 9.21 9.36 ...\n $ gene_1480        : num  12.8 11.9 12.5 13.6 13.9 ...\n $ gene_1973        : num  7.73 9.63 9.2 11.02 10.55 ...\n $ gene_1448        : num  8.36 9.11 9.25 10.44 10.87 ...\n $ gene_1733        : num  8.62 9.31 9.51 7.86 8.09 ...\n $ gene_41          : num  9.21 7.15 8.6 5.2 5.9 ...\n $ gene_1668        : num  7.02 7.55 7.11 8.68 8.46 ...\n $ gene_1400        : num  7.15 9.14 9.23 7.51 7.77 ...\n $ gene_1716        : num  5.66 8.06 7.21 7 6.89 ...\n $ gene_1213        : num  8.31 9.42 9 6.85 7.21 ...\n $ gene_873         : num  8.9 9.47 8.53 10.9 10.53 ...\n $ gene_207         : num  8.2 4.94 6.85 4.77 5.29 ...\n $ gene_609         : num  9.53 5.43 7.58 7.2 7.43 ...\n $ gene_1923        : num  6.72 9.03 8.05 9.67 9.14 ...\n $ gene_772         : num  8.3 5.56 6.6 4.61 5.24 ...\n $ gene_1039        : num  6.4 2.99 3.99 4.44 4.66 ...\n $ gene_1291        : num  7.87 5.9 7.13 4.26 5.25 ...\n $ gene_58          : num  7.87 5.55 6.54 4.29 4.83 ...\n $ gene_1760        : num  8.53 11.48 10.64 11 10.96 ...\n $ gene_1232        : num  6.9 10.18 8.68 9.72 9.23 ...\n $ gene_1925        : num  6.74 9.46 8.76 9.63 9.39 ...\n $ gene_340         : num  8.04 5.27 6.25 4.5 5.5 ...\n $ gene_663         : num  6.73 5.7 5.92 7.5 6.88 ...\n $ gene_681         : num  9.33 11.85 10.72 11.88 11.28 ...\n $ gene_1596        : num  8.57 6.87 7.99 8.61 9.36 ...\n $ gene_821         : num  9.9 9 9.22 7.91 7.64 ...\n $ gene_94          : num  7.07 10.25 9.02 9.21 8.83 ...\n $ gene_814         : num  9.67 9.43 9.13 11.23 11.75 ...\n $ gene_719         : num  7.67 9.99 9.59 10.78 10.31 ...\n $ gene_122         : num  10.63 7.86 9.68 9.93 10.12 ...\n $ gene_1240        : num  8.11 9.81 9.22 7.75 7.62 ...\n $ gene_577         : num  7.99 7.24 7.91 5.92 5.57 ...\n $ gene_1233        : num  8.47 9.85 9.58 8.99 8.55 ...\n $ gene_1811        : num  5.15 9.11 6.91 7.83 7.81 ...\n $ gene_713         : num  9.17 9.19 9.81 8.37 7.77 ...\n $ gene_819         : num  6.83 6.82 7.52 5.99 5.88 ...\n $ gene_1196        : num  8.61 5.52 7.53 5.67 6.2 ...\n $ gene_56          : num  9.82 11.92 10.52 12.27 11.62 ...\n $ gene_1188        : num  7.96 7.07 7.63 5.88 5.97 ...\n $ gene_1319        : num  7.12 6.62 7.17 8.65 8.2 ...\n $ gene_1820        : num  9.55 9.21 9.55 7.33 7.9 ...\n $ gene_926         : num  6.4 8.88 7.85 8.8 8.17 ...\n $ gene_1698        : num  5.39 4.88 5.97 3.48 4.02 ...\n $ gene_1690        : num  8.9 7.24 8.09 6.53 7.3 ...\n $ gene_1758        : num  7.05 6.13 6.55 4.88 5.44 ...\n $ gene_1909        : num  8.39 11.61 10.65 11.21 10.83 ...\n $ gene_1003        : num  6.47 9.16 7.79 8.45 8.03 ...\n $ gene_890         : num  7.18 9.41 9.01 10.49 9.55 ...\n $ gene_186         : num  7.28 7.69 7.89 6.38 6.82 ...\n $ gene_1912        : num  7.82 5.25 7.02 4.84 5.09 ...\n $ gene_787         : num  8.28 4.86 6.78 5.39 6.16 ...\n $ gene_967         : num  5.7 6.21 6.14 4.4 4.78 ...\n $ gene_233         : num  7.88 5.22 6.81 7.11 7.25 ...\n $ gene_564         : num  8.28 10.65 9.85 9.52 9.29 ...\n $ gene_307         : num  9.96 10.27 10.56 7.9 8.56 ...\n $ gene_1099        : num  8.56 10.69 9.48 10.21 9.53 ...\n $ gene_1460        : num  6.46 3.58 4.58 3.36 4.57 ...\n $ gene_1830        : num  5.11 7 6.18 7.59 7.26 ...\n $ gene_289         : num  7.53 4.57 5.83 6.41 6.27 ...\n $ gene_1740        : num  10.31 8.9 10.31 7.95 8.67 ...\n $ gene_1116        : num  11.36 9.97 10.66 12.17 11.82 ...\n $ gene_1946        : num  10.12 9.64 9.98 7.82 8.42 ...\n $ gene_371         : num  10.82 8.71 9.29 10.47 10.59 ...\n $ gene_1033        : num  12.3 13.8 13.2 14.8 14.2 ...\n $ gene_579         : num  8.39 9.28 9.36 8.57 9.26 ...\n $ gene_333         : num  8.79 8.73 8.16 9.2 8.9 ...\n $ gene_1222        : num  6.78 7.3 7.32 6.71 6.57 ...\n $ gene_1139        : num  10.48 9.69 10.02 10.82 10.64 ...\n $ gene_1302        : num  8.87 6.51 7.46 7.85 8.06 ...\n $ gene_1215        : num  7.42 8.53 8.2 7.68 7.62 ...\n $ gene_655         : num  7.44 5.14 5.82 5.39 5.6 ...\n $ gene_50          : num  7.93 7.18 7.6 7.37 7.6 ...\n $ gene_1195        : num  7.31 8.83 8.35 9.25 8.63 ...\n $ gene_1380        : num  6.96 5.45 6.14 5.2 5.33 ...\n $ gene_24          : num  10.34 9.81 9.98 9.37 9.3 ...\n $ gene_1940        : num  7.64 6.54 7.27 6.62 6.88 ...\n $ gene_370         : num  4.34 5 4.65 5.19 5.09 ...\n $ gene_1687        : num  8.6 9.92 9.11 10.74 10.32 ...\n  [list output truncated]\n - attr(*, \"created_with\")= chr \"variance+random gene selection (80+20), target_total_genes=100\"\n - attr(*, \"created_at\")= POSIXct[1:1], format: \"2025-11-11 11:42:34\"\n\n\nFirst we force the data to be binary\n\n# === Ensure binary target and remove any accidental leakage ===================\n# ct_reduced SHOULD already include 'high_response' (0/1). We enforce 0/1 safely.\n\nto_binary01 &lt;- function(x) {\n  if (is.logical(x)) return(as.integer(x))\n  if (is.factor(x))  return(as.integer(as.numeric(x) == max(as.numeric(x))))\n  as.integer(x)\n}\n\nif (!\"high_response\" %in% names(ct_reduced)) {\n  stop(\"`high_response` is not in ct_reduced. Recreate ct_reduced with the target.\")\n}\n\nct_reduced &lt;- ct_reduced %&gt;%\n  mutate(high_response = to_binary01(high_response))\n\n# If any of these exist, drop them for modeling as potential leakage/IDs.\nleak_or_id &lt;- intersect(\n  c(\"patient_id\", \"response_percent\", \"baseline_tumor_mm\", \"post_tumor_mm\"),\n  names(ct_reduced)\n)\nif (length(leak_or_id)) {\n  message(\"Dropping potential leakage/ID columns: \", paste(leak_or_id, collapse = \", \"))\n  ct_reduced &lt;- dplyr::select(ct_reduced, -all_of(leak_or_id))\n}\n\n# Optional: remove zero-variance predictors (defensive)\nnzv &lt;- function(x) is.numeric(x) && (sd(x, na.rm = TRUE) == 0)\ndrop_nzv &lt;- names(ct_reduced)[vapply(ct_reduced, nzv, logical(1))]\nif (length(drop_nzv)) {\n  message(\"Dropping zero-variance numeric columns: \", paste(drop_nzv, collapse = \", \"))\n  ct_reduced &lt;- dplyr::select(ct_reduced, -all_of(drop_nzv))\n}\n\nAfter reading and preparing the data we proceed with the division of the dataset into training and testing dataset.\n\n\n\n# === Stratified train/test split (e.g., 70/30) ================================\nset.seed(42)\nprop_train &lt;- 0.70\n\nidx_train &lt;- ct_reduced %&gt;%\n  mutate(row_id = dplyr::row_number()) %&gt;%\n  group_by(high_response) %&gt;%\n  slice_sample(prop = prop_train) %&gt;%\n  ungroup() %&gt;%\n  pull(row_id)\n\ntrain_df &lt;- ct_reduced[idx_train, , drop = FALSE]\ntest_df  &lt;- ct_reduced[-idx_train, , drop = FALSE]\n\n# Quick balance check\ntable_train &lt;- table(train_df$high_response)\ntable_test  &lt;- table(test_df$high_response)\ntable_train; table_test\n\n\n   0    1 \n4385 2614 \n\n\n\n   0    1 \n1880 1121 \n\n\nNow we will build the model formula to be used by R\n\n# === Build the modeling formula (use everything except the target) ============\npredictors &lt;- setdiff(names(train_df), \"high_response\")\nrhs &lt;- paste(predictors, collapse = \" + \")\nf_logit &lt;- as.formula(paste(\"high_response ~\", rhs))\nf_logit\n\nhigh_response ~ treatment + dose_intensity + patient_age + tumor_grade + \n    performance_score + gene_1242 + gene_397 + gene_1754 + gene_355 + \n    gene_308 + gene_234 + gene_1598 + gene_1551 + gene_588 + \n    gene_779 + gene_599 + gene_2000 + gene_1190 + gene_1568 + \n    gene_1321 + gene_1379 + gene_746 + gene_1480 + gene_1973 + \n    gene_1448 + gene_1733 + gene_41 + gene_1668 + gene_1400 + \n    gene_1716 + gene_1213 + gene_873 + gene_207 + gene_609 + \n    gene_1923 + gene_772 + gene_1039 + gene_1291 + gene_58 + \n    gene_1760 + gene_1232 + gene_1925 + gene_340 + gene_663 + \n    gene_681 + gene_1596 + gene_821 + gene_94 + gene_814 + gene_719 + \n    gene_122 + gene_1240 + gene_577 + gene_1233 + gene_1811 + \n    gene_713 + gene_819 + gene_1196 + gene_56 + gene_1188 + gene_1319 + \n    gene_1820 + gene_926 + gene_1698 + gene_1690 + gene_1758 + \n    gene_1909 + gene_1003 + gene_890 + gene_186 + gene_1912 + \n    gene_787 + gene_967 + gene_233 + gene_564 + gene_307 + gene_1099 + \n    gene_1460 + gene_1830 + gene_289 + gene_1740 + gene_1116 + \n    gene_1946 + gene_371 + gene_1033 + gene_579 + gene_333 + \n    gene_1222 + gene_1139 + gene_1302 + gene_1215 + gene_655 + \n    gene_50 + gene_1195 + gene_1380 + gene_24 + gene_1940 + gene_370 + \n    gene_1687 + gene_170 + gene_643 + gene_1617 + gene_426 + \n    gene_934 + gene_1373\n\n\n\n\n2.14.6 Fitting logistic regression\nIn order to fit the logistic regression we use the R function `glm` that is the short for generalized linear models, a generic type of models from which logistic regression is a specific example.\n\n\n\n# === Fit logistic regression (glm) and get predictions =======================\nsuppressPackageStartupMessages({\n  library(broom)   # tidy output\n})\n\nmod_logit &lt;- glm(f_logit,\n                 data   = train_df,\n                 family = binomial(),\n                 control = glm.control(maxit = 100))  # a few more iterations can help\n\n# Predicted probabilities\np_train &lt;- predict(mod_logit, newdata = train_df, type = \"response\")\np_test  &lt;- predict(mod_logit, newdata = test_df,  type = \"response\")\n\n# Class predictions at a default 0.5 threshold (tune later as needed)\nthr &lt;- 0.5\ny_train &lt;- train_df$high_response\ny_test  &lt;- test_df$high_response\nyhat_train &lt;- as.integer(p_train &gt;= thr)\nyhat_test  &lt;- as.integer(p_test  &gt;= thr)\n\nNow we create some helper functions to get metrics from the model\n\n# === Metrics helpers (confusion-matrix stats, ROC AUC, PR AUC) ===============\nsuppressPackageStartupMessages({\n  library(pROC)       # ROC/AUC\n  suppressWarnings(require(PRROC))  # PR curves (optional)\n  library(tidyr)\n  library(ggplot2)\n})\n\ncm_metrics &lt;- function(y, yhat) {\n  tp &lt;- sum(y == 1 & yhat == 1)\n  tn &lt;- sum(y == 0 & yhat == 0)\n  fp &lt;- sum(y == 0 & yhat == 1)\n  fn &lt;- sum(y == 1 & yhat == 0)\n  acc  &lt;- (tp + tn) / (tp + tn + fp + fn)\n  sens &lt;- ifelse((tp + fn) &gt; 0, tp / (tp + fn), NA_real_)  # recall\n  spec &lt;- ifelse((tn + fp) &gt; 0, tn / (tn + fp), NA_real_)\n  prec &lt;- ifelse((tp + fp) &gt; 0, tp / (tp + fp), NA_real_)  # PPV\n  f1   &lt;- ifelse((prec + sens) &gt; 0, 2 * prec * sens / (prec + sens), NA_real_)\n  dplyr::tibble(TP = tp, TN = tn, FP = fp, FN = fn,\n                Accuracy = acc, Sensitivity = sens, Specificity = spec,\n                Precision = prec, F1 = f1)\n}\n\nauc_roc &lt;- function(labels, scores) {\n  as.numeric(pROC::auc(pROC::roc(labels, scores, quiet = TRUE)))\n}\n\nauc_pr &lt;- function(labels, scores, positive_class = 1) {\n  if (!\"PRROC\" %in% .packages()) return(NA_real_)\n  s &lt;- as.numeric(scores); y &lt;- as.integer(labels)\n  fg &lt;- s[y == positive_class]\n  bg &lt;- s[y != positive_class]\n  out &lt;- PRROC::pr.curve(scores.class0 = fg, scores.class1 = bg, curve = FALSE)\n  as.numeric(out$auc.integral)\n}\n\nLets calculate metrics of true positives true negatives false positives and false negatives for our example.\n\n# === Metrics table for glm on TRAIN/TEST ======================================\nlogit_train_metrics &lt;- cm_metrics(y_train, yhat_train) %&gt;%\n  mutate(Model = \"Logistic (glm)\", Dataset = \"Train\",\n         AUC_ROC = auc_roc(y_train, p_train),\n         AUC_PR  = auc_pr(y_train,  p_train))\n\nlogit_test_metrics &lt;- cm_metrics(y_test, yhat_test) %&gt;%\n  mutate(Model = \"Logistic (glm)\", Dataset = \"Test\",\n         AUC_ROC = auc_roc(y_test, p_test),\n         AUC_PR  = auc_pr(y_test,  p_test))\n\ndplyr::bind_rows(logit_train_metrics, logit_test_metrics)\n\n# A tibble: 2 × 13\n     TP    TN    FP    FN Accuracy Sensitivity Specificity Precision    F1 Model\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1  2406  4017   368   208    0.918       0.920       0.916     0.867 0.893 Logi…\n2  1005  1717   163   116    0.907       0.897       0.913     0.860 0.878 Logi…\n# ℹ 3 more variables: Dataset &lt;chr&gt;, AUC_ROC &lt;dbl&gt;, AUC_PR &lt;dbl&gt;\n\n\nMoving we can construct the ROC and AUC curves for our example\n\n\n\n# === ROC curves for glm (downsampled for plotting stability) ==================\nroc_train &lt;- pROC::roc(y_train, p_train, quiet = TRUE)\nroc_test  &lt;- pROC::roc(y_test,  p_test,  quiet = TRUE)\n\nroc_df &lt;- dplyr::bind_rows(\n  tibble::tibble(FPR = 1 - roc_train$specificities, TPR = roc_train$sensitivities, Dataset = \"Train\"),\n  tibble::tibble(FPR = 1 - roc_test$specificities,  TPR = roc_test$sensitivities,  Dataset = \"Test\")\n) %&gt;%\n  dplyr::group_by(Dataset) %&gt;%\n  dplyr::slice( unique(round(seq(1, dplyr::n(), length.out = pmin(400L, dplyr::n())))) ) %&gt;%\n  dplyr::ungroup()\n\nggplot(roc_df, aes(x = FPR, y = TPR, linetype = Dataset)) +\n  geom_abline(slope = 1, intercept = 0, alpha = 0.4) +\n  geom_line(linewidth = 1) +\n  coord_equal() +\n  labs(\n    title = \"ROC curves    Logistic regression\",\n    subtitle = sprintf(\"AUC Train = %.3f, AUC Test = %.3f\",\n                       as.numeric(pROC::auc(roc_train)), as.numeric(pROC::auc(roc_test))),\n    x = \"False Positive Rate\", y = \"True Positive Rate\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOptionally we can create a PR curve\n\n\n\n# === Optional PR curves for glm ===============================================\nif (\"PRROC\" %in% .packages()) {\n  fg_train &lt;- p_train[y_train == 1]; bg_train &lt;- p_train[y_train == 0]\n  fg_test  &lt;- p_test[y_test  == 1];  bg_test  &lt;- p_test[y_test  == 0]\n  pr_train &lt;- PRROC::pr.curve(scores.class0 = fg_train, scores.class1 = bg_train, curve = TRUE)\n  pr_test  &lt;- PRROC::pr.curve(scores.class0 = fg_test,  scores.class1 = bg_test,  curve = TRUE)\n\n  pr_df &lt;- dplyr::bind_rows(\n    tibble::tibble(recall = pr_train$curve[,1], precision = pr_train$curve[,2],\n                   Dataset = paste0(\"Train (AUC-PR = \", sprintf(\"%.3f\", pr_train$auc.integral), \")\")),\n    tibble::tibble(recall = pr_test$curve[,1],  precision = pr_test$curve[,2],\n                   Dataset = paste0(\"Test (AUC-PR = \",  sprintf(\"%.3f\", pr_test$auc.integral), \")\"))\n  )\n\n  ggplot(pr_df, aes(x = recall, y = precision, color = Dataset)) +\n    geom_line(linewidth = 1) +\n    labs(title = \"Precision–Recall curves    Logistic regression\",\n         x = \"Recall (Sensitivity)\", y = \"Precision (PPV)\") +\n    theme_minimal()\n}\n\n\n\n\n\n\n\n\n\nand get the coefficients from the models\n\n# === Coefficient table with Wald-style CIs (robust to confint() failures) =====\ncoef_tbl_logit &lt;- broom::tidy(mod_logit) %&gt;%\n  mutate(\n    conf.low  = estimate - qnorm(0.975) * std.error,\n    conf.high = estimate + qnorm(0.975) * std.error\n  ) %&gt;%\n  arrange(estimate)\n\nprint(coef_tbl_logit, n = min(25, nrow(coef_tbl_logit)))\n\n# A tibble: 107 × 7\n   term              estimate std.error statistic  p.value conf.low conf.high\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 performance_score   -0.676    0.0767    -8.82  1.13e-18   -0.826  -0.526  \n 2 gene_1379           -0.338    0.179     -1.89  5.89e- 2   -0.688   0.0126 \n 3 gene_1733           -0.335    0.170     -1.97  4.87e- 2   -0.668  -0.00196\n 4 gene_1598           -0.305    0.174     -1.75  7.93e- 2   -0.646   0.0357 \n 5 gene_426            -0.283    0.174     -1.63  1.04e- 1   -0.625   0.0579 \n 6 gene_1820           -0.280    0.175     -1.60  1.09e- 1   -0.622   0.0627 \n 7 gene_1760           -0.257    0.180     -1.43  1.54e- 1   -0.610   0.0964 \n 8 gene_719            -0.255    0.177     -1.45  1.48e- 1   -0.601   0.0906 \n 9 gene_170            -0.246    0.174     -1.42  1.57e- 1   -0.587   0.0945 \n10 gene_655            -0.237    0.177     -1.34  1.81e- 1   -0.584   0.110  \n11 gene_934            -0.230    0.174     -1.32  1.87e- 1   -0.572   0.112  \n12 gene_1909           -0.225    0.173     -1.30  1.94e- 1   -0.565   0.115  \n13 gene_819            -0.225    0.175     -1.29  1.99e- 1   -0.568   0.118  \n14 gene_1687           -0.208    0.177     -1.18  2.39e- 1   -0.555   0.138  \n15 gene_1925           -0.206    0.174     -1.19  2.34e- 1   -0.547   0.134  \n16 gene_1291           -0.199    0.174     -1.15  2.52e- 1   -0.540   0.142  \n17 gene_289            -0.198    0.176     -1.13  2.60e- 1   -0.543   0.147  \n18 gene_821            -0.183    0.175     -1.05  2.96e- 1   -0.526   0.160  \n19 gene_1099           -0.182    0.173     -1.05  2.93e- 1   -0.522   0.157  \n20 gene_577            -0.178    0.177     -1.00  3.16e- 1   -0.525   0.170  \n21 gene_1940           -0.176    0.178     -0.988 3.23e- 1   -0.524   0.173  \n22 gene_50             -0.172    0.178     -0.969 3.33e- 1   -0.521   0.176  \n23 gene_1740           -0.157    0.174     -0.900 3.68e- 1   -0.497   0.184  \n24 gene_41             -0.150    0.174     -0.862 3.89e- 1   -0.492   0.191  \n25 gene_1946           -0.148    0.176     -0.842 4.00e- 1   -0.493   0.197  \n# ℹ 82 more rows\n\n\n\n\n2.14.7 Understanding Log-Odds and Coefficient Interpretation in Logistic Regression\nIn linear regression, we predict a continuous outcome. In logistic regression, the outcome is binary (e.g., high_response = 1 for “high responder” vs 0 for “low responder”). To keep predicted probabilities between 0 and 1, we model the logarithm of the odds (the logit) of being a responder.\n\n2.14.7.1 Odds and Logit\nThe odds of success are: $[ = ] $If (p=0.75), then (=0.75/0.25=3) (three-to-one).\nThe logit (log-odds) is: \\(\\[ \\text{logit}(p) = \\log!\\left(\\frac{p}{1-p}\\right) \\]\\)\nLogistic regression is linear in the log-odds: \\(\\[ \\log!\\left(\\frac{p}{1-p}\\right)=w_0+w_1 x_1+\\cdots+w_k x_k \\]\\)\nEach coefficient ( $w_j $) is the change in log-odds for a one-unit increase in (x_j), holding other variables fixed.\n\n\n2.14.7.2 Odds Ratios\nExponentiating a coefficient yields an odds ratio (OR): \\(\\[ \\text{OR}\\_j = e\\^{w_j} \\] - (\\text{OR}\\_j \\&gt; 1)\\): increasing (x_j) raises the odds of response\n- (\\(\\text{OR}\\_j &lt; 1\\)): increasing (x_j) lowers the odds of response\n- (\\(\\text{OR}\\_j = 1\\)): no change in odds\nExample interpretations: - \\(( \\w=+0.80 \\Rightarrow \\text{OR}=e\\^{0.80}\\approx 2.22)\\): odds a bit more than double per unit increase.\n- \\(( w=-0.50 \\Rightarrow \\text{OR}\\approx 0.61)\\): odds drop by \\(\\~39%\\) per unit increase.\n\n\n2.14.7.3 From Log-Odds Back to Probability\n\\(\\[ p=\\frac{1}{1+e^{-(w_0+w_1 x_1+\\cdots+w_k x_k)}} \\]\\) If log-odds = (1.5), then (p). If log-odds = (-1.5), then (p).\n\n\n\n2.14.8 Compute Odds Ratios from the Fitted Model\n\n# Build odds–ratio table from the fitted model\n# (uses normal-approx CI; avoids profile-likelihood warnings/slowdowns)\nlibrary(dplyr)\nlibrary(broom)\n\nzcrit &lt;- qnorm(0.975)  # 1.96\n\nor_tbl &lt;- broom::tidy(mod_logit) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(\n    OR      = exp(estimate),\n    OR_low  = exp(estimate - zcrit * std.error),\n    OR_high = exp(estimate + zcrit * std.error)\n  ) %&gt;%\n  arrange(desc(abs(estimate)))\n\n# --- Ways to *see* the OR columns clearly ---\n\n# A) Select and print only the columns you care about\nor_tbl %&gt;%\n  select(term, estimate, std.error, statistic, p.value, OR, OR_low, OR_high) %&gt;%\n  slice_head(n = 12)\n\n# A tibble: 12 × 8\n   term            estimate std.error statistic  p.value      OR  OR_low OR_high\n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 treatmentchemo     6.78     0.658      10.3  6.35e-25 882.    243.    3.20e+3\n 2 dose_intensity     3.74     0.595       6.29 3.14e-10  42.2    13.1   1.35e+2\n 3 tumor_gradeG3      1.24     0.143       8.68 4.08e-18   3.47    2.62  4.59e+0\n 4 performance_sc…   -0.676    0.0767     -8.82 1.13e-18   0.509   0.438 5.91e-1\n 5 gene_1319          0.450    0.175       2.57 1.03e- 2   1.57    1.11  2.21e+0\n 6 gene_1039          0.435    0.178       2.44 1.45e- 2   1.55    1.09  2.19e+0\n 7 tumor_gradeG2      0.402    0.124       3.25 1.14e- 3   1.50    1.17  1.91e+0\n 8 gene_1302          0.355    0.177       2.01 4.48e- 2   1.43    1.01  2.02e+0\n 9 gene_1379         -0.338    0.179      -1.89 5.89e- 2   0.714   0.503 1.01e+0\n10 gene_1733         -0.335    0.170      -1.97 4.87e- 2   0.715   0.513 9.98e-1\n11 gene_1598         -0.305    0.174      -1.75 7.93e- 2   0.737   0.524 1.04e+0\n12 gene_58            0.292    0.177       1.65 9.93e- 2   1.34    0.946 1.90e+0\n\n# B) Print all columns without truncation\nprint(or_tbl, n = 12, width = Inf)\n\n# A tibble: 106 × 8\n   term              estimate std.error statistic  p.value      OR  OR_low\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 treatmentchemo       6.78     0.658      10.3  6.35e-25 882.    243.   \n 2 dose_intensity       3.74     0.595       6.29 3.14e-10  42.2    13.1  \n 3 tumor_gradeG3        1.24     0.143       8.68 4.08e-18   3.47    2.62 \n 4 performance_score   -0.676    0.0767     -8.82 1.13e-18   0.509   0.438\n 5 gene_1319            0.450    0.175       2.57 1.03e- 2   1.57    1.11 \n 6 gene_1039            0.435    0.178       2.44 1.45e- 2   1.55    1.09 \n 7 tumor_gradeG2        0.402    0.124       3.25 1.14e- 3   1.50    1.17 \n 8 gene_1302            0.355    0.177       2.01 4.48e- 2   1.43    1.01 \n 9 gene_1379           -0.338    0.179      -1.89 5.89e- 2   0.714   0.503\n10 gene_1733           -0.335    0.170      -1.97 4.87e- 2   0.715   0.513\n11 gene_1598           -0.305    0.174      -1.75 7.93e- 2   0.737   0.524\n12 gene_58              0.292    0.177       1.65 9.93e- 2   1.34    0.946\n    OR_high\n      &lt;dbl&gt;\n 1 3202.   \n 2  135.   \n 3    4.59 \n 4    0.591\n 5    2.21 \n 6    2.19 \n 7    1.91 \n 8    2.02 \n 9    1.01 \n10    0.998\n11    1.04 \n12    1.90 \n# ℹ 94 more rows\n\n# C) Nicely formatted table (if in a report)\n# knitr::kable(\n#   or_tbl %&gt;%\n#     transmute(\n#       term,\n#       `Estimate (β)` = estimate,\n#       `Std. Error`    = std.error,\n#       `z`             = statistic,\n#       `p`             = p.value,\n#       `OR = exp(β)`   = OR,\n#       `OR low`        = OR_low,\n#       `OR high`       = OR_high\n#     ) %&gt;%\n#     slice_head(n = 12),\n#   digits = 3, align = \"lrrrrrrr\",\n#   caption = \"Top coefficients by |β| with odds ratios and 95% CI\"\n# )\n\n# D) If you want rounded values for readability\nor_tbl_rounded &lt;- or_tbl %&gt;%\n  mutate(\n    across(c(estimate, std.error, statistic), ~round(.x, 3)),\n    across(c(p.value), ~signif(.x, 3)),\n    across(c(OR, OR_low, OR_high), ~round(.x, 2))\n  )\n\nor_tbl_rounded %&gt;%\n  select(term, estimate, std.error, statistic, p.value, OR, OR_low, OR_high) %&gt;%\n  slice_head(n = 12)\n\n# A tibble: 12 × 8\n   term              estimate std.error statistic  p.value     OR OR_low OR_high\n   &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n 1 treatmentchemo       6.78      0.658     10.3  6.35e-25 882.   243.   3202.  \n 2 dose_intensity       3.74      0.595      6.29 3.14e-10  42.2   13.2   135.  \n 3 tumor_gradeG3        1.24      0.143      8.68 4.08e-18   3.47   2.62    4.59\n 4 performance_score   -0.676     0.077     -8.82 1.13e-18   0.51   0.44    0.59\n 5 gene_1319            0.45      0.175      2.57 1.03e- 2   1.57   1.11    2.21\n 6 gene_1039            0.435     0.178      2.44 1.45e- 2   1.55   1.09    2.19\n 7 tumor_gradeG2        0.402     0.124      3.25 1.14e- 3   1.5    1.17    1.91\n 8 gene_1302            0.355     0.177      2.01 4.48e- 2   1.43   1.01    2.02\n 9 gene_1379           -0.338     0.179     -1.89 5.89e- 2   0.71   0.5     1.01\n10 gene_1733           -0.335     0.17      -1.97 4.87e- 2   0.72   0.51    1   \n11 gene_1598           -0.305     0.174     -1.76 7.93e- 2   0.74   0.52    1.04\n12 gene_58              0.292     0.177      1.65 9.93e- 2   1.34   0.95    1.9 \n\n\nThe treatment variable (chemo vs. no_chemo) shows by far the strongest association, with an estimated coefficient of 6.78, corresponding to an odds ratio (OR) of approximately 885. This means that, holding other predictors constant, patients who received chemotherapy had odds of achieving a high tumor response nearly 900 times greater than those who did not receive it.\nThe dose_intensity variable also exerts a very large positive effect (w = 3.74; OR ≈ 42). Each one-unit increase in dose intensity multiplies the odds of response by roughly 42, indicating a steep dose–response relationship.\nTumor-related characteristics also influence response probability. A higher tumor_grade (e.g., G2 or G3 relative to the baseline category) is associated with an estimated b of 1.24 (OR ≈ 3.46), suggesting about a three-and-a-half-fold increase in the odds of response. A smaller positive contrast (w = 0.40; OR ≈ 1.49) indicates that other grade categories also contribute modestly to improved response rates.\nIn contrast, performance_score has a negative coefficient (w = −0.676; OR ≈ 0.51), meaning that each additional point on this score reflecting poorer clinical performance reduces the odds of a favorable response by about half.\nSeveral genes show smaller but biologically interesting effects. gene_1598 (w = +0.45; OR ≈ 1.57) and gene_1551 (w = +0.44; OR ≈ 1.55) both display modest positive associations, where higher expression slightly increases the probability of response. gene_779 (w = +0.36; OR ≈ 1.43) and gene_588 (w = +0.29; OR ≈ 1.34) show similarly mild but consistent trends toward higher response odds.\nConversely, gene_565 (w = −0.34; OR ≈ 0.71), gene_323 (w = −0.34; OR ≈ 0.72), and gene_1183 (w = −0.31; OR ≈ 0.74) are negatively associated with high response, suggesting that higher expression of these genes slightly decreases the odds of tumor reduction, possibly reflecting resistance pathways.\n\n\n2.14.9 Benchmarking Logistic regression with LASSO, RIDGE and ELASTIC NET counterparts\n\n# === Penalized logistic: LASSO, Ridge, Elastic Net ============================\nsuppressPackageStartupMessages({\n  library(glmnet)\n})\n\n# Freeze factor levels/dummies using the TRAIN design implied by f_logit\nmf_train  &lt;- model.frame(f_logit, data = train_df)\nterms_log &lt;- terms(mf_train)\n\nX_train &lt;- model.matrix(terms_log, data = train_df)[, -1, drop = FALSE]\nX_test  &lt;- model.matrix(terms_log, data = test_df)[,  -1, drop = FALSE]\n\ny_train &lt;- train_df$high_response\ny_test  &lt;- test_df$high_response\n\nset.seed(42)\n\n# LASSO (alpha = 1)\ncv_lasso  &lt;- cv.glmnet(X_train, y_train, family = \"binomial\", alpha = 1, nfolds = 10)\nmod_lasso &lt;- glmnet(X_train, y_train, family = \"binomial\", alpha = 1, lambda = cv_lasso$lambda.min)\np_lasso_train &lt;- as.numeric(predict(mod_lasso, newx = X_train, type = \"response\"))\np_lasso_test  &lt;- as.numeric(predict(mod_lasso, newx = X_test,  type = \"response\"))\n\n# Ridge (alpha = 0)\ncv_ridge  &lt;- cv.glmnet(X_train, y_train, family = \"binomial\", alpha = 0, nfolds = 10)\nmod_ridge &lt;- glmnet(X_train, y_train, family = \"binomial\", alpha = 0, lambda = cv_ridge$lambda.min)\np_ridge_train &lt;- as.numeric(predict(mod_ridge, newx = X_train, type = \"response\"))\np_ridge_test  &lt;- as.numeric(predict(mod_ridge, newx = X_test,  type = \"response\"))\n\n# Elastic Net (alpha = 0.15 as an example)\nalpha_en   &lt;- 0.15\ncv_enet    &lt;- cv.glmnet(X_train, y_train, family = \"binomial\", alpha = alpha_en, nfolds = 10)\nmod_enet   &lt;- glmnet(X_train, y_train, family = \"binomial\", alpha = alpha_en, lambda = cv_enet$lambda.min)\np_enet_train &lt;- as.numeric(predict(mod_enet, newx = X_train, type = \"response\"))\np_enet_test  &lt;- as.numeric(predict(mod_enet, newx = X_test,  type = \"response\"))\n\nLet’s now compare the models\n\n\n\n# === Compare models on TEST ====================================================\ncompare_test &lt;- tibble::tibble(\n  Model = c(\"Logistic (glm)\", \"LASSO-logit\", \"Ridge-logit\", \"ElasticNet-logit\"),\n  Prob  = list(p_test,        p_lasso_test,  p_ridge_test,  p_enet_test)\n) %&gt;%\n  dplyr::rowwise() %&gt;%\n  dplyr::mutate(\n    yhat    = list(as.integer(unlist(Prob) &gt;= 0.5)),\n    Metrics = list(cm_metrics(y_test, unlist(yhat))),\n    AUC_ROC = auc_roc(y_test, unlist(Prob)),\n    AUC_PR  = auc_pr(y_test,  unlist(Prob))\n  ) %&gt;%\n  tidyr::unnest(Metrics) \n\ncompare_test\n\n# A tibble: 4 × 14\n  Model     Prob  yhat     TP    TN    FP    FN Accuracy Sensitivity Specificity\n  &lt;chr&gt;     &lt;lis&gt; &lt;lis&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 Logistic… &lt;dbl&gt; &lt;int&gt;  1005  1717   163   116    0.907       0.897       0.913\n2 LASSO-lo… &lt;dbl&gt; &lt;int&gt;  1019  1716   164   102    0.911       0.909       0.913\n3 Ridge-lo… &lt;dbl&gt; &lt;int&gt;  1034  1697   183    87    0.910       0.922       0.903\n4 ElasticN… &lt;dbl&gt; &lt;int&gt;  1019  1712   168   102    0.910       0.909       0.911\n# ℹ 4 more variables: Precision &lt;dbl&gt;, F1 &lt;dbl&gt;, AUC_ROC &lt;dbl&gt;, AUC_PR &lt;dbl&gt;\n\n\nand plot metrics of performance\n\n\n\n# === Faceted ROC across models (TEST) =========================================\nroc_facets &lt;- function(models, labels,\n                       suptitle = \"ROC curves by model (TEST)\",\n                       xlaw = \"False positive rate (1 - specificity)\",\n                       ylaw = \"True positive rate (sensitivity)\",\n                       caption = NULL) {\n  df_list &lt;- vector(\"list\", length(models))\n  model_names &lt;- names(models)\n  if (is.null(model_names)) model_names &lt;- paste0(\"Model \", seq_along(models))\n  for (i in seq_along(models)) {\n    roc_i &lt;- pROC::roc(labels, models[[i]], quiet = TRUE)\n    auc_i &lt;- as.numeric(pROC::auc(roc_i))\n    pts   &lt;- tibble::tibble(\n      fpr   = 1 - roc_i$specificities,\n      tpr   = roc_i$sensitivities,\n      model = sprintf(\"%s (AUC = %.3f)\", model_names[i], auc_i)\n    )\n    df_list[[i]] &lt;- pts\n  }\n  roc_df &lt;- dplyr::bind_rows(df_list)\n\n  ggplot(roc_df, aes(x = fpr, y = tpr)) +\n    geom_path(linewidth = 1) +\n    geom_abline(slope = 1, intercept = 0, linetype = 2) +\n    facet_wrap(~ model) +\n    labs(title = suptitle, x = xlab, y = ylab, caption = caption) +\n    theme_minimal(base_size = 12)\n}\n\nmodels_scores &lt;- list(\n  \"Logistic (glm)\" = p_test,\n  \"LASSO-logit\"    = p_lasso_test,\n  \"Ridge-logit\"    = p_ridge_test,\n  \"ENet-logit\"     = p_enet_test\n)\n\nroc_facets(models_scores, labels = y_test,\n           suptitle = \"ROC by model on TEST (ct_reduced)\")\n\n\n\n\n\n\n\n\n\n# === Faceted PR across models (TEST, optional) ================================\nif (\"PRROC\" %in% .packages()) {\n  pr_facets &lt;- function(models, labels,\n                        positive_class = 1,\n                        suptitle = \"Precision–Recall curves by model (TEST)\",\n                        caption = NULL) {\n    df_list &lt;- vector(\"list\", length(models))\n    model_names &lt;- names(models)\n    if (is.null(model_names)) model_names &lt;- paste0(\"Model \", seq_along(models))\n    for (i in seq_along(models)) {\n      s &lt;- as.numeric(models[[i]])\n      y &lt;- as.integer(labels)\n      s_pos &lt;- s[y == positive_class]\n      s_neg &lt;- s[y != positive_class]\n      pr &lt;- PRROC::pr.curve(scores.class0 = s_pos, scores.class1 = s_neg, curve = TRUE)\n      tmp &lt;- tibble::tibble(\n        recall    = pr$curve[, 1],\n        precision = pr$curve[, 2],\n        model     = sprintf(\"%s (AUC-PR = %.3f)\", model_names[i], pr$auc.integral)\n      )\n      df_list[[i]] &lt;- tmp\n    }\n    pr_df &lt;- dplyr::bind_rows(df_list)\n\n    ggplot(pr_df, aes(recall, precision)) +\n      geom_path(linewidth = 1) +\n      facet_wrap(~ model) +\n      labs(title = suptitle, x = \"Recall (sensitivity)\", y = \"Precision (PPV)\", caption = caption) +\n      theme_minimal(base_size = 12)\n  }\n\n  pr_facets(models_scores, labels = y_test,\n            suptitle = \"Precision–Recall by model on TEST (ct_reduced)\")\n}\n\n\n\n\n\n\n\n\n\n\n\n\nEisenhauer, Elizabeth A., Patrick Therasse, Jan Bogaerts, Lawrence H. Schwartz, Daniel Sargent, Rebecca Ford, Janet Dancey, et al. 2009. “New Response Evaluation Criteria in Solid Tumours: Revised RECIST Guideline (Version 1.1).” European Journal of Cancer 45 (2): 228–47. https://doi.org/10.1016/j.ejca.2008.10.026.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Supervised Learning: Regression tasks</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bell, David E., Howard Raiffa, and Amos Tversky, eds. 1988. Decision\nMaking: Descriptive, Normative, and Prescriptive Interactions.\nCambridge: Cambridge University Press. https://www.cambridge.org/core/books/decision-making/D29E12748C31B85FE83C9CEDEE9D4D88.\n\n\nEisenhauer, Elizabeth A., Patrick Therasse, Jan Bogaerts, Lawrence H.\nSchwartz, Daniel Sargent, Rebecca Ford, Janet Dancey, et al. 2009.\n“New Response Evaluation Criteria in Solid Tumours: Revised RECIST\nGuideline (Version 1.1).” European Journal of Cancer 45\n(2): 228–47. https://doi.org/10.1016/j.ejca.2008.10.026.\n\n\nHasin, Yoram, Michael Seldin, and Aldons Lusis. 2017. “Multi-Omics\nApproaches to Disease.” Genome Biology 18: 83. https://doi.org/10.1186/s13059-017-1215-1.\n\n\nHilbert, Martin, and Priscila López. 2011. “The World’s\nTechnological Capacity to Store, Communicate, and Compute\nInformation.” Science 332 (6025): 60–65. https://doi.org/10.1126/science.1200970.\n\n\nHoward, Ronald A. 1988. “Decision Analysis: Practice and\nPromise.” Management Science 34 (6): 679–95.\n\n\nHripcsak, George, and David J. Albers. 2013. “Next-Generation\nPhenotyping of Electronic Health Records.” Journal of the\nAmerican Medical Informatics Association 20 (1): 117–21. https://doi.org/10.1136/amiajnl-2012-001145.\n\n\nJensen, Peter B., Lars J. Jensen, and Søren Brunak. 2012. “Mining\nElectronic Health Records: Towards Better Research Applications and\nClinical Care.” Nature Reviews Genetics 13 (6): 395–405.\nhttps://doi.org/10.1038/nrg3208.\n\n\nKarczewski, Konrad J., and Michael P. Snyder. 2018. “Integrative\nOmics for Health and Disease.” Nature Reviews Genetics\n19: 299–310. https://doi.org/10.1038/nrg.2018.4.\n\n\nRaghupathi, Wullianallur, and Viju Raghupathi. 2014. “Big Data\nAnalytics in Healthcare: Promise and Potential.” Health\nInformation Science and Systems 2 (3): 1–10. https://doi.org/10.1186/2047-2501-2-3.\n\n\nSarti, Danilo Augusto. 2013. “Uncertainty Management Through\nDecision Analysis: Applications to Production Optimization and Uncertain\nDemands.” PhD thesis, Universidade de São Paulo.\n\n\nSpetzler, Carl, Hannah Winter, and Jennifer Meyer. 2016. Decision\nQuality: Value Creation from Better Business Decisions. Hoboken,\nNJ: John Wiley & Sons.\n\n\nStephens, Zachary D., Skylar Y. Lee, Faraz Faghri, R. Ilkayeva Campbell,\net al. 2015. “Big Data: Astronomical or Genomical?”\nPLOS Biology 13 (7): e1002195. https://doi.org/10.1371/journal.pbio.1002195.\n\n\nTopol, Eric J. 2019. Deep Medicine: How Artificial Intelligence Can\nMake Healthcare Human Again. New York: Basic Books.\n\n\nWhite, Douglas John. 1970. Decision Theory. New Brunswick, NJ:\nRoutledge / Transaction Publishers. https://www.routledge.com/Decision-Theory/White/p/book/9780202308982.",
    "crumbs": [
      "References"
    ]
  }
]